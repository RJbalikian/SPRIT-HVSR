<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>sprit.sprit_hvsr API documentation</title>
<meta name="description" content="This module is the main SpRIT module that contains all the functions needed to run HVSR analysis â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>sprit.sprit_hvsr</code></h1>
</header>
<section id="section-intro">
<p>This module is the main SpRIT module that contains all the functions needed to run HVSR analysis.</p>
<p>The functions defined here are read both by the SpRIT graphical user interface and by the command-line interface to run HVSR analysis on input data.</p>
<p>See documentation for individual functions for more information.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
This module is the main SpRIT module that contains all the functions needed to run HVSR analysis.

The functions defined here are read both by the SpRIT graphical user interface and by the command-line interface to run HVSR analysis on input data.

See documentation for individual functions for more information.
&#34;&#34;&#34;
import copy
import datetime
import inspect
import json
import math
import os
import pathlib
import pickle
import pkg_resources
import tempfile
import traceback
import warnings
import xml.etree.ElementTree as ET
import sys

import matplotlib
from matplotlib.backend_bases import MouseButton
import matplotlib.dates as mdates
import matplotlib.pyplot as plt
import numpy as np
import obspy
from obspy.signal import PPSD
import pandas as pd
from pyproj import CRS, Transformer
import scipy

try: #For distribution
    from sprit import sprit_utils
except: #For testing
    import sprit_utils
    pass


#Main variables
greek_chars = {&#39;sigma&#39;: u&#39;\u03C3&#39;, &#39;epsilon&#39;: u&#39;\u03B5&#39;, &#39;teta&#39;: u&#39;\u03B8&#39;}
channel_order = {&#39;Z&#39;: 0, &#39;1&#39;: 1, &#39;N&#39;: 1, &#39;2&#39;: 2, &#39;E&#39;: 2}
separator_character = &#39;=&#39;
obspyFormats =  [&#39;AH&#39;, &#39;ALSEP_PSE&#39;, &#39;ALSEP_WTH&#39;, &#39;ALSEP_WTN&#39;, &#39;CSS&#39;, &#39;DMX&#39;, &#39;GCF&#39;, &#39;GSE1&#39;, &#39;GSE2&#39;, &#39;KINEMETRICS_EVT&#39;, &#39;KNET&#39;, &#39;MSEED&#39;, &#39;NNSA_KB_CORE&#39;, &#39;PDAS&#39;, &#39;PICKLE&#39;, &#39;Q&#39;, &#39;REFTEK130&#39;, &#39;RG16&#39;, &#39;SAC&#39;, &#39;SACXY&#39;, &#39;SEG2&#39;, &#39;SEGY&#39;, &#39;SEISAN&#39;, &#39;SH_ASC&#39;, &#39;SLIST&#39;, &#39;SU&#39;, &#39;TSPAIR&#39;, &#39;WAV&#39;, &#39;WIN&#39;, &#39;Y&#39;]

t0 = datetime.datetime.now().time()
max_rank = 0
plotRows = 4

sample_data_dir = pathlib.Path(pkg_resources.resource_filename(__name__, &#39;resources/sample_data/&#39;))
sampleFileKeyMap = {&#39;1&#39;:sample_data_dir.joinpath(&#39;SampleHVSRSite1_AM.RAC84.00.2023.046_2023-02-15_1704-1734.MSEED&#39;),
                    &#39;2&#39;:sample_data_dir.joinpath(&#39;SampleHVSRSite2_AM.RAC84.00.2023-02-15_2132-2200.MSEED&#39;),
                    &#39;3&#39;:sample_data_dir.joinpath(&#39;SampleHVSRSite3_AM.RAC84.00.2023.199_2023-07-18_1432-1455.MSEED&#39;),
                    &#39;4&#39;:sample_data_dir.joinpath(&#39;SampleHVSRSite4_AM.RAC84.00.2023.199_2023-07-18_1609-1629.MSEED&#39;),
                    &#39;5&#39;:sample_data_dir.joinpath(&#39;SampleHVSRSite5_AM.RAC84.00.2023.199_2023-07-18_2039-2100.MSEED&#39;),
                    &#39;6&#39;:sample_data_dir.joinpath(&#39;SampleHVSRSite6_AM.RAC84.00.2023.192_2023-07-11_1510-1528.MSEED&#39;),
                    
                    &#39;sample1&#39;:sample_data_dir.joinpath(&#39;SampleHVSRSite1_AM.RAC84.00.2023.046_2023-02-15_1704-1734.MSEED&#39;),
                    &#39;sample2&#39;:sample_data_dir.joinpath(&#39;SampleHVSRSite2_AM.RAC84.00.2023-02-15_2132-2200.MSEED&#39;),
                    &#39;sample3&#39;:sample_data_dir.joinpath(&#39;SampleHVSRSite3_AM.RAC84.00.2023.199_2023-07-18_1432-1455.MSEED&#39;),
                    &#39;sample4&#39;:sample_data_dir.joinpath(&#39;SampleHVSRSite4_AM.RAC84.00.2023.199_2023-07-18_1609-1629.MSEED&#39;),
                    &#39;sample5&#39;:sample_data_dir.joinpath(&#39;SampleHVSRSite5_AM.RAC84.00.2023.199_2023-07-18_2039-2100.MSEED&#39;),
                    &#39;sample6&#39;:sample_data_dir.joinpath(&#39;SampleHVSRSite6_AM.RAC84.00.2023.192_2023-07-11_1510-1528.MSEED&#39;),

                    &#39;sample_1&#39;:sample_data_dir.joinpath(&#39;SampleHVSRSite1_AM.RAC84.00.2023.046_2023-02-15_1704-1734.MSEED&#39;),
                    &#39;sample_2&#39;:sample_data_dir.joinpath(&#39;SampleHVSRSite2_AM.RAC84.00.2023-02-15_2132-2200.MSEED&#39;),
                    &#39;sample_3&#39;:sample_data_dir.joinpath(&#39;SampleHVSRSite3_AM.RAC84.00.2023.199_2023-07-18_1432-1455.MSEED&#39;),
                    &#39;sample_4&#39;:sample_data_dir.joinpath(&#39;SampleHVSRSite4_AM.RAC84.00.2023.199_2023-07-18_1609-1629.MSEED&#39;),
                    &#39;sample_5&#39;:sample_data_dir.joinpath(&#39;SampleHVSRSite5_AM.RAC84.00.2023.199_2023-07-18_2039-2100.MSEED&#39;),
                    &#39;sample_6&#39;:sample_data_dir.joinpath(&#39;SampleHVSRSite6_AM.RAC84.00.2023.192_2023-07-11_1510-1528.MSEED&#39;),
                    
                    &#39;batch&#39;:sample_data_dir.joinpath(&#39;Batch_SampleData.csv&#39;)}

#CLASSES

#Check if the data is already the right class
# Define a decorator that wraps the __init__ method
def check_instance(init):
    def wrapper(self, *args, **kwargs):
        # Check if the first argument is an instance of self.__class__
        if args and isinstance(args[0], self.__class__):
            # Copy its attributes to self
            self.__dict__.update(args[0].__dict__)
        else:
            # Call the original __init__ method
            init(self, *args, **kwargs)
    return wrapper

#Class for batch data
class HVSRBatch:
    &#34;&#34;&#34;HVSRBatch is the data container used for batch processing. It contains several HVSRData objects (one for each site). These can be accessed using their site name, either square brackets (HVSRBatchVariable[&#34;SiteName&#34;]) or the dot (HVSRBatchVariable.SiteName) accessor.
    
    The dot accessor may not work if there is a space in the site name.
    
    All of the  functions in the sprit.pacakge are designed to perform the bulk of their operations iteratively on the individual HVSRData objects contained in the HVSRBatch object, and do little with the HVSRBatch object itself, besides using it determine which sites are contained within it.
    
    &#34;&#34;&#34;
    @check_instance
    def __init__(self, sites_dict):
      
        self._batch_dict = sites_dict
        self.batch_dict = self._batch_dict
        self.batch = True

        for sitename, hvsrdata in sites_dict.items():
            setattr(self, sitename, hvsrdata)
            self[sitename][&#39;batch&#39;]=True
            
        self.sites = list(self._batch_dict.keys())

    #METHODS
    def __to_json(self, filepath):
        &#34;&#34;&#34;Not yet implemented, but may allow import/export to json files in the future, rather than just .hvsr pickles

        Parameters
        ----------
        filepath : filepath object
            Location to save HVSRBatch object as json
        &#34;&#34;&#34;
        # open the file with the given filepath
        with open(filepath, &#39;w&#39;) as f:
            # dump the JSON string to the file
            json.dump(self, f, default=lambda o: o.__dict__, sort_keys=True, indent=4)

    def export(self, export_path=True, ext=&#39;hvsr&#39;):
        &#34;&#34;&#34;Method to export HVSRData objects in HVSRBatch container to indivdual .hvsr pickle files.

        Parameters
        ----------
        export_path : filepath, default=True
            Filepath to save file. Can be either directory (which will assign a filename based on the HVSRData attributes). By default True. If True, it will first try to save each file to the same directory as datapath, then if that does not work, to the current working directory, then to the user&#39;s home directory, by default True
        ext : str, optional
            The extension to use for the output, by default &#39;hvsr&#39;. This is still a pickle file that can be read with pickle.load(), but will have .hvsr extension.
        &#34;&#34;&#34;
        export_data(hvsr_data=self, export_path=export_path, ext=ext)

    def keys(self):
        &#34;&#34;&#34;Method to return the &#34;keys&#34; of the HVSRBatch object. For HVSRBatch objects, these are the site names. Functions similar to dict.keys().

        Returns
        -------
        dict_keys
            A dict_keys object listing the site names of each of the HVSRData objects contained in the HVSRBatch object
        &#34;&#34;&#34;
        return self.batch_dict.keys()

    def items(self):
        &#34;&#34;&#34;Method to return both the site names and the HVSRData object as a set of dict_items tuples. Functions similar to dict.items().

        Returns
        -------
        _type_
            _description_
        &#34;&#34;&#34;
        return self.batch_dict.items()

    def copy(self, type=&#39;shallow&#39;):
        &#34;&#34;&#34;Make a copy of the HVSRBatch object. Uses python copy module.
        
        Parameters
        ----------
        type : str {&#39;shallow&#39;, &#39;deep&#39;}
            Based on input, creates either a shallow or deep copy of the HVSRBatch object. Shallow is equivalent of copy.copy(). Input of &#39;deep&#39; is equivalent of copy.deepcopy() (still experimental). Defaults to shallow.
    
        &#34;&#34;&#34;
        if type.lower()==&#39;deep&#39;:
            return HVSRBatch(copy.deepcopy(self._batch_dict))
        else:
            return HVSRBatch(copy.copy(self._batch_dict))

    #Method wrapper of sprit.plot_hvsr function
    def plot(self, **kwargs):
        &#34;&#34;&#34;Method to plot data, based on the sprit.plot_hvsr() function. All the same kwargs and default values apply as plot_hvsr(). For return_fig, returns it to the &#39;Plot_Report&#39; attribute of each HVSRData object

        Returns
        -------
        _type_
            _description_
        &#34;&#34;&#34;
        for sitename in self:
            if &#39;return_fig&#39; in kwargs.keys() and kwargs[&#39;return_fig&#39;]:
                self[sitename][&#39;Plot_Report&#39;] = plot_hvsr(self[sitename], **kwargs)
            else:
                plot_hvsr(self[sitename], **kwargs)

        return self
    
    def get_report(self, **kwargs):
        &#34;&#34;&#34;Method to get report from processed data, in print, graphical, or tabular format.

        Returns
        -------
        Variable
            May return nothing, pandas.Dataframe, or pyplot Figure, depending on input.
        &#34;&#34;&#34;
        if &#39;report_format&#39; in kwargs.keys():
            if &#39;csv&#39; == kwargs[&#39;report_format&#39;]:
                for sitename in self:
                    rowList = []
                    rowList.append(get_report(self[sitename], **kwargs))
                return pd.concat(rowList, ignore_index=True)
            elif &#39;plot&#39; == kwargs[&#39;report_format&#39;]:
                plotDict = {}
                for sitename in self:
                    if &#39;return_fig&#39; in kwargs.keys() and kwargs[&#39;return_fig&#39;]:
                        plotDict[sitename] = get_report(self[sitename], **kwargs)
                    else:
                        get_report(self[sitename], **kwargs)
                return plotDict
            
        #Only report_format left is print, doesn&#39;t return anything, so doesn&#39;t matter if defalut or not
        for sitename in self:
            get_report(self[sitename], **kwargs)
        return

    def report(self, **kwargs):
        &#34;&#34;&#34;Wrapper of get_report()&#34;&#34;&#34;
        return self.get_report(**kwargs)

    def __iter__(self):
        return iter(self._batch_dict.keys())

    def __setitem__(self, key, value):
        setattr(self, key, value)

    def __getitem__(self, key):
        return getattr(self, key)

#Class for each HVSR site
class HVSRData:
    &#34;&#34;&#34;HVSRData is the basic data class of the sprit package. It contains all the processed data, input parameters, and reports.
    
    These attributes and objects can be accessed using square brackets or the dot accessor. For example, to access the site name, HVSRData[&#39;site&#39;] and HVSRData.site will both return the site name.
    
    Some of the methods that work on the HVSRData object (e.g., .plot() and .get_report()) are essentially wrappers for some of the main sprit package functions (sprit.plot_hvsr() and sprit.get_report(), respectively)
    &#34;&#34;&#34;
    #Old way of using this
    #def __new__(cls, params):
    #    if isinstance(params, (cls, HVSRBatch)):
    #        return params
    #    return super().__new__(cls)

    @check_instance    
    def __init__(self, params):
        self.params = params
        #self.datastream = None
        self.batch = False
        #self.tsteps_used = []

        for key, value in params.items():
            setattr(self, key, value)
            if key==&#39;input_params&#39;:
                for k, v in params[key].items():
                    setattr(self, k, v)

    def __setitem__(self, key, value):
        setattr(self, key, value)

    def __getitem__(self, key):
        return getattr(self, key)

    def __to_json(self, filepath):
        &#34;&#34;&#34;Not yet supported, will export HVSRData object to json&#34;&#34;&#34;
        # open the file with the given filepath
        def unseriable_fun(o):
            if isinstance(o, np.ndarray):
                output = o.tolist()
            try:
                output = o.__dict__
            except:
                output = dir(o)
            return output

        with open(filepath, &#39;w&#39;) as f:
            # dump the JSON string to the file
            json.dump(self, f, default=unseriable_fun, sort_keys=True, indent=4)

    def export(self, export_path=None, ext=&#39;hvsr&#39;):
        &#34;&#34;&#34;Method to export HVSRData objects to .hvsr pickle files.

        Parameters
        ----------
        export_path : filepath, default=True
            Filepath to save file. Can be either directory (which will assign a filename based on the HVSRData attributes). By default True. If True, it will first try to save each file to the same directory as datapath, then if that does not work, to the current working directory, then to the user&#39;s home directory, by default True
        ext : str, optional
            The extension to use for the output, by default &#39;hvsr&#39;. This is still a pickle file that can be read with pickle.load(), but will have .hvsr extension.
        &#34;&#34;&#34;
        export_data(hvsr_data=self, export_path=export_path, ext=ext)

    #METHODS (many reflect dictionary methods)
    def keys(self):
        &#34;&#34;&#34;Method to return the &#34;keys&#34; of the HVSRData object. For HVSRData objects, these are the attributes and parameters of the object. Functions similar to dict.keys().

        Returns
        -------
        dict_keys
            A dict_keys object of the HVSRData objects attributes, parameters, etc.
        &#34;&#34;&#34;        
        keyList = []
        for k in dir(self):
            if not k.startswith(&#39;_&#39;):
                keyList.append(k)
        return keyList

    def items(self):
        &#34;&#34;&#34;Method to return the &#34;items&#34; of the HVSRData object. For HVSRData objects, this is a dict_items object with the keys and values in tuples. Functions similar to dict.items().

        Returns
        -------
        dict_items
            A dict_items object of the HVSRData objects attributes, parameters, etc.
        &#34;&#34;&#34;                
        return self.params.items()

    def copy(self, type=&#39;shallow&#39;):
        &#34;&#34;&#34;Make a copy of the HVSRData object. Uses python copy module.
        
        Parameters
        ----------
        type : str {&#39;shallow&#39;, &#39;deep&#39;}
            Based on input, creates either a shallow or deep copy of the HVSRData object. Shallow is equivalent of copy.copy(). Input of type=&#39;deep&#39; is equivalent of copy.deepcopy() (still experimental). Defaults to shallow.
    
        &#34;&#34;&#34;
        if type.lower()==&#39;deep&#39;:
            return HVSRData(copy.deepcopy(self.params))
        else:
            return HVSRData(copy.copy(self.params))
        
    def plot(self, **kwargs):
        &#34;&#34;&#34;Method to plot data, wrapper of sprit.plot_hvsr()

        Returns
        -------
        matplotlib.Figure, matplotlib.Axis (if return_fig=True)
        &#34;&#34;&#34;
        if &#39;close_figs&#39; not in kwargs.keys():
            kwargs[&#39;close_figs&#39;]=True
        plot_return = plot_hvsr(self, **kwargs)
        plt.show()
        return plot_return
        
    def get_report(self, **kwargs):
        &#34;&#34;&#34;Method to get report from processed data, in print, graphical, or tabular format.

        Returns
        -------
        Variable
            May return nothing, pandas.Dataframe, or pyplot Figure, depending on input.
        &#34;&#34;&#34;
        report_return = get_report(self, **kwargs)
        return report_return

    def report(self, **kwargs):
        &#34;&#34;&#34;Wrapper of get_report()&#34;&#34;&#34;
        report_return = get_report(self, **kwargs)
        return report_return
    
    #ATTRIBUTES
    #params
    @property
    def params(self):
        &#34;&#34;&#34;Dictionary containing the parameters used to process the data

        Returns
        -------
        dict
            Dictionary containing the process parameters
        &#34;&#34;&#34;
        return self._params

    @params.setter
    def params(self, value):
        if not (isinstance(value, dict)):
            raise ValueError(&#34;params must be a dict type, currently passing {} type.&#34;.format(type(value)))
        self._params = value
    
    #datastream
    @property
    def datastream(self):
        &#34;&#34;&#34;A copy of the original obspy datastream read in. This helps to retain the original data even after processing is carried out.

        Returns
        -------
        obspy.core.Stream.stream
            Obspy stream
        &#34;&#34;&#34;
        return self._datastream

    @datastream.setter
    def datastream(self, value):
        if value is not None and (not isinstance(value, obspy.core.stream.Stream)):
            raise ValueError(&#34;datastream must be an obspy Stream.&#34;)
        self._datastream = value
        
    #batch
    @property
    def batch(self):
        &#34;&#34;&#34;Whether this HVSRData object is part of an HVSRBatch object. This is used throughout the code to help direct the object into the proper processing pipeline.

        Returns
        -------
        bool
            True if HVSRData object is part of HVSRBatch object, otherwise, False
        &#34;&#34;&#34;
        return self._batch

    @batch.setter
    def batch(self, value):
        if value == 0:
            value = False
        elif value == 1:
            value = True
        else:
            value = None
        if not isinstance(value, bool):
            raise ValueError(&#34;batch must be boolean type&#34;)
        self._batch = value

    #PPSD object from obspy (static)
    @property
    def ppsds_obspy(self):
        &#34;&#34;&#34;The original ppsd information from the obspy.signal.spectral_estimation.PPSD(), so as to keep original if copy is manipulated/changed.&#34;&#34;&#34;        
        return self._ppsds_obspy

    @ppsds_obspy.setter
    def ppsds_obspy(self, value):
        &#34;&#34;&#34;Checks whether the ppsd_obspy is of the proper type before saving as attribute&#34;&#34;&#34;
        if not isinstance(value, obspy.signal.spectral_estimation.PPSD):
            if not isinstance(value, dict):
                raise ValueError(&#34;ppsds_obspy must be obspy.PPSD or dict with osbpy.PPSDs&#34;)
            else:
                for key in value.keys():
                    if not isinstance(value[key], obspy.signal.spectral_estimation.PPSD):
                        raise ValueError(&#34;ppsds_obspy must be obspy.PPSD or dict with osbpy.PPSDs&#34;)
        self._ppsds_obspy=value
                        
    #PPSD dict, copied from obspy ppsds (dynamic)
    @property
    def ppsds(self):
        &#34;&#34;&#34;Dictionary copy of the class object obspy.signal.spectral_estimation.PPSD(). The dictionary copy allows manipulation of the data in PPSD, whereas that data cannot be easily manipulated in the original Obspy object.

        Returns
        -------
        dict
            Dictionary copy of the PPSD information from generate_ppsds()
        &#34;&#34;&#34;
        return self._ppsds

    @ppsds.setter
    def ppsds(self, value):
        if not isinstance(value, dict):
            raise ValueError(&#34;ppsds dict with infomration from osbpy.PPSD (created by sprit.generate_ppsds())&#34;)                  
        self._ppsds=value

#Launch the tkinter gui
def gui():
    &#34;&#34;&#34;Function to open a window with a graphical user interface (gui)
    
    No parameters, no returns; just opens the gui window.
    &#34;&#34;&#34;
    import pkg_resources
    #guiPath = pathlib.Path(os.path.realpath(__file__))
    try:
        from sprit.sprit_gui import SPRIT_App
    except:
        from sprit_gui import SPRIT_App
    
    try:
        import tkinter as tk
    except:
        if sys.platform == &#39;linux&#39;:
            raise ImportError(&#39;The SpRIT graphical interface uses tkinter, which ships with python but is not pre-installed on linux machines. Use &#34;apt-get install python-tk&#34; or &#34;apt-get install python3-tk&#34; to install tkinter. You may need to use the sudo command at the start of those commands.&#39;)

    def on_gui_closing():
        plt.close(&#39;all&#39;)
        gui_root.quit()
        gui_root.destroy()

    if sys.platform == &#39;linux&#39;:
        if not pathlib.Path(&#34;/usr/share/doc/python3-tk&#34;).exists():
            warnings.warn(&#39;The SpRIT graphical interface uses tkinter, which ships with python but is not pre-installed on linux machines. Use &#34;apt-get install python-tk&#34; or &#34;apt-get install python3-tk&#34; to install tkinter. You may need to use the sudo command at the start of those commands.&#39;)

    gui_root = tk.Tk()
    try:
        try:
            icon_path =pathlib.Path(pkg_resources.resource_filename(__name__, &#39;resources/icon/sprit_icon_alpha.ico&#39;)) 
            gui_root.iconbitmap(icon_path)
        except:
            icon_path = pathlib.Path(pkg_resources.resource_filename(__name__, &#39;resources/icon/sprit_icon.png&#39;))
            gui_root.iconphoto(False, tk.PhotoImage(file=icon_path.as_posix()))
    except Exception as e:
        print(&#34;ICON NOT LOADED, still opening GUI&#34;)

    gui_root.resizable(True, True)
    SPRIT_App(master=gui_root) #Open the app with a tk.Tk root

    gui_root.protocol(&#34;WM_DELETE_WINDOW&#34;, on_gui_closing)    
    gui_root.mainloop() #Run the main loop

#FUNCTIONS AND METHODS
#The run function to rule them all (runs all needed for simply processing HVSR)
def run(datapath, source=&#39;file&#39;, verbose=False, **kwargs):
    &#34;&#34;&#34;The sprit.run() is the main function that allows you to do all your HVSR processing in one simple step (sprit.run() is how you would call it in your code, but it may also be called using sprit.sprit_hvsr.run())
    
    The datapath parameter of sprit.run() is the only required parameter. This can be either a single file, a list of files (one for each component, for example), a directory (in which case, all obspy-readable files will be added to an HVSRBatch instance), a Rasp. Shake raw data directory, or sample data.
    
        The sprit.run() function calls the following functions. This is the recommended order/set of functions to run to process HVSR using SpRIT. See the API documentation for these functions for more information:
        - input_params(): The datapath parameter of input_params() is the only required variable, though others may also need to be called for your data to process correctly.
        - fetch_data(): the source parameter of fetch_data() is the only explicit variable in the sprit.run() function aside from datapath and verbose. Everything else gets delivered to the correct function via the kwargs dictionary
        - remove_noise(): by default, the kind of noise removal is remove_method=&#39;auto&#39;. See the remove_noise() documentation for more information. If remove_method is set to anything other than one of the explicit options in remove_noise, noise removal will not be carried out.
        - generate_ppsds(): generates ppsds for each component, which will be combined/used later. Any parameter of obspy.signal.spectral_estimation.PPSD() may also be read into this function.
        - process_hvsr(): this is the main function processing the hvsr curve and statistics. See process_hvsr() documentation for more details. The hvsr_band parameter sets the frequency spectrum over which these calculations occur.
        - check_peaks(): this is the main function that will find and &#39;score&#39; peaks to get a best peak. The parameter peak_freq_range can be set to limit the frequencies within which peaks are checked and scored.
        - get_report(): this is the main function that will print, plot, and/or save the results of the data. See the get_report() API documentation for more information.
        - export_data(): this function exports the final data output as a pickle file (by default, this pickle object has a .hvsr extension). This can be used to read data back into SpRIT without having to reprocess data.

    Parameters
    ----------
    datapath : str or filepath object that can be read by obspy
        Filepath to data to be processed. This may be a file or directory, depending on what kind of data is being processed (this can be specified with the source parameter). 
        For sample data, The following can be specified as the datapath parameter:
            - Any integer 1-6 (inclusive), or the string (e.g., datapath=&#34;1&#34; or datapath=1 will work)
            - The word &#34;sample&#34; before any integer (e.g., datapath=&#34;sample1&#34;)
            - The word &#34;sample&#34; will default to &#34;sample1&#34; if source=&#39;file&#39;. 
            - If source=&#39;batch&#39;, datapath should be datapath=&#39;sample&#39; or datapath=&#39;batch&#39;. In this case, it will read and process all the sample files using the HVSRBatch class. Set verbose=True to see all the information in the sample batch csv file.
    source : str, optional
        _description_, by default &#39;file&#39;
    verbose : bool, optional
        _description_, by default False
    **kwargs
        Keyword arguments for the functions listed above. The keyword arguments are unique, so they will get parsed out and passed into the appropriate function.

    Returns
    -------
    hvsr_results : sprit.HVSRData or sprit.HVSRBatch object
        If a single file/data point is being processed, a HVSRData object will be returned. Otherwise, it will be a HVSRBatch object. See their documention for more information.

    Raises
    ------
    RuntimeError
        If the input parameter may not be read correctly. This is raised if the input_params() function fails. This raises an error since no other data processing or reading steps will be able to carried out correctly.
    RuntimeError
        If the data is not read/fetched correctly using fetch_data(), an error will be raised. This is raised if the fetch_data() function fails. This raises an error since no other data processing steps will be able to carried out correctly.
    RuntimeError
        If the data being processed is a single file, an error will be raised if generate_ppsds() does not work correctly. No errors are raised for remove_noise() errors (since that is an optional step) and the process_hvsr() step (since that is the last processing step) .
    &#34;&#34;&#34;
    #Get the input parameters
    input_params_kwargs = {k: v for k, v in locals()[&#39;kwargs&#39;].items() if k in input_params.__code__.co_varnames}
    try:
        params = input_params(datapath=datapath, verbose=verbose, **input_params_kwargs)
    except:
        #Even if batch, this is reading in data for all sites so we want to raise error, not just warn
        raise RuntimeError(&#39;Input parameters not read correctly, see sprit.input_params() function and parameters&#39;)
        #If input_params fails, initialize params as an HVSRDATA
        params = {&#39;ProcessingStatus&#39;:{&#39;InputStatus&#39;:False, &#39;OverallStatus&#39;:False}}
        params.update(input_params_kwargs)
        params = sprit_utils.make_it_classy(params)

    #Fetch Data
    try:
        fetch_data_kwargs = {k: v for k, v in locals()[&#39;kwargs&#39;].items() if k in fetch_data.__code__.co_varnames}
        dataIN = fetch_data(params=params, source=source, verbose=verbose, **fetch_data_kwargs)    
    except:
        #Even if batch, this is reading in data for all sites so we want to raise error, not just warn
        raise RuntimeError(&#39;Data not read correctly, see sprit.fetch_data() function and parameters for more details.&#39;)
    
    #Remove Noise
    try:
        remove_noise_kwargs = {k: v for k, v in locals()[&#39;kwargs&#39;].items() if k in remove_noise.__code__.co_varnames}
        data_noiseRemoved = remove_noise(hvsr_data=dataIN, verbose=verbose,**remove_noise_kwargs)   
    except:
        data_noiseRemoved = dataIN
        
        #Reformat data so HVSRData and HVSRBatch data both work here
        if isinstance(data_noiseRemoved, HVSRData):
            data_noiseRemoved = {&#39;place_holder_sitename&#39;:data_noiseRemoved}
            dataIN = {&#39;place_holder_sitename&#39;:dataIN}
            
        for site_name in data_noiseRemoved.keys():
            data_noiseRemoved[site_name][&#39;ProcessingStatus&#39;][&#39;RemoveNoiseStatus&#39;]=False
            #Since noise removal is not required for data processing, check others first
            if dataIN[site_name][&#39;ProcessingStatus&#39;][&#39;OverallStatus&#39;]:
                data_noiseRemoved[site_name][&#39;ProcessingStatus&#39;][&#39;OverallStatus&#39;] = True        
            else:
                data_noiseRemoved[site_name][&#39;ProcessingStatus&#39;][&#39;OverallStatus&#39;] = False

            #If it wasn&#39;t originally HVSRBatch, make it HVSRData object again
            if not data_noiseRemoved[site_name][&#39;batch&#39;]:
                data_noiseRemoved = data_noiseRemoved[site_name]
    
    #Generate PPSDs
    try:
        generate_ppsds_kwargs = {k: v for k, v in locals()[&#39;kwargs&#39;].items() if k in generate_ppsds.__code__.co_varnames}
        PPSDkwargs = {k: v for k, v in locals()[&#39;kwargs&#39;].items() if k in PPSD.__init__.__code__.co_varnames}
        generate_ppsds_kwargs.update(PPSDkwargs)
        ppsd_data = generate_ppsds(params=data_noiseRemoved, verbose=verbose,**generate_ppsds_kwargs)
    except Exception as e:
        if source == &#39;file&#39; or source==&#39;raw&#39;:
            if hasattr(e, &#39;message&#39;):
                errMsg = e.message
            else:
                errMsg = e
            raise RuntimeError(f&#34;generate_ppsds() error: {errMsg}&#34;)

        #Reformat data so HVSRData and HVSRBatch data both work here
        ppsd_data = data_noiseRemoved
        if isinstance(ppsd_data, HVSRData):
            ppsd_data = {&#39;place_holder_sitename&#39;:ppsd_data}
            
        for site_name in ppsd_data.keys(): #This should work more or less the same for batch and regular data now
            ppsd_data[site_name][&#39;ProcessingStatus&#39;][&#39;PPSDStatus&#39;]=False
            ppsd_data[site_name][&#39;ProcessingStatus&#39;][&#39;OverallStatus&#39;] = False
    
            #If it wasn&#39;t originally HVSRBatch, make it HVSRData object again
            if not ppsd_data[site_name][&#39;batch&#39;]:
                ppsd_data = ppsd_data[site_name]
    
    #Process HVSR Curves
    try:
        process_hvsr_kwargs = {k: v for k, v in locals()[&#39;kwargs&#39;].items() if k in process_hvsr.__code__.co_varnames}
        hvsr_results = process_hvsr(params=ppsd_data, verbose=verbose,**process_hvsr_kwargs)
    except Exception as e:
        traceback.print_exception(sys.exc_info()[1])
        exc_type, exc_obj, tb = sys.exc_info()
        f = tb.tb_frame
        lineno = tb.tb_lineno
        filename = f.f_code.co_filename
        errLineNo = str(traceback.extract_tb(sys.exc_info()[2])[-1].lineno)
        error_category = type(e).__name__.title().replace(&#39;error&#39;, &#39;Error&#39;)
        error_message = f&#34;{e} ({errLineNo})&#34;
        print(f&#34;{error_category} ({errLineNo}): {error_message}&#34;)
        print(lineno, filename, f)

        hvsr_results = ppsd_data
        if isinstance(hvsr_results, HVSRData):
            hvsr_results = {&#39;place_holder_sitename&#39;:hvsr_results}
            
        for site_name in hvsr_results.keys(): #This should work more or less the same for batch and regular data now
        
            hvsr_results[site_name][&#39;ProcessingStatus&#39;][&#39;HVStatus&#39;]=False
            hvsr_results[site_name][&#39;ProcessingStatus&#39;][&#39;OverallStatus&#39;] = False
            
            #If it wasn&#39;t originally HVSRBatch, make it HVSRData object again
            if not hvsr_results[site_name][&#39;batch&#39;]:
                hvsr_results = hvsr_results[site_name]            
            
    #Final post-processing/reporting

    #Check peaks
    check_peaks_kwargs = {k: v for k, v in locals()[&#39;kwargs&#39;].items() if k in check_peaks.__code__.co_varnames}
    hvsr_results = check_peaks(hvsr_data=hvsr_results, verbose=verbose, **check_peaks_kwargs)

    get_report_kwargs = {k: v for k, v in locals()[&#39;kwargs&#39;].items() if k in get_report.__code__.co_varnames}
    get_report(hvsr_results=hvsr_results, verbose=verbose, **get_report_kwargs)

    if verbose:
        if &#39;report_format&#39; in get_report_kwargs.keys():
            #if report_format is &#39;print&#39;, we would have already printed it in previous step
            if get_report_kwargs[&#39;report_format&#39;]==&#39;print&#39; or &#39;print&#39; in get_report_kwargs[&#39;report_format&#39;] or isinstance(hvsr_results, HVSRBatch):
                #We do not need to print another report if already printed to terminal
                pass
            else:
                #We will just change the report_format kwarg to print, since we already got the originally intended report format above, 
                #   now need to print for verbose output
                get_report_kwargs[&#39;report_format&#39;]=&#39;print&#39;
                get_report(hvsr_results=hvsr_results, **get_report_kwargs)
                
            if get_report_kwargs[&#39;report_format&#39;]==&#39;plot&#39; or &#39;plot&#39; in get_report_kwargs[&#39;report_format&#39;]:
                #We do not need to plot another report if already plotted
                pass
            else:
                hvplot_kwargs = {k: v for k, v in locals()[&#39;kwargs&#39;].items() if k in plot_hvsr.__code__.co_varnames}
                hvsr_results[&#39;HV_Plot&#39;] = plot_hvsr(hvsr_results, **hvplot_kwargs)
        else:
            pass
    
    #Export processed data if export_path(as pickle currently, default .hvsr extension)
    if &#39;export_path&#39; in kwargs.keys():
        if kwargs[&#39;export_path&#39;] is None:
            pass
        else:
            if &#39;ext&#39; in kwargs.keys():
                ext = kwargs[&#39;ext&#39;]
            else:
                ext = &#39;hvsr&#39;
            export_data(hvsr_data=hvsr_results, export_path=kwargs[&#39;export_path&#39;], ext=ext, verbose=verbose)        

    return hvsr_results

#Quality checks, stability tests, clarity tests
#def check_peaks(hvsr, x, y, index_list, peak, peakm, peakp, hvsr_peaks, stdf, hvsr_log_std, rank, hvsr_band=[0.4, 40], do_rank=False):
def check_peaks(hvsr_data, hvsr_band=[0.4, 40], peak_freq_range=[1, 20], verbose=False):
    &#34;&#34;&#34;Function to run tests on HVSR peaks to find best one and see if it passes quality checks

        Parameters
        ----------
        hvsr_data : dict
            Dictionary containing all the calculated information about the HVSR data (i.e., hvsr_out returned from process_hvsr)
        hvsr_band : tuple or list, default=[0.4, 40]
            2-item tuple or list with lower and upper limit of frequencies to analyze
        peak_freq_range : tuple or list, default=[1, 20];
            The frequency range within which to check for peaks. If there is an HVSR curve with multiple peaks, this allows the full range of data to be processed while limiting peak picks to likely range.
        verbose : bool, default=False
            Whether to print results and inputs to terminal.
        
        Returns
        -------
        hvsr_data   : HVSRData or HVSRBatch object
            Object containing previous input data, plus information about peak tests
    &#34;&#34;&#34;
    orig_args = locals().copy() #Get the initial arguments

    if (verbose and &#39;input_params&#39; not in hvsr_data.keys()) or (verbose and not hvsr_data[&#39;batch&#39;]):
        if isinstance(hvsr_data, HVSRData) and hvsr_data[&#39;batch&#39;]:
            pass
        else:
            print(&#39;\nChecking peaks in the H/V Curve (check_peaks())&#39;)
            print(&#39;\tUsing the following parameters:&#39;)
            for key, value in orig_args.items():
                if key==&#39;hvsr_data&#39;:
                    pass
                else:
                    print(&#39;\t  {}={}&#39;.format(key, value))
            print()
  
    #First, divide up for batch or not
    if isinstance(hvsr_data, HVSRBatch):
        if verbose:
            print(&#39;\t  Running in batch mode&#39;)
        #If running batch, we&#39;ll loop through each site
        for site_name in hvsr_data.keys():
            args = orig_args.copy() #Make a copy so we don&#39;t accidentally overwrite
            args[&#39;hvsr_data&#39;] =  hvsr_data[site_name] #Get what would normally be the &#34;params&#34; variable for each site
            if hvsr_data[site_name][&#39;ProcessingStatus&#39;][&#39;OverallStatus&#39;]:
                try:
                    hvsr_data[site_name] = _check_peaks_batch(**args) #Call another function, that lets us run this function again
                except:
                    if verbose:
                        print(f&#34;\t{site_name}: check_peaks() unsuccessful. Peaks not checked.&#34;)
                    else:
                        warnings.warn(f&#34;\t{site_name}: check_peaks() unsuccessful. Peaks not checked.&#34;, RuntimeWarning)
                
        hvsr_data = HVSRBatch(hvsr_data)
    else:
        if hvsr_data[&#39;ProcessingStatus&#39;][&#39;OverallStatus&#39;]:
            if not hvsr_band:
                hvsr_band = [0.4,40]
            hvsr_data[&#39;hvsr_band&#39;] = hvsr_band

            anyK = list(hvsr_data[&#39;x_freqs&#39;].keys())[0]

            x = hvsr_data[&#39;x_freqs&#39;][anyK] #Consistent for all curves
            y = hvsr_data[&#39;hvsr_curve&#39;] #Calculated based on &#34;Use&#34; column
            index_list = hvsr_data[&#39;hvsr_peak_indices&#39;] #Calculated based on hvsr_curve
            hvsrp = hvsr_data[&#39;hvsrp&#39;] #Calculated based on &#34;Use&#34; column
            hvsrm = hvsr_data[&#39;hvsrm&#39;] #Calculated based on &#34;Use&#34; column

            hvsrPeaks = hvsr_data[&#39;hvsr_df&#39;][hvsr_data[&#39;hvsr_df&#39;][&#39;Use&#39;]][&#39;CurvesPeakIndices&#39;]
            #hvsrPeaks = hvsr_data[&#39;ind_hvsr_peak_indices&#39;] #Original calculation

            hvsr_log_std = hvsr_data[&#39;hvsr_log_std&#39;]
            peak_freq_range = hvsr_data[&#39;peak_freq_range&#39;]

            #Do for hvsr
            peak = __init_peaks(x, y, index_list, hvsr_band, peak_freq_range)

            peak = __check_curve_reliability(hvsr_data, peak)
            peak = __check_clarity(x, y, peak, do_rank=True)

            #Do for hvsrp
            # Find  the relative extrema of hvsrp (hvsr + 1 standard deviation)
            if not np.isnan(np.sum(hvsrp)):
                index_p = __find_peaks(hvsrp)
            else:
                index_p = list()

            peakp = __init_peaks(x, hvsrp, index_p, hvsr_band, peak_freq_range)
            peakp = __check_clarity(x, hvsrp, peakp, do_rank=True)

            #Do for hvsrm
            # Find  the relative extrema of hvsrm (hvsr - 1 standard deviation)
            if not np.isnan(np.sum(hvsrm)):
                index_m = __find_peaks(hvsrm)
            else:
                index_m = list()

            peakm = __init_peaks(x, hvsrm, index_m, hvsr_band, peak_freq_range)
            peakm = __check_clarity(x, hvsrm, peakm, do_rank=True)

            stdf = __get_stdf(x, index_list, hvsrPeaks)

            peak = __check_freq_stability(peak, peakm, peakp)
            peak = __check_stability(stdf, peak, hvsr_log_std, rank=True)

            hvsr_data[&#39;Peak Report&#39;] = peak

            #Iterate through peaks and 
            #   Get the BestPeak based on the peak score
            #   Calculate whether each peak passes enough tests
            curveTests = [&#39;WindowLengthFreq.&#39;,&#39;SignificantCycles&#39;, &#39;LowCurveStDevOverTime&#39;]
            peakTests = [&#39;PeakFreqClarityBelow&#39;, &#39;PeakFreqClarityAbove&#39;, &#39;PeakAmpClarity&#39;, &#39;FreqStability&#39;, &#39;PeakStability_FreqStD&#39;, &#39;PeakStability_AmpStD&#39;]
            bestPeakScore = 0

            for p in hvsr_data[&#39;Peak Report&#39;]:
                #Get BestPeak
                if p[&#39;Score&#39;] &gt; bestPeakScore:
                    bestPeakScore = p[&#39;Score&#39;]
                    bestPeak = p

                #Calculate if peak passes criteria
                cTestsPass = 0
                pTestsPass = 0
                for testName in p[&#39;PassList&#39;].keys():
                    if testName in curveTests:
                        if p[&#39;PassList&#39;][testName]:
                            cTestsPass += 1
                    elif testName in peakTests:
                        if p[&#39;PassList&#39;][testName]:
                            pTestsPass += 1

                if cTestsPass == 3 and pTestsPass &gt;= 5:
                    p[&#39;PeakPasses&#39;] = True
                else:
                    p[&#39;PeakPasses&#39;] = False
                
            #Designate BestPeak in output dict
            if len(hvsr_data[&#39;Peak Report&#39;]) == 0:
                bestPeak={}
                print(f&#34;No Best Peak identified for {hvsr_data[&#39;site&#39;]}&#34;)

            hvsr_data[&#39;BestPeak&#39;] = bestPeak
        else:
            hvsr_data[&#39;BestPeak&#39;] = {}
            print(f&#34;Processing Errors: No Best Peak identified for {hvsr_data[&#39;site&#39;]}&#34;)
            
    return hvsr_data

#Function to export data to file
def export_data(hvsr_data, export_path=None, ext=&#39;hvsr&#39;, verbose=False):
    &#34;&#34;&#34;Export data into pickle format that can be read back in using import_data() so data does not need to be processed each time. 
    Default extension is .hvsr but it is still a pickled file that can be read in using pickle.load().

    Parameters
    ----------
    hvsr_data : HVSRData or HVSRBatch
        Data to be exported
    export_path : str or filepath object, default = None
        String or filepath object to be read by pathlib.Path() and/or a with open(export_path, &#39;wb&#39;) statement. If None, defaults to input datapath directory, by default None
    ext : str, default = &#39;hvsr&#39;
        Filepath extension to use for data file, by default &#39;hvsr&#39;
    &#34;&#34;&#34;
    def _do_export(_hvsr_data=hvsr_data, _export_path=export_path, _ext=ext):
        
        fname = f&#34;{_hvsr_data.site}_{_hvsr_data.acq_date}_pickled.{ext}&#34;
        if _export_path is None or _export_path is True:
            _export_path = _hvsr_data[&#39;datapath&#39;]
            _export_path = pathlib.Path(_export_path).with_name(fname)
        else:
            _export_path = pathlib.Path(_export_path)
            if _export_path.is_dir():
                _export_path = _export_path.joinpath(fname)    

        _export_path = str(_export_path)
        with open(_export_path, &#39;wb&#39;) as f:
            pickle.dump(_hvsr_data, f) 
            
        if verbose:
            print(f&#34;Processed data exported as pickled data to: {_export_path} [~{round(float(pathlib.Path(_export_path).stat().st_size)/2**20,1)} Mb]&#34;)    
            
    if isinstance(hvsr_data, HVSRBatch):
        for sitename in hvsr_data.keys():
            _do_export(hvsr_data[sitename], export_path, ext)
    elif isinstance(hvsr_data, HVSRData):
        _do_export(hvsr_data, export_path, ext)
    else:
        print(&#34;Error in data export. Data must be either of type sprit.HVSRData or sprit.HVSRBatch&#34;)         
    return

#Reads in traces to obspy stream
def fetch_data(params, inv=None, source=&#39;file&#39;, trim_dir=None, export_format=&#39;mseed&#39;, detrend=&#39;spline&#39;, detrend_order=2, update_metadata=True, plot_input_stream=False, verbose=False, **kwargs):
    import warnings

    &#34;&#34;&#34;Fetch ambient seismic data from a source to read into obspy stream
        
        Parameters
        ----------
        params  : dict
            Dictionary containing all the necessary params to get data.
                Parameters defined using input_params() function.
        inv     : obspy inventory object, default=None
            Obspy inventory object containing metadata for instrument that collected data to be fetched. By default, the inventory object is read from params[&#39;inv&#39;], but this can be manually specified here too.
        source  : str, {&#39;raw&#39;, &#39;dir&#39;, &#39;file&#39;, &#39;batch&#39;}
            String indicating where/how data file was created. For example, if raw data, will need to find correct channels.
                &#39;raw&#39; finds raspberry shake data, from raw output copied using scp directly from Raspberry Shake, either in folder or subfolders; 
                &#39;dir&#39; is used if the day&#39;s 3 component files (currently Raspberry Shake supported only) are all 3 contained in a directory by themselves.
                &#39;file&#39; is used if the params[&#39;datapath&#39;] specified in input_params() is the direct filepath to a single file to be read directly into an obspy stream.
                &#39;batch&#39; is used to read a list or specified set of seismic files. 
                    Most commonly, a csv file can be read in with all the parameters. Each row in the csv is a separate file. Columns can be arranged by parameter.
        trim_dir : None or str or pathlib obj, default=None
            If None (or False), data is not trimmed in this function.
            Otherwise, this is the directory to save trimmed and exported data.
        export_format: str=&#39;mseed&#39;
            If trim_dir is not False, this is the format in which to save the data
        detrend : str or bool, default=&#39;spline&#39;
            If False, data is not detrended.
            Otherwise, this should be a string accepted by the type parameter of the obspy.core.trace.Trace.detrend method: https://docs.obspy.org/packages/autogen/obspy.core.trace.Trace.detrend.html
        detrend_order : int, default=2
            If detrend parameter is &#39;spline&#39; or &#39;polynomial&#39;, this is passed directly to the order parameter of obspy.core.trace.Trace.detrend method.
        update_metadata : bool, default=True
            Whether to update the metadata file, used primarily with Raspberry Shake data which uses a generic inventory file.
        plot_input_stream : bool, default=False
            Whether to plot the raw input stream. This plot includes a spectrogram (Z component) and the raw (with decimation for speed) plots of each component signal.
        verbose : bool, default=False
            Whether to print outputs and inputs to the terminal
        **kwargs
            Keywords arguments, primarily for &#39;batch&#39; and &#39;dir&#39; sources
            
        Returns
        -------
        params : HVSRData or HVSRBatch object
            Same as params parameter, but with an additional &#34;stream&#34; attribute with an obspy data stream with 3 traces: Z (vertical), N (North-south), and E (East-west)
        &#34;&#34;&#34;
    if source != &#39;batch&#39; and verbose:
        print(&#39;\nFetching data (fetch_data())&#39;)
        print()

    params = get_metadata(params, update_metadata=update_metadata, source=source)
    inv = params[&#39;inv&#39;]
    date=params[&#39;acq_date&#39;]

    #Cleanup for gui input
    if &#39;}&#39; in str(params[&#39;datapath&#39;]):
        params[&#39;datapath&#39;] = params[&#39;datapath&#39;].as_posix().replace(&#39;{&#39;,&#39;&#39;)
        params[&#39;datapath&#39;] = params[&#39;datapath&#39;].split(&#39;}&#39;)
    
    sampleListNos = [&#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;4&#39;, &#39;5&#39;, &#39;6&#39;]
    sampleList = [&#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;4&#39;, &#39;5&#39;, &#39;6&#39;, &#39;batch&#39;, &#39;sample&#39;, &#39;sample_batch&#39;]
    for s in sampleListNos:
        sampleList.append(f&#39;sample{s}&#39;)
        sampleList.append(f&#39;sample_{s}&#39;)

    #Make sure datapath is pointing to an actual file
    if isinstance(params[&#39;datapath&#39;],list):
        for i, d in enumerate(params[&#39;datapath&#39;]):
            params[&#39;datapath&#39;][i] = sprit_utils.checkifpath(str(d).strip())
    else:
        dPath = sprit_utils.checkifpath(params[&#39;datapath&#39;], sample_list=sampleList)

    inst = params[&#39;instrument&#39;]

    #Need to put dates and times in right formats first
    if type(date) is datetime.datetime:
        doy = date.timetuple().tm_yday
        year = date.year
    elif type(date) is datetime.date:
        date = datetime.datetime.combine(date, datetime.time(hour=0, minute=0, second=0))
        doy = date.timetuple().tm_yday
        year = date.year
    elif type(date) is tuple:
        if date[0]&gt;366:
            raise ValueError(&#39;First item in date tuple must be day of year (0-366)&#39;, 0)
        elif date[1] &gt; datetime.datetime.now().year:
            raise ValueError(&#39;Second item in date tuple should be year, but given item is in the future&#39;, 0)
        else:
            doy = date[0]
            year = date[1]
    elif type(date) is str:
        if &#39;/&#39; in date:
            dateSplit = date.split(&#39;/&#39;)            
        elif &#39;-&#39; in date:
            dateSplit = date.split(&#39;-&#39;)
        else:
            dateSplit = date

        if int(dateSplit[0]) &gt; 31:
            date = datetime.datetime(int(dateSplit[0]), int(dateSplit[1]), int(dateSplit[2]))
            doy = date.timetuple().tm_yday
            year = date.year
        elif int(dateSplit[0])&lt;=12 and int(dateSplit[2]) &gt; 31:
            warnings.warn(&#34;Preferred date format is &#39;yyyy-mm-dd&#39; or &#39;yyyy/mm/dd&#39;. Will attempt to parse date.&#34;)
            date = datetime.datetime(int(dateSplit[2]), int(dateSplit[0]), int(dateSplit[1]))
            doy = date.timetuple().tm_yday
            year = date.year
        else:
            warnings.warn(&#34;Preferred date format is &#39;yyyy-mm-dd&#39; or &#39;yyyy/mm/dd&#39;. Cannot parse date.&#34;)
    elif type(date) is int:
        doy = date
        year = datetime.datetime.today().year
    else: #FOR NOW, need to update
        date = datetime.datetime.now()
        doy = date.timetuple().tm_yday
        year = date.year
        warnings.warn(&#34;Did not recognize date, using year {} and day {}&#34;.format(year, doy))

    #Select which instrument we are reading from (requires different processes for each instrument)
    raspShakeInstNameList = [&#39;raspberry shake&#39;, &#39;shake&#39;, &#39;raspberry&#39;, &#39;rs&#39;, &#39;rs3d&#39;, &#39;rasp. shake&#39;, &#39;raspshake&#39;]

    #Select how reading will be done
    if source==&#39;raw&#39;:
        if inst.lower() in raspShakeInstNameList:
            try:
                rawDataIN = __read_RS_file_struct(dPath, source, year, doy, inv, params, verbose=verbose)
            except:
                raise RuntimeError(f&#34;Data not fetched for {params[&#39;site&#39;]}. Check input parameters or the data file.&#34;)
                return params
    elif source==&#39;dir&#39;:
        if inst.lower() in raspShakeInstNameList:
            rawDataIN = __read_RS_file_struct(dPath, source, year, doy, inv, params, verbose=verbose)
        else:
            obspyFiles = {}
            for obForm in obspyFormats:
                temp_file_glob = pathlib.Path(dPath.as_posix().lower()).glob(&#39;.&#39;+obForm.lower())
                for f in temp_file_glob:
                    currParams = params
                    currParams[&#39;datapath&#39;] = f
                    curr_data = fetch_data(params, source=&#39;file&#39;, #all the same as input, except just reading the one file using the source=&#39;file&#39;
                                trim_dir=trim_dir, export_format=export_format, detrend=detrend, detrend_order=detrend_order, update_metadata=update_metadata, verbose=verbose, **kwargs), 
                    obspyFiles[f.stem] = curr_data  #Add path object to dict, with filepath&#39;s stem as the site name
            return HVSRBatch(obspyFiles)
        
    elif source==&#39;file&#39; and str(params[&#39;datapath&#39;]).lower() not in sampleList:
        if isinstance(dPath, list) or isinstance(dPath, tuple):
            rawStreams = []
            for datafile in dPath:
                rawStream = obspy.read(datafile)
                rawStreams.append(rawStream) #These are actually streams, not traces
            
            for i, stream in enumerate(rawStreams):
                if i == 0:
                    rawDataIN = obspy.Stream(stream) #Just in case
                else:
                    rawDataIN = rawDataIN + stream #This adds a stream/trace to the current stream object
            
        elif str(dPath)[:6].lower()==&#39;sample&#39;:
            pass
        else:
            rawDataIN = obspy.read(dPath)#, starttime=obspy.core.UTCDateTime(params[&#39;starttime&#39;]), endttime=obspy.core.UTCDateTime(params[&#39;endtime&#39;]), nearest_sample =True)
        import warnings
        with warnings.catch_warnings():
            warnings.simplefilter(action=&#39;ignore&#39;, category=UserWarning)
            rawDataIN.attach_response(inv)
    elif source==&#39;batch&#39; and str(params[&#39;datapath&#39;]).lower() not in sampleList:
        if verbose:
            print(&#39;\nFetching data (fetch_data())&#39;)
        batch_data_read_kwargs = {k: v for k, v in locals()[&#39;kwargs&#39;].items() if k in batch_data_read.__code__.co_varnames}
        params = batch_data_read(input_data=params[&#39;datapath&#39;], verbose=verbose, **batch_data_read_kwargs)
        params = HVSRBatch(params)
        return params
    elif str(params[&#39;datapath&#39;]).lower() in sampleList or f&#34;sample{params[&#39;datapath&#39;].lower()}&#34; in sampleList:
        sample_data_dir = pathlib.Path(pkg_resources.resource_filename(__name__, &#39;resources/sample_data/&#39;))
        if source==&#39;batch&#39;:
            params[&#39;datapath&#39;] = sample_data_dir.joinpath(&#39;Batch_SampleData.csv&#39;)
            params = batch_data_read(input_data=params[&#39;datapath&#39;], batch_type=&#39;sample&#39;, verbose=verbose)
            params = HVSRBatch(params)
            return params

        elif source==&#39;dir&#39;:
            params[&#39;datapath&#39;] = sample_data_dir.joinpath(&#39;Batch_SampleData.csv&#39;)
            params = batch_data_read(input_data=params[&#39;datapath&#39;], batch_type=&#39;sample&#39;, verbose=verbose)
            params = HVSRBatch(params)
            return params

        elif source==&#39;file&#39;:
            params[&#39;datapath&#39;] = str(params[&#39;datapath&#39;]).lower()
            
            if params[&#39;datapath&#39;].lower() in sampleFileKeyMap.keys():
                params[&#39;datapath&#39;] = sampleFileKeyMap[params[&#39;datapath&#39;].lower()]
            else:
                params[&#39;datapath&#39;] = sample_data_dir.joinpath(&#39;SampleHVSRSite1_AM.RAC84.00.2023.046_2023-02-15_1704-1734.MSEED&#39;)

            dPath = params[&#39;datapath&#39;]
            rawDataIN = obspy.read(dPath)#, starttime=obspy.core.UTCDateTime(params[&#39;starttime&#39;]), endttime=obspy.core.UTCDateTime(params[&#39;endtime&#39;]), nearest_sample =True)
            import warnings
            with warnings.catch_warnings():
                warnings.simplefilter(action=&#39;ignore&#39;, category=UserWarning)
                rawDataIN.attach_response(inv)
    else:
        try:
            rawDataIN = obspy.read(dPath)
            rawDataIN.attach_response(inv)
        except:
            RuntimeError(f&#39;source={source} not recognized, and datapath cannot be read using obspy.read()&#39;)

    try:
        dataIN = rawDataIN.copy()
        if source!=&#39;raw&#39;:
            #Use metadata from file for;
            # site
            if params[&#39;site&#39;] == &#34;HVSR Site&#34;:
                params[&#39;site&#39;] = dPath.stem
                params[&#39;params&#39;][&#39;site&#39;] = dPath.stem
            
            # network
            if str(params[&#39;net&#39;]) == &#39;AM&#39;:
                params[&#39;net&#39;] = dataIN[0].stats.network
                params[&#39;params&#39;][&#39;net&#39;] = dataIN[0].stats.network

            # station
            if str(params[&#39;sta&#39;]) == &#39;RAC84&#39;:
                params[&#39;sta&#39;] = dataIN[0].stats.station
                params[&#39;params&#39;][&#39;sta&#39;] = dataIN[0].stats.station

            # loc
            if str(params[&#39;loc&#39;]) == &#39;00&#39;:
                params[&#39;loc&#39;] = dataIN[0].stats.location
                params[&#39;params&#39;][&#39;loc&#39;] = dataIN[0].stats.location
            
            # channels
            channelList = []
            if str(params[&#39;cha&#39;]) == [&#39;EHZ&#39;, &#39;EHN&#39;, &#39;EHE&#39;]:
                for tr in dataIN:
                    if tr.stats.channel not in channelList:
                        channelList.append(tr.stats.channel)
                        channelList.sort(reverse=True) #Just so z is first, just in case
                params[&#39;cha&#39;] = channelList
                params[&#39;params&#39;][&#39;cha&#39;] = channelList
           
            # Acquisition date
            if str(params[&#39;acq_date&#39;]) == str(datetime.datetime.now().date()):
                params[&#39;acq_date&#39;] = dataIN[0].stats.starttime.date

            # starttime
            today_Starttime = obspy.UTCDateTime(datetime.datetime(year=datetime.date.today().year, month=datetime.date.today().month,
                                                                 day = datetime.date.today().day,
                                                                hour=0, minute=0, second=0, microsecond=0))
            maxStarttime = datetime.time(hour=0, minute=0, second=0, microsecond=0)
            if str(params[&#39;starttime&#39;]) == str(today_Starttime):
                for tr in dataIN.merge():
                    currTime = datetime.time(hour=tr.stats.starttime.hour, minute=tr.stats.starttime.minute, 
                                       second=tr.stats.starttime.second, microsecond=tr.stats.starttime.microsecond)
                    if currTime &gt; maxStarttime:
                        maxStarttime = currTime

                newStarttime = obspy.UTCDateTime(datetime.datetime(year=params[&#39;acq_date&#39;].year, month=params[&#39;acq_date&#39;].month,
                                                                 day = params[&#39;acq_date&#39;].day,
                                                                hour=maxStarttime.hour, minute=maxStarttime.minute, 
                                                                second=maxStarttime.second, microsecond=maxStarttime.microsecond))
                params[&#39;starttime&#39;] = newStarttime
                params[&#39;params&#39;][&#39;starttime&#39;] = newStarttime

            # endttime
            today_Endtime = obspy.UTCDateTime(datetime.datetime(year=datetime.date.today().year, month=datetime.date.today().month,
                                                                 day = datetime.date.today().day,
                                                                hour=23, minute=59, second=59, microsecond=999999))
            minEndtime = datetime.time(hour=23, minute=59, second=59, microsecond=999999)
            if str(params[&#39;endtime&#39;]) == str(today_Endtime):
                for tr in dataIN.merge():
                    currTime = datetime.time(hour=tr.stats.endtime.hour, minute=tr.stats.endtime.minute, 
                                       second=tr.stats.endtime.second, microsecond=tr.stats.endtime.microsecond)
                    if currTime &lt; minEndtime:
                        minEndtime = currTime
                newEndtime = obspy.UTCDateTime(datetime.datetime(year=params[&#39;acq_date&#39;].year, month=params[&#39;acq_date&#39;].month,
                                                                 day = params[&#39;acq_date&#39;].day,
                                                                hour=minEndtime.hour, minute=minEndtime.minute, 
                                                                second=minEndtime.second, microsecond=minEndtime.microsecond))
                params[&#39;endtime&#39;] = newEndtime
                params[&#39;params&#39;][&#39;endtime&#39;] = newEndtime


            #print(dataIN)
            #print(params[&#39;starttime&#39;])
            #print(params[&#39;endtime&#39;])
            dataIN = dataIN.split()
            dataIN = dataIN.trim(starttime=params[&#39;starttime&#39;], endtime=params[&#39;endtime&#39;])
            dataIN.merge()
            #print(dataIN)
    except:
        raise RuntimeError(&#39;Data not fetched. Check your input parameters or the data file.&#39;)
        
    #Trim and save data as specified
    if not trim_dir:
        pass
    else:
        if isinstance(params, HVSRBatch):
            pass
        else:
            dataIN = _trim_data(input=params, stream=dataIN, export_dir=trim_dir, source=source, export_format=export_format)

    #Split data if masked array (if there are gaps)...detrending cannot be done without
    for tr in dataIN:
        if isinstance(tr.data, np.ma.masked_array):
            dataIN = dataIN.split()
            #Splits entire stream if any trace is masked_array
            break

    #Detrend data
    if isinstance(params, HVSRBatch):
        pass
    else:
        dataIN =  __detrend_data(input=dataIN, detrend=detrend, detrend_order=detrend_order, verbose=verbose, source=source)

    #Remerge data
    dataIN = dataIN.merge(method=1)

    if plot_input_stream:
        #dataIN.plot(method=&#39;full&#39;, linewidth=0.25)
        params[&#39;InputPlot&#39;] = _plot_specgram_stream(stream=dataIN, params=params, component=&#39;Z&#39;, stack_type=&#39;linear&#39;, detrend=&#39;mean&#39;, dbscale=True, fill_gaps=None, ylimstd=3, return_fig=True, fig=None, ax=None, show_plot=False)
        _get_removed_windows(input=dataIN, fig=params[&#39;InputPlot&#39;][0], ax=params[&#39;InputPlot&#39;][1], lineArtist =[], winArtist = [], existing_lineArtists=[], existing_xWindows=[], exist_win_format=&#39;matplotlib&#39;, keep_line_artists=True, time_type=&#39;matplotlib&#39;, show_plot=True)

        #Sort channels (make sure Z is first, makes things easier later)
    if isinstance(params, HVSRBatch):
        pass
    else:
        dataIN = _sort_channels(input=dataIN, source=source, verbose=verbose)

    if &#39;clean_ends&#39; not in kwargs.keys():
        clean_ends=True 
    else:
        clean_ends = kwargs[&#39;clean_ends&#39;]

    if clean_ends:
        maxStarttime = datetime.datetime.utcnow().replace(tzinfo=datetime.timezone.utc) - datetime.timedelta(days=36500) #100 years ago
        minEndtime = datetime.datetime.utcnow().replace(tzinfo=datetime.timezone.utc) 

        for tr in dataIN:
            currStarttime = datetime.datetime(year=tr.stats.starttime.year, month=tr.stats.starttime.month, day=tr.stats.starttime.day, 
                                         hour=tr.stats.starttime.hour, minute=tr.stats.starttime.minute, 
                                         second=tr.stats.starttime.second, microsecond=tr.stats.starttime.microsecond, tzinfo=datetime.timezone.utc)
            if currStarttime &gt; maxStarttime:
                maxStarttime = currStarttime

            currEndtime = datetime.datetime(year=tr.stats.endtime.year, month=tr.stats.endtime.month, day=tr.stats.endtime.day, 
                                         hour=tr.stats.endtime.hour, minute=tr.stats.endtime.minute, 
                                         second=tr.stats.endtime.second, microsecond=tr.stats.endtime.microsecond, tzinfo=datetime.timezone.utc)

            if currEndtime &lt; minEndtime:
                minEndtime = currEndtime


        maxStarttime = obspy.UTCDateTime(maxStarttime)
        minEndtime = obspy.UTCDateTime(minEndtime)
        dataIN = dataIN.split()
        for tr in dataIN:
            tr.trim(starttime=maxStarttime, endtime=minEndtime)
            pass
        dataIN.merge()
    
    params[&#39;batch&#39;] = False #Set False by default, will get corrected later in batch mode        
    params[&#39;input_stream&#39;] = dataIN
    params[&#39;stream&#39;] = dataIN.copy()
    params[&#39;ProcessingStatus&#39;][&#39;FetchDataStatus&#39;] = True
    if verbose and not isinstance(params, HVSRBatch):
        dataINStr = dataIN.__str__().split(&#39;\n&#39;)
        for line in dataINStr:
            print(&#39;\t&#39;,line)
    
    params = _check_processing_status(params)

    return params

#Generate PPSDs for each channel
def generate_ppsds(params, remove_outliers=True, outlier_std=3, verbose=False, **ppsd_kwargs):
    &#34;&#34;&#34;Generates PPSDs for each channel

        Channels need to be in Z, N, E order
        Info on PPSD creation here: https://docs.obspy.org/packages/autogen/obspy.signal.spectral_estimation.PPSD.html
        
        Parameters
        ----------
        params : dict, HVSRData object, or HVSRBatch object
            Data object containing all the parameters and other data of interest (stream and paz, for example)
        remove_outliers : bool, default=True
            Whether to remove outlier h/v curves. This is recommended, particularly if remove_noise() has been used.
        outlier_std :  float, default=3
            The standard deviation value to use as a threshold for determining whether a curve is an outlier. 
            This averages over the entire curve so that curves with very abberant data (often occurs when using the remove_noise() method), can be identified.
        verbose : bool, default=True
            Whether to print inputs and results to terminal
        **ppsd_kwargs : dict
            Dictionary with keyword arguments that are passed directly to obspy.signal.PPSD.
            If the following keywords are not specified, their defaults are amended in this function from the obspy defaults for its PPSD function. Specifically:
                - ppsd_length defaults to 60 (seconds) here instead of 3600
                - skip_on_gaps defaults to True instead of False
                - period_step_octaves defaults to 0.03125 instead of 0.125

        Returns
        -------
            ppsds : HVSRData object
                Dictionary containing entries with ppsds for each channel
    &#34;&#34;&#34;
    #First, divide up for batch or not
    orig_args = locals().copy() #Get the initial arguments

    ppsd_kwargs_sprit_defaults = ppsd_kwargs.copy()
    #Set defaults here that are different than obspy defaults
    if &#39;ppsd_length&#39; not in ppsd_kwargs:
        ppsd_kwargs_sprit_defaults[&#39;ppsd_length&#39;] = 30.0
    if &#39;skip_on_gaps&#39; not in ppsd_kwargs:
        ppsd_kwargs_sprit_defaults[&#39;skip_on_gaps&#39;] = True
    if &#39;period_step_octaves&#39; not in ppsd_kwargs:
        ppsd_kwargs_sprit_defaults[&#39;period_step_octaves&#39;] = 0.03125

    #Get Probablistic power spectral densities (PPSDs)
    #Get default args for function
    def get_default_args(func):
        signature = inspect.signature(func)
        return {
            k: v.default
            for k, v in signature.parameters.items()
            if v.default is not inspect.Parameter.empty
            }
    
    ppsd_kwargs = get_default_args(PPSD)
    ppsd_kwargs.update(ppsd_kwargs_sprit_defaults)#Update with sprit defaults, or user input

    orig_args[&#39;ppsd_kwargs&#39;] = [ppsd_kwargs]

    if (verbose and isinstance(params, HVSRBatch)) or (verbose and not params[&#39;batch&#39;]):
        if isinstance(params, HVSRData) and params[&#39;batch&#39;]:
            pass
        else:
            print(&#39;\nGenerating Probabilistic Power Spectral Densities (generate_ppsds())&#39;)
            print(&#39;\tUsing the following parameters:&#39;)
            for key, value in orig_args.items():
                if key==&#39;params&#39;:
                    pass
                else:
                    print(&#39;\t  {}={}&#39;.format(key, value))
            print()
    
    #Site is in the keys anytime it&#39;s not batch
    if isinstance(params, HVSRBatch):
        #If running batch, we&#39;ll loop through each one
        for site_name in params.keys():
            args = orig_args.copy() #Make a copy so we don&#39;t accidentally overwrite
            individual_params = params[site_name] #Get what would normally be the &#34;params&#34; variable for each site
            args[&#39;params&#39;] = individual_params #reset the params parameter we originally read in to an individual site params
            #args[&#39;params&#39;][&#39;batch&#39;] = False #Set to false, since only running this time
            if params[site_name][&#39;ProcessingStatus&#39;][&#39;OverallStatus&#39;]:
                try:
                    params[site_name] = _generate_ppsds_batch(**args) #Call another function, that lets us run this function again
                except:
                    params[site_name][&#39;ProcessingStatus&#39;][&#39;PPSDStatus&#39;]=False
                    params[site_name][&#39;ProcessingStatus&#39;][&#39;OverallStatus&#39;] = False                     
            else:
                params[site_name][&#39;ProcessingStatus&#39;][&#39;PPSDStatus&#39;]=False
                params[site_name][&#39;ProcessingStatus&#39;][&#39;OverallStatus&#39;] = False                
        return params
    else:
        paz=params[&#39;paz&#39;]
        stream = params[&#39;stream&#39;]

        #Get ppsds of e component
        eStream = stream.select(component=&#39;E&#39;)
        estats = eStream.traces[0].stats
        ppsdE = PPSD(estats, paz[&#39;E&#39;],  **ppsd_kwargs)
        ppsdE.add(eStream)

        #Get ppsds of n component
        nStream = stream.select(component=&#39;N&#39;)
        nstats = nStream.traces[0].stats
        ppsdN = PPSD(nstats, paz[&#39;N&#39;], **ppsd_kwargs)
        ppsdN.add(nStream)

        #Get ppsds of z component
        zStream = stream.select(component=&#39;Z&#39;)
        zstats = zStream.traces[0].stats
        ppsdZ = PPSD(zstats, paz[&#39;Z&#39;], **ppsd_kwargs)
        ppsdZ.add(zStream)

        ppsds = {&#39;Z&#39;:ppsdZ, &#39;N&#39;:ppsdN, &#39;E&#39;:ppsdE}

        #Add to the input dictionary, so that some items can be manipulated later on, and original can be saved
        params[&#39;ppsds_obspy&#39;] = ppsds
        params[&#39;ppsds&#39;] = {}
        anyKey = list(params[&#39;ppsds_obspy&#39;].keys())[0]
        
        #Get ppsd class members
        members = [mems for mems in dir(params[&#39;ppsds_obspy&#39;][anyKey]) if not callable(mems) and not mems.startswith(&#34;_&#34;)]
        params[&#39;ppsds&#39;][&#39;Z&#39;] = {}
        params[&#39;ppsds&#39;][&#39;E&#39;] = {}
        params[&#39;ppsds&#39;][&#39;N&#39;] = {}
        
        #Get lists/arrays so we can manipulate data later and copy everything over to main &#39;ppsds&#39; subdictionary (convert lists to np.arrays for consistency)
        listList = [&#39;times_data&#39;, &#39;times_gaps&#39;, &#39;times_processed&#39;,&#39;current_times_used&#39;, &#39;psd_values&#39;] #Things that need to be converted to np.array first, for consistency
        timeKeys= [&#39;times_processed&#39;,&#39;current_times_used&#39;,&#39;psd_values&#39;]
        timeDiffWarn = True
        dfList = []
        time_data = {}
        time_dict = {}
        for m in members:
            params[&#39;ppsds&#39;][&#39;Z&#39;][m] = getattr(params[&#39;ppsds_obspy&#39;][&#39;Z&#39;], m)
            params[&#39;ppsds&#39;][&#39;E&#39;][m] = getattr(params[&#39;ppsds_obspy&#39;][&#39;E&#39;], m)
            params[&#39;ppsds&#39;][&#39;N&#39;][m] = getattr(params[&#39;ppsds_obspy&#39;][&#39;N&#39;], m)
            if m in listList:
               
                params[&#39;ppsds&#39;][&#39;Z&#39;][m] = np.array(params[&#39;ppsds&#39;][&#39;Z&#39;][m])
                params[&#39;ppsds&#39;][&#39;E&#39;][m] = np.array(params[&#39;ppsds&#39;][&#39;E&#39;][m])
                params[&#39;ppsds&#39;][&#39;N&#39;][m] = np.array(params[&#39;ppsds&#39;][&#39;N&#39;][m])
            
            if str(m)==&#39;times_processed&#39;:
                unique_times = np.unique(np.array([params[&#39;ppsds&#39;][&#39;Z&#39;][m],
                                          params[&#39;ppsds&#39;][&#39;E&#39;][m],
                                          params[&#39;ppsds&#39;][&#39;N&#39;][m]]))
                
                common_times = []
                for currTime in unique_times:
                    if currTime in params[&#39;ppsds&#39;][&#39;Z&#39;][m]:
                        if currTime in params[&#39;ppsds&#39;][&#39;E&#39;][m]:
                            if currTime in params[&#39;ppsds&#39;][&#39;N&#39;][m]:
                                common_times.append(currTime)

                cTimeIndList = []
                for cTime in common_times:
                    ZArr = params[&#39;ppsds&#39;][&#39;Z&#39;][m]
                    EArr = params[&#39;ppsds&#39;][&#39;E&#39;][m]
                    NArr = params[&#39;ppsds&#39;][&#39;N&#39;][m]

                    cTimeIndList.append([int(np.where(ZArr == cTime)[0][0]),
                                        int(np.where(EArr == cTime)[0][0]),
                                        int(np.where(NArr == cTime)[0][0])])
                    
            #Make sure number of time windows is the same between PPSDs (this can happen with just a few slightly different number of samples)
            if m in timeKeys:
                if str(m) != &#39;times_processed&#39;:
                    time_data[str(m)] = (params[&#39;ppsds&#39;][&#39;Z&#39;][m], params[&#39;ppsds&#39;][&#39;E&#39;][m], params[&#39;ppsds&#39;][&#39;N&#39;][m])

                #print(m, params[&#39;ppsds&#39;][&#39;Z&#39;][m])

                tSteps_same = params[&#39;ppsds&#39;][&#39;Z&#39;][m].shape[0] == params[&#39;ppsds&#39;][&#39;E&#39;][m].shape[0] == params[&#39;ppsds&#39;][&#39;N&#39;][m].shape[0]

                if not tSteps_same:
                    shortestTimeLength = min(params[&#39;ppsds&#39;][&#39;Z&#39;][m].shape[0], params[&#39;ppsds&#39;][&#39;E&#39;][m].shape[0], params[&#39;ppsds&#39;][&#39;N&#39;][m].shape[0])

                    maxPctDiff = 0
                    for comp in params[&#39;ppsds&#39;].keys():
                        currCompTimeLength = params[&#39;ppsds&#39;][comp][m].shape[0]
                        timeLengthDiff = currCompTimeLength - shortestTimeLength
                        percentageDiff = timeLengthDiff / currCompTimeLength
                        if percentageDiff &gt; maxPctDiff:
                            maxPctDiff = percentageDiff

                    for comp in params[&#39;ppsds&#39;].keys():
                        while params[&#39;ppsds&#39;][comp][m].shape[0] &gt; shortestTimeLength:
                            params[&#39;ppsds&#39;][comp][m] = params[&#39;ppsds&#39;][comp][m][:-1]
                    
                    
                    if maxPctDiff &gt; 0.05 and timeDiffWarn:
                        warnings.warn(f&#34;\t  Number of ppsd time windows between different components is significantly different: {round(maxPctDiff*100,2)}% &gt; 5%. Last windows will be trimmed.&#34;)
                    elif verbose  and timeDiffWarn:
                        print(f&#34;\t  Number of ppsd time windows between different components is different by {round(maxPctDiff*100,2)}%. Last window(s) of components with larger number of ppsd windows will be trimmed.&#34;)
                    timeDiffWarn = False #So we only do this warning once, even though there are multiple arrays that need to be trimmed

        for i, currTStep in enumerate(cTimeIndList):
            colList = []
            currTStepList = []
            colList.append(&#39;TimesProcessed_Obspy&#39;)
            currTStepList.append(common_times[i])
            for tk in time_data.keys():
                colList.append(str(tk)+&#39;_Z&#39;)
                colList.append(str(tk)+&#39;_E&#39;)
                colList.append(str(tk)+&#39;_N&#39;)
                currTStepList.append(time_data[tk][0][currTStep[0]])#z
                currTStepList.append(time_data[tk][1][currTStep[1]])#e
                currTStepList.append(time_data[tk][2][currTStep[2]])#n

            dfList.append(currTStepList)
            
        hvsrDF = pd.DataFrame(dfList, columns=colList)
        hvsrDF[&#39;TimesProcessed_ObspyEnd&#39;] = hvsrDF[&#39;TimesProcessed_Obspy&#39;] + ppsd_kwargs[&#39;ppsd_length&#39;]
        
        #Add other times (for start times)
        def convert_to_datetime(obspyUTCDateTime):
            return obspyUTCDateTime.datetime.replace(tzinfo=datetime.timezone.utc)

        def convert_to_mpl_dates(obspyUTCDateTime):
            return obspyUTCDateTime.matplotlib_date

        hvsrDF[&#39;TimesProcessed&#39;] = hvsrDF[&#39;TimesProcessed_Obspy&#39;].apply(convert_to_datetime)     
        hvsrDF[&#39;TimesProcessed_End&#39;] = hvsrDF[&#39;TimesProcessed&#39;] + datetime.timedelta(days=0,seconds=ppsd_kwargs[&#39;ppsd_length&#39;]) 
        hvsrDF[&#39;TimesProcessed_MPL&#39;] = hvsrDF[&#39;TimesProcessed_Obspy&#39;].apply(convert_to_mpl_dates)
        hvsrDF[&#39;TimesProcessed_MPLEnd&#39;] = hvsrDF[&#39;TimesProcessed_MPL&#39;] + (ppsd_kwargs[&#39;ppsd_length&#39;]/86400)
        
        hvsrDF[&#39;Use&#39;] = True
        for gap in params[&#39;ppsds&#39;][&#39;Z&#39;][&#39;times_gaps&#39;]:
            hvsrDF[&#39;Use&#39;] = (hvsrDF[&#39;TimesProcessed_Obspy&#39;].gt(gap[0]) &amp; hvsrDF[&#39;TimesProcessed_Obspy&#39;].gt(gap[1]) )| \
                                (hvsrDF[&#39;TimesProcessed_ObspyEnd&#39;].lt(gap[0]) &amp; hvsrDF[&#39;TimesProcessed_ObspyEnd&#39;].lt(gap[1]))# | \

        hvsrDF.set_index(&#39;TimesProcessed&#39;, inplace=True)
        params[&#39;hvsr_df&#39;] = hvsrDF
        #Create dict entry to keep track of how many outlier hvsr curves are removed (2-item list with [0]=current number, [1]=original number of curves)
        params[&#39;tsteps_used&#39;] = [hvsrDF[&#39;Use&#39;].sum(), hvsrDF[&#39;Use&#39;].shape[0]]
        #params[&#39;tsteps_used&#39;] = [params[&#39;ppsds&#39;][&#39;Z&#39;][&#39;times_processed&#39;].shape[0], params[&#39;ppsds&#39;][&#39;Z&#39;][&#39;times_processed&#39;].shape[0]]
        
        #Remove outlier ppsds (those derived from data within the windows to be removed)

        if remove_outliers and &#39;xwindows_out&#39; in params.keys():
            params = remove_outlier_curves(params, outlier_std=outlier_std, ppsd_length=ppsd_kwargs[&#39;ppsd_length&#39;])
        params[&#39;tsteps_used&#39;][0] = params[&#39;ppsds&#39;][&#39;Z&#39;][&#39;current_times_used&#39;].shape[0]
        
        params = sprit_utils.make_it_classy(params)
    
    params[&#39;ProcessingStatus&#39;][&#39;PPSDStatus&#39;] = True
    params = _check_processing_status(params)
    return params

#Gets the metadata for Raspberry Shake, specifically for 3D v.7
def get_metadata(params, write_path=&#39;&#39;, update_metadata=True, source=None):
    &#34;&#34;&#34;Get metadata and calculate or get paz parameter needed for PPSD

    Parameters
    ----------
    params : dict
        Dictionary containing all the input and other parameters needed for processing
            Ouput from input_params() function
    write_path : str
        String with output filepath of where to write updated inventory or metadata file
            If not specified, does not write file 

    Returns
    -------
    params : dict
        Modified input dictionary with additional key:value pair containing paz dictionary (key = &#34;paz&#34;)
    &#34;&#34;&#34;
    
    invPath = params[&#39;metapath&#39;]
    raspShakeInstNameList = [&#39;raspberry shake&#39;, &#39;shake&#39;, &#39;raspberry&#39;, &#39;rs&#39;, &#39;rs3d&#39;, &#39;rasp. shake&#39;, &#39;raspshake&#39;]
    if params[&#39;instrument&#39;].lower() in raspShakeInstNameList:
        if update_metadata:
            params = _update_shake_metadata(filepath=invPath, params=params, write_path=write_path)
        params = _read_RS_Metadata(params, source=source)
    else:
        warnings.warn(&#39;{} not currently supported\n Returning input params dictionary.&#39;.format(params[&#39;instrument&#39;]))
        return params
    return params

#Get or print report
def get_report(hvsr_results, report_format=&#39;print&#39;, plot_type=&#39;HVSR p ann C+ p ann Spec&#39;, export_path=None, return_results=False, csv_overwrite_opt=&#39;append&#39;, no_output=False, verbose=False):    
    &#34;&#34;&#34;Print a report of the HVSR analysis (not currently implemented)
        
    Parameters
    ----------
    hvsr_results : dict
        Dictionary containing all the information about the processed hvsr data
    report_format : {&#39;csv&#39;, &#39;print&#39;, plot}
        Format in which to print or export the report.
    plot_type : str, default = &#39;HVSR p ann C+ p ann Spec
        What type of plot to plot, if &#39;plot&#39; part of report_format input
    export_path : None, bool, or filepath, default = None
        If None or False, does not export; if True, will export to same directory as the datapath parameter in the input_params() function.
        Otherwise, it should be a string or path object indicating where to export results. May be a file or directory.
        If a directory is specified, the filename will be  &#34;&lt;site_name&gt;_&lt;acq_date&gt;_&lt;UTC start time&gt;-&lt;UTC end time&gt;&#34;. The suffix defaults to png for report_format=&#34;plot&#34;, csv for &#39;csv&#39;, and does not export if &#39;print.&#39;
    return_results : bool, default=False
        Whether to return results. The following report_formats return the following items:
            &#39;plot&#39;- str
            &#39;print&#39; - matplotlib.Figure object
            &#39;csv&#39; - pandas.DataFrame object
            list/tuple - a list or tuple of the above objects, in the same order they are in the report_format list
    csv_overwrite_opts : str, {&#39;append&#39;, &#39;overwrite&#39;, &#39;keep/rename&#39;}
        How to handle csv report outputs if the designated csv output file already exists. By default, appends the new information to the end of the existing file.
    no_output : bool, default=False
        If True, only reads output to appropriate attribute of data class (ie, print does not print, only reads text into variable). If False, performs as normal.
    verbose : bool, default=True
        Whether to print the results to terminal. This is the same output as report_format=&#39;print&#39;, and will not repeat if that is already selected

    Returns
    -------
    If return_results=True, the following report_formats return the following items:
        &#39;plot&#39;-  matplotlib.Figure object
        &#39;print&#39; - str
        &#39;csv&#39; - pandas.DataFrame object
        list/tuple - a list or tuple of the above objects, in the same order they are in the report_format list

    &#34;&#34;&#34;
    #print statement
    #Check if results are good
    #Curve pass?
    orig_args = locals().copy() #Get the initial arguments

    if (verbose and isinstance(hvsr_results, HVSRBatch)) or (verbose and not hvsr_results[&#39;batch&#39;]):
        if isinstance(hvsr_results, HVSRData) and hvsr_results[&#39;batch&#39;]:
            pass
        else:
            print(&#39;\nGetting HVSR Report: get_report()&#39;)
            print(&#39;\tUsing the following parameters:&#39;)
            for key, value in orig_args.items():
                if key==&#39;params&#39;:
                    pass
                else:
                    print(&#39;\t  {}={}&#39;.format(key, value))
            print()

    if isinstance(hvsr_results, HVSRBatch):
        if verbose:
            print(&#39;\nGetting Reports: Running in batch mode&#39;)

            print(&#39;\tUsing parameters:&#39;)
            for key, value in orig_args.items():
                print(f&#39;\t  {key}={value}&#39;)    
            print()
        #If running batch, we&#39;ll loop through each site
        for site_name in hvsr_results.keys():
            args = orig_args.copy() #Make a copy so we don&#39;t accidentally overwrite
            individual_params = hvsr_results[site_name] #Get what would normally be the &#34;params&#34; variable for each site
            args[&#39;hvsr_results&#39;] = individual_params #reset the params parameter we originally read in to an individual site params
            if hvsr_results[site_name][&#39;ProcessingStatus&#39;][&#39;OverallStatus&#39;]:
                try:
                    hvsr_results[site_name] = _get_report_batch(**args) #Call another function, that lets us run this function again
                except:
                    hvsr_results[site_name] = hvsr_results[site_name]
            else:
                hvsr_results[site_name] = hvsr_results[site_name]
        
        combined_csvReport = pd.DataFrame()
        for site_name in hvsr_results.keys():
            if &#39;CSV_Report&#39; in hvsr_results[site_name].keys():
                combined_csvReport = pd.concat([combined_csvReport, hvsr_results[site_name][&#39;CSV_Report&#39;]], ignore_index=True, join=&#39;inner&#39;)
        
        if export_path is not None:
            if export_path is True:
                if pathlib.Path(hvsr_results[&#39;input_params&#39;][&#39;datapath&#39;]) in sampleFileKeyMap.values():
                    csvExportPath = pathlib.Path(os.getcwd())
                else:
                    csvExportPath = pathlib.Path(hvsr_results[&#39;input_params&#39;][&#39;datapath&#39;])
            elif pathlib.Path(export_path).is_dir():
                csvExportPath = export_path
            elif pathlib.Path(export_path).is_file():
                csvExportPath = export_path.parent
            else:
                csvExportPath = pathlib.Path(hvsr_results[site_name].datapath)
                if csvExportPath.is_dir():
                    pass
                else:
                    csvExportPath = csvExportPath.parent
                
            combined_csvReport.to_csv(csvExportPath, index=False)
        
        if return_results:
            return hvsr_results
    else:
        #if &#39;BestPeak&#39; in hvsr_results.keys() and &#39;PassList&#39; in hvsr_results[&#39;BestPeak&#39;].keys():
        try:
            curvTestsPassed = (hvsr_results[&#39;BestPeak&#39;][&#39;PassList&#39;][&#39;WindowLengthFreq.&#39;] +
                                hvsr_results[&#39;BestPeak&#39;][&#39;PassList&#39;][&#39;SignificantCycles&#39;]+
                                hvsr_results[&#39;BestPeak&#39;][&#39;PassList&#39;][&#39;LowCurveStDevOverTime&#39;])
            curvePass = curvTestsPassed &gt; 2
            
            #Peak Pass?
            peakTestsPassed = ( hvsr_results[&#39;BestPeak&#39;][&#39;PassList&#39;][&#39;PeakFreqClarityBelow&#39;] +
                        hvsr_results[&#39;BestPeak&#39;][&#39;PassList&#39;][&#39;PeakFreqClarityAbove&#39;]+
                        hvsr_results[&#39;BestPeak&#39;][&#39;PassList&#39;][&#39;PeakAmpClarity&#39;]+
                        hvsr_results[&#39;BestPeak&#39;][&#39;PassList&#39;][&#39;FreqStability&#39;]+
                        hvsr_results[&#39;BestPeak&#39;][&#39;PassList&#39;][&#39;PeakStability_FreqStD&#39;]+
                        hvsr_results[&#39;BestPeak&#39;][&#39;PassList&#39;][&#39;PeakStability_AmpStD&#39;])
            peakPass = peakTestsPassed &gt;= 5
        except Exception as e:
            errMsg= &#39;No BestPeak identified. Check peak_freq_range or hvsr_band or try to remove bad noise windows using remove_noise() or change processing parameters in process_hvsr() or generate_ppsds(). Otherwise, data may not be usable for HVSR.&#39;
            print(errMsg)
            print(e)
            return hvsr_results
            #raise RuntimeError(&#39;No BestPeak identified. Check peak_freq_range or hvsr_band or try to remove bad noise windows using remove_noise() or change processing parameters in process_hvsr() or generate_ppsds(). Otherwise, data may not be usable for HVSR.&#39;)
    
        if isinstance(report_format, (list, tuple)):
            pass
        else:
            #We will use a loop later even if it&#39;s just one report type, so reformat to prepare for for loop
            allList = [&#39;:&#39;, &#39;all&#39;]
            if report_format.lower() in allList:
                report_format = [&#39;print&#39;, &#39;csv&#39;, &#39;plot&#39;]
            else:
                report_format = [report_format]   

        def export_report(export_obj, _export_path, _rep_form):
            if _export_path is None:
                return
            else:
                if _rep_form == &#39;csv&#39;:
                    ext = &#39;.csv&#39;
                elif _rep_form ==&#39;plot&#39;:
                    ext=&#39;.png&#39;
                else:
                    ext=&#39;&#39;
                    
                sitename=hvsr_results[&#39;input_params&#39;][&#39;site&#39;]#.replace(&#39;.&#39;, &#39;-&#39;)
                fname = f&#34;{sitename}_{hvsr_results[&#39;input_params&#39;][&#39;acq_date&#39;]}_{str(hvsr_results[&#39;input_params&#39;][&#39;starttime&#39;].time)[:5]}-{str(hvsr_results[&#39;input_params&#39;][&#39;endtime&#39;].time)[:5]}{ext}&#34;
                fname = fname.replace(&#39;:&#39;, &#39;&#39;)

                if _export_path==True:
                    #Check so we don&#39;t write in sample directory
                    if pathlib.Path(hvsr_results[&#39;input_params&#39;][&#39;datapath&#39;]) in sampleFileKeyMap.values():
                        if pathlib.Path(os.getcwd()) in sampleFileKeyMap.values(): #Just in case current working directory is also sample directory
                            inFile = pathlib.Path.home() #Use the path to user&#39;s home if all else fails
                        else:
                            inFile = pathlib.Path(os.getcwd())
                    else:
                        inFile = pathlib.Path(hvsr_results[&#39;input_params&#39;][&#39;datapath&#39;])
                                 
                    if inFile.is_dir():
                        outFile = inFile.joinpath(fname)
                    else:
                        outFile = inFile.with_name(fname)
                else:
                    if pathlib.Path(_export_path).is_dir():
                        outFile = pathlib.Path(_export_path).joinpath(fname)
                    else:
                        outFile=pathlib.Path(_export_path)

            if _rep_form == &#39;csv&#39;:
                if outFile.exists():
                    existFile = pd.read_csv(outFile)
                    if csv_overwrite_opt.lower() == &#39;append&#39;:
                        export_obj = pd.concat([existFile, export_obj], ignore_index=True, join=&#39;inner&#39;)
                    elif csv_overwrite_opt.lower() == &#39;overwrite&#39;:
                        pass
                    else:# csv_overwrite_opt.lower() in [&#39;keep&#39;, &#39;rename&#39;]:
                        fileNameExists = True
                        i=1
                        while fileNameExists:
                            outFile = outFile.with_stem(f&#34;{outFile.stem}_{i}&#34;)
                            i+=1
                            if not outFile.exists():
                                fileNameExists = False
                try:
                    print(f&#39;\nSaving csv data to: {outFile}&#39;)
                    export_obj.to_csv(outFile, index_label=&#39;ID&#39;)
                except:
                    warnings.warn(&#34;Report not exported. \n\tDataframe to be exported as csv has been saved in hvsr_results[&#39;BestPeak&#39;][&#39;Report&#39;][&#39;CSV_Report]&#34;, category=RuntimeWarning)
            elif _rep_form ==&#39;plot&#39;:
                if verbose:
                    print(f&#39;\nSaving plot to: {outFile}&#39;)
                plt.scf = export_obj
                plt.savefig(outFile)
            return 

        def report_output(_report_format, _plot_type=&#39;HVSR p ann C+ p ann Spec&#39;, _return_results=False, _export_path=None, _no_output=False, verbose=False):
            if _report_format==&#39;print&#39;:
                #Print results

                #Make separators for nicely formatted print output
                sepLen = 99
                siteSepSymbol = &#39;=&#39;
                intSepSymbol = u&#34;\u2013&#34;
                extSepSymbol = u&#34;\u2014&#34;
                
                if sepLen % 2 == 0:
                    remainVal = 1
                else:
                    remainVal = 0

                siteWhitespace = 2
                #Format the separator lines internal to each site
                internalSeparator = intSepSymbol.center(sepLen-4, intSepSymbol).center(sepLen, &#39; &#39;)

                extSiteSeparator = &#34;&#34;.center(sepLen, extSepSymbol)
                siteSeparator = f&#34;{hvsr_results[&#39;input_params&#39;][&#39;site&#39;]}&#34;.center(sepLen - siteWhitespace, &#39; &#39;).center(sepLen, siteSepSymbol)
                endSiteSeparator = &#34;&#34;.center(sepLen, siteSepSymbol)

                #Start building list to print
                report_string_list = []
                report_string_list.append(&#34;&#34;) #Blank line to start
                report_string_list.append(extSiteSeparator)
                report_string_list.append(siteSeparator)
                report_string_list.append(extSiteSeparator)
                #report_string_list.append(internalSeparator)
                report_string_list.append(&#39;&#39;)
                report_string_list.append(f&#34;\tSite Name: {hvsr_results[&#39;input_params&#39;][&#39;site&#39;]}&#34;)
                report_string_list.append(f&#34;\tAcq. Date: {hvsr_results[&#39;input_params&#39;][&#39;acq_date&#39;]}&#34;)
                report_string_list.append(f&#34;\tLocation : {hvsr_results[&#39;input_params&#39;][&#39;longitude&#39;]}, {hvsr_results[&#39;input_params&#39;][&#39;latitude&#39;]}&#34;)
                report_string_list.append(f&#34;\tElevation: {hvsr_results[&#39;input_params&#39;][&#39;elevation&#39;]}&#34;)
                report_string_list.append(&#39;&#39;)
                report_string_list.append(internalSeparator)
                report_string_list.append(&#39;&#39;)
                if &#39;BestPeak&#39; not in hvsr_results.keys():
                    report_string_list.append(&#39;\tNo identifiable BestPeak was present between {} for {}&#39;.format(hvsr_results[&#39;input_params&#39;][&#39;hvsr_band&#39;], hvsr_results[&#39;input_params&#39;][&#39;site&#39;]))
                else:
                    report_string_list.append(&#39;\t{0:.3f} Hz Peak Frequency&#39;.format(hvsr_results[&#39;BestPeak&#39;][&#39;f0&#39;]))        
                    if curvePass and peakPass:
                        report_string_list.append(&#39;\t  {} Curve at {} Hz passed quality checks! â˜º :D&#39;.format(sprit_utils.check_mark(), round(hvsr_results[&#39;BestPeak&#39;][&#39;f0&#39;],3)))
                    else:
                        report_string_list.append(&#39;\t  {} Peak at {} Hz did NOT pass quality checks â˜¹:(&#39;.format(sprit_utils.x_mark(), round(hvsr_results[&#39;BestPeak&#39;][&#39;f0&#39;],3)))            
                    report_string_list.append(&#39;&#39;)
                    report_string_list.append(internalSeparator)
                    report_string_list.append(&#39;&#39;)

                    #Print individual results
                    report_string_list.append(&#39;\tCurve Tests: {}/3 passed (3/3 needed)&#39;.format(curvTestsPassed))
                    report_string_list.append(f&#34;\t\t {hvsr_results[&#39;BestPeak&#39;][&#39;Report&#39;][&#39;Lw&#39;][-1]} Length of processing windows: {hvsr_results[&#39;BestPeak&#39;][&#39;Report&#39;][&#39;Lw&#39;]}&#34;)
                    report_string_list.append(f&#34;\t\t {hvsr_results[&#39;BestPeak&#39;][&#39;Report&#39;][&#39;Nc&#39;][-1]} Number of significant cycles: {hvsr_results[&#39;BestPeak&#39;][&#39;Report&#39;][&#39;Nc&#39;]}&#34;)
                    report_string_list.append(f&#34;\t\t {hvsr_results[&#39;BestPeak&#39;][&#39;Report&#39;][&#39;Ïƒ_A(f)&#39;][-1]} Low StDev. of H/V Curve over time: {hvsr_results[&#39;BestPeak&#39;][&#39;Report&#39;][&#39;Ïƒ_A(f)&#39;]}&#34;)

                    report_string_list.append(&#39;&#39;)
                    report_string_list.append(&#34;\tPeak Tests: {}/6 passed (5/6 needed)&#34;.format(peakTestsPassed))
                    report_string_list.append(f&#34;\t\t {hvsr_results[&#39;BestPeak&#39;][&#39;Report&#39;][&#39;A(f-)&#39;][-1]} Clarity Below Peak Frequency: {hvsr_results[&#39;BestPeak&#39;][&#39;Report&#39;][&#39;A(f-)&#39;]}&#34;)
                    report_string_list.append(f&#34;\t\t {hvsr_results[&#39;BestPeak&#39;][&#39;Report&#39;][&#39;A(f+)&#39;][-1]} Clarity Above Peak Frequency: {hvsr_results[&#39;BestPeak&#39;][&#39;Report&#39;][&#39;A(f+)&#39;]}&#34;)
                    report_string_list.append(f&#34;\t\t {hvsr_results[&#39;BestPeak&#39;][&#39;Report&#39;][&#39;A0&#39;][-1]} Clarity of Peak Amplitude: {hvsr_results[&#39;BestPeak&#39;][&#39;Report&#39;][&#39;A0&#39;]}&#34;)
                    if hvsr_results[&#39;BestPeak&#39;][&#39;PassList&#39;][&#39;FreqStability&#39;]:
                        res = sprit_utils.check_mark()
                    else:
                        res = sprit_utils.x_mark()
                    report_string_list.append(f&#34;\t\t {res} Stability of Peak Freq. Over time: {hvsr_results[&#39;BestPeak&#39;][&#39;Report&#39;][&#39;P-&#39;][:5]} and {hvsr_results[&#39;BestPeak&#39;][&#39;Report&#39;][&#39;P+&#39;][:-1]} {res}&#34;)
                    report_string_list.append(f&#34;\t\t {hvsr_results[&#39;BestPeak&#39;][&#39;Report&#39;][&#39;Sf&#39;][-1]} Stability of Peak (Freq. StDev): {hvsr_results[&#39;BestPeak&#39;][&#39;Report&#39;][&#39;Sf&#39;]}&#34;)
                    report_string_list.append(f&#34;\t\t {hvsr_results[&#39;BestPeak&#39;][&#39;Report&#39;][&#39;Sa&#39;][-1]} Stability of Peak (Amp. StDev): {hvsr_results[&#39;BestPeak&#39;][&#39;Report&#39;][&#39;Sa&#39;]}&#34;)
                report_string_list.append(&#39;&#39;)
                report_string_list.append(f&#34;Calculated using {hvsr_results[&#39;hvsr_df&#39;][&#39;Use&#39;].sum()}/{hvsr_results[&#39;hvsr_df&#39;][&#39;Use&#39;].count()} time windows&#34;.rjust(sepLen-1))
                report_string_list.append(extSiteSeparator)
                #report_string_list.append(endSiteSeparator)
                #report_string_list.append(extSiteSeparator)
                report_string_list.append(&#39;&#39;)
                
                reportStr=&#39;&#39;
                #Now print it
                for line in report_string_list:
                    reportStr = reportStr+&#39;\n&#39;+line

                if not _no_output:
                    print(reportStr)

                export_report(export_obj=reportStr, _export_path=_export_path, _rep_form=_report_format)
                hvsr_results[&#39;BestPeak&#39;][&#39;Report&#39;][&#39;Print_Report&#39;] = reportStr
                hvsr_results[&#39;Print_Report&#39;] = reportStr

            elif _report_format==&#39;csv&#39;:
                import pandas as pd
                pdCols = [&#39;Site Name&#39;, &#39;Acq_Date&#39;, &#39;Longitude&#39;, &#39;Latitide&#39;, &#39;Elevation&#39;, &#39;PeakFrequency&#39;, 
                        &#39;WindowLengthFreq.&#39;,&#39;SignificantCycles&#39;,&#39;LowCurveStDevOverTime&#39;,
                        &#39;PeakFreqClarityBelow&#39;,&#39;PeakFreqClarityAbove&#39;,&#39;PeakAmpClarity&#39;,&#39;FreqStability&#39;, &#39;PeakStability_FreqStD&#39;,&#39;PeakStability_AmpStD&#39;, &#39;PeakPasses&#39;]
                d = hvsr_results
                criteriaList = []
                for p in hvsr_results[&#39;BestPeak&#39;][&#34;PassList&#34;]:
                    criteriaList.append(hvsr_results[&#39;BestPeak&#39;][&#34;PassList&#34;][p])
                criteriaList.append(hvsr_results[&#39;BestPeak&#39;][&#34;PeakPasses&#34;])
                dfList = [[d[&#39;input_params&#39;][&#39;site&#39;], d[&#39;input_params&#39;][&#39;acq_date&#39;], d[&#39;input_params&#39;][&#39;longitude&#39;], d[&#39;input_params&#39;][&#39;latitude&#39;], d[&#39;input_params&#39;][&#39;elevation&#39;], round(d[&#39;BestPeak&#39;][&#39;f0&#39;], 3)]]
                dfList[0].extend(criteriaList)
                outDF = pd.DataFrame(dfList, columns=pdCols)

                if verbose:
                    print(&#39;\nCSV Report:\n&#39;)
                    maxColWidth = 13
                    print(&#39;  &#39;, end=&#39;&#39;)
                    for col in outDF.columns:
                        if len(str(col)) &gt; maxColWidth:
                            colStr = str(col)[:maxColWidth-3]+&#39;...&#39;
                        else:
                            colStr = str(col)
                        print(colStr.ljust(maxColWidth), end=&#39;  &#39;)
                    print() #new line
                    for c in range(len(outDF.columns) * (maxColWidth+2)):
                        if c % (maxColWidth+2) == 0:
                            print(&#39;|&#39;, end=&#39;&#39;)
                        else:
                            print(&#39;-&#39;, end=&#39;&#39;)
                    print(&#39;|&#39;) #new line
                    print(&#39;  &#39;, end=&#39;&#39;) #Small indent at start                    
                    for row in outDF.iterrows():
                        for col in row[1]:
                            if len(str(col)) &gt; maxColWidth:
                                colStr = str(col)[:maxColWidth-3]+&#39;...&#39;
                            else:
                                colStr = str(col)
                            print(colStr.ljust(maxColWidth), end=&#39;  &#39;)
                        print()

                try:
                    export_report(export_obj=outDF, _export_path=_export_path, _rep_form=_report_format)
                except:
                    print(&#34;Error in exporting csv report. CSV not exported&#34;)
                hvsr_results[&#39;BestPeak&#39;][&#39;Report&#39;][&#39;CSV_Report&#39;] = outDF
                hvsr_results[&#39;CSV_Report&#39;] = outDF
                        
            elif _report_format==&#39;plot&#39;:
                fig_ax = plot_hvsr(hvsr_results, plot_type=_plot_type, show=False, return_fig=True)

                export_report(export_obj=fig_ax[0], _export_path=_export_path, _rep_form=_report_format)
                hvsr_results[&#39;BestPeak&#39;][&#39;Report&#39;][&#39;HV_Plot&#39;]=hvsr_results[&#39;HV_Plot&#39;]=fig_ax

                print(&#39;\nPlot of data report:&#39;)
                plt.show()
                
            return hvsr_results

        for i, rep_form in enumerate(report_format):
            if isinstance(export_path, (list, tuple)):
                if not isinstance(report_format, (list, tuple)):
                    warnings.warn(&#39;export_path is a list/tuple and report_format is not. This may result in unexpected behavior.&#39;)
                if isinstance(report_format, (list, tuple)) and isinstance(export_path, (list, tuple)) and len(report_format) != len(export_path):
                    warnings.warn(&#39;export_path and report_format are both lists or tuples, but they are not the same length. This may result in unexpected behavior.&#39;)
            
                exp_path = export_path[i]
            else:
                exp_path = export_path
            hvsr_results = report_output(_report_format=rep_form, _plot_type=plot_type, _return_results=return_results, _export_path=exp_path, _no_output=no_output, verbose=verbose)
    return hvsr_results

#Main function for plotting results
def plot_hvsr(hvsr_data, plot_type=&#39;HVSR ann p C+ ann p SPEC&#39;, use_subplots=True, xtype=&#39;freq&#39;, fig=None, ax=None, return_fig=False,  save_dir=None, save_suffix=&#39;&#39;, show_legend=False, show=True, close_figs=False, clear_fig=True,**kwargs):
    &#34;&#34;&#34;Function to plot HVSR data

    Parameters
    ----------
    hvsr_data : dict                  
        Dictionary containing output from process_hvsr function
    plot_type : str=&#39;HVSR&#39; or list    
        The plot_type of plot(s) to plot. If list, will plot all plots listed
        &#39;HVSR&#39; : Standard HVSR plot, including standard deviation
        - &#39;[HVSR] p&#39; : HVSR plot with BestPeaks shown
        - &#39;[HVSR] p&#39; : HVSR plot with best picked peak shown                
        - &#39;[HVSR] p* all&#39; : HVSR plot with all picked peaks shown                
        - &#39;[HVSR] p* t&#39; : HVSR plot with peaks from all time steps in background                
        - &#39;[HVSR p* ann] : Annotates plot with peaks
        - &#39;[HVSR] -s&#39; : HVSR plots don&#39;t show standard deviation
        - &#39;[HVSR] t&#39; : HVSR plot with individual hv curves for each time step shown
        - &#39;[HVSR] c&#39; : HVSR plot with each components&#39; spectra. Recommended to do this last (or just before &#39;specgram&#39;), since doing c+ can make the component chart its own chart
        &#39;Specgram&#39; : Combined spectrogram of all components
        - &#39;[spec]&#39; : basic spectrogram plot of H/V curve
    use_subplots : bool, default = True
        Whether to output the plots as subplots (True) or as separate plots (False)
    xtype : str, default = &#39;freq&#39;    
        String for what to use, between frequency or period
            For frequency, the following are accepted (case does not matter): &#39;f&#39;, &#39;Hz&#39;, &#39;freq&#39;, &#39;frequency&#39;
            For period, the following are accepted (case does not matter): &#39;p&#39;, &#39;T&#39;, &#39;s&#39;, &#39;sec&#39;, &#39;second&#39;, &#39;per&#39;, &#39;period&#39;
    fig : matplotlib.Figure, default = None
        If not None, matplotlib figure on which plot is plotted
    ax : matplotlib.Axis, default = None
        If not None, matplotlib axis on which plot is plotted
    return_fig : bool
        Whether to return figure and axis objects
    save_dir : str or None
        Directory in which to save figures
    save_suffix : str
        Suffix to add to end of figure filename(s), if save_dir is used
    show_legend : bool, default=False
        Whether to show legend in plot
    show : bool
        Whether to show plot
    close_figs : bool, default=False
        Whether to close figures before plotting
    clear_fig : bool, default=True
        Whether to clear figures before plotting
    **kwargs : keyword arguments
        Keyword arguments for matplotlib.pyplot

    Returns
    -------
    fig, ax : matplotlib figure and axis objects
        Returns figure and axis matplotlib.pyplot objects if return_fig=True, otherwise, simply plots the figures
    &#34;&#34;&#34;
    orig_args = locals().copy() #Get the initial arguments
    if isinstance(hvsr_data, HVSRBatch):
        #If running batch, we&#39;ll loop through each site
        for site_name in hvsr_data.keys():
            args = orig_args.copy() #Make a copy so we don&#39;t accidentally overwrite
            individual_params = hvsr_data[site_name] #Get what would normally be the &#34;params&#34; variable for each site
            args[&#39;hvsr_results&#39;] = individual_params #reset the params parameter we originally read in to an individual site params
            if hvsr_data[site_name][&#39;ProcessingStatus&#39;][&#39;OverallStatus&#39;]:
                try:
                    _hvsr_plot_batch(**args) #Call another function, that lets us run this function again
                except:
                    print(f&#34;{site_name} not able to be plotted.&#34;)
    else:
        if clear_fig and fig is not None and ax is not None: #Intended use for tkinter
            #Clear everything
            for key in ax:
                ax[key].clear()
            fig.clear()
        if close_figs:
            plt.close(&#39;all&#39;)

        compList = [&#39;c&#39;, &#39;comp&#39;, &#39;component&#39;, &#39;components&#39;]
        specgramList = [&#39;spec&#39;, &#39;specgram&#39;, &#39;spectrogram&#39;]
        hvsrList = [&#39;hvsr&#39;, &#39;hv&#39;, &#39;h&#39;]

        hvsrInd = np.nan
        compInd = np.nan
        specInd = np.nan

        kList = plot_type.split(&#39; &#39;)
        for i, k in enumerate(kList):
            kList[i] = k.lower()

        #Get the plots in the right order, no matter how they were input (and ensure the right options go with the right plot)
        #HVSR index
        if len(set(hvsrList).intersection(kList)):
            for i, hv in enumerate(hvsrList):
                if hv in kList:
                    hvsrInd = kList.index(hv)
                    break
        #Component index
        #if len(set(compList).intersection(kList)):
        for i, c in enumerate(kList):
            if &#39;+&#39; in c and c[:-1] in compList:
                compInd = kList.index(c)
                break
            
        #Specgram index
        if len(set(specgramList).intersection(kList)):
            for i, sp in enumerate(specgramList):
                if sp in kList:
                    specInd = kList.index(sp)
                    break        

        indList = [hvsrInd, compInd, specInd]
        indListCopy = indList.copy()
        plotTypeList = [&#39;hvsr&#39;, &#39;comp&#39;, &#39;spec&#39;]

        plotTypeOrder = []
        plotIndOrder = []

        lastVal = 0
        while lastVal != 99:
            firstInd = np.nanargmin(indListCopy)
            plotTypeOrder.append(plotTypeList[firstInd])
            plotIndOrder.append(indList[firstInd])
            lastVal = indListCopy[firstInd]
            indListCopy[firstInd] = 99 #just a high number

        plotTypeOrder.pop()
        plotIndOrder[-1]=len(kList)

        for i, p in enumerate(plotTypeOrder):
            pStartInd = plotIndOrder[i]
            pEndInd = plotIndOrder[i+1]
            plotComponents = kList[pStartInd:pEndInd]

            if use_subplots and i==0 and fig is None and ax is None:
                mosaicPlots = []
                for pto in plotTypeOrder:
                    mosaicPlots.append([pto])
                fig, ax = plt.subplot_mosaic(mosaicPlots, gridspec_kw={&#39;hspace&#39;:0.3})
                axis = ax[p]
            elif use_subplots:
                with warnings.catch_warnings():
                    warnings.simplefilter(&#34;ignore&#34;) #Often warns about xlim when it is not an issue
                    ax[p].clear()
                axis = ax[p]
            else:
                fig, axis = plt.subplots()
                    
            if p == &#39;hvsr&#39;:
                _plot_hvsr(hvsr_data, fig=fig, ax=axis, plot_type=plotComponents, xtype=&#39;x_freqs&#39;, show_legend=show_legend, axes=ax)
            elif p==&#39;comp&#39;:
                plotComponents[0] = plotComponents[0][:-1]
                _plot_hvsr(hvsr_data, fig=fig, ax=axis, plot_type=plotComponents, xtype=&#39;x_freqs&#39;, show_legend=show_legend, axes=ax)
            elif p==&#39;spec&#39;:
                _plot_specgram_hvsr(hvsr_data, fig=fig, ax=axis, colorbar=False)
            else:
                warnings.warn(&#39;Plot type {p} not recognized&#39;, UserWarning)   

        if show:
            fig.canvas.draw()
            
        if return_fig:
            return fig, ax
    return

#Import data
def import_data(import_filepath, data_format=&#39;pickle&#39;):
    &#34;&#34;&#34;Function to import .hvsr (or other extension) data exported using export_data() function

    Parameters
    ----------
    import_filepath : str or path object
        Filepath of file created using export_data() function. This is usually a pickle file with a .hvsr extension
    data_format : str, default=&#39;pickle&#39;
        Type of format data is in. Currently, only &#39;pickle&#39; supported. Eventually, json or other type may be supported, by default &#39;pickle&#39;.

    Returns
    -------
    HVSRData or HVSRBatch object
    &#34;&#34;&#34;
    if data_format==&#39;pickle&#39;:
        with open(import_filepath, &#39;rb&#39;) as f:
            dataIN = pickle.load(f)
    else:
        dataIN = import_filepath
    return dataIN

#Define input parameters
def input_params(datapath,
                site=&#39;HVSR Site&#39;,
                network=&#39;AM&#39;, 
                station=&#39;RAC84&#39;, 
                loc=&#39;00&#39;, 
                channels=[&#39;EHZ&#39;, &#39;EHN&#39;, &#39;EHE&#39;],
                acq_date=str(datetime.datetime.now().date()),
                starttime = &#39;00:00:00.00&#39;,
                endtime = &#39;23:59:59.999999&#39;,
                tzone = &#39;UTC&#39;,
                xcoord = -88.2290526,
                ycoord =  40.1012122,
                elevation = 755,
                input_crs=&#39;EPSG:4326&#39;,#4269 is NAD83, defautling to WGS
                output_crs=&#39;EPSG:4326&#39;,
                elev_unit = &#39;feet&#39;,
                depth = 0,
                instrument = &#39;Raspberry Shake&#39;,
                metapath = &#39;&#39;,
                hvsr_band = [0.4, 40],
                peak_freq_range=[0.4, 40],
                verbose=False
                ):
    &#34;&#34;&#34;Function for designating input parameters for reading in and processing data
    
    Parameters
    ----------
    datapath : str or pathlib.Path object
        Filepath of data. This can be a directory or file, but will need to match with what is chosen later as the source parameter in fetch_data()
    site : str, default=&#34;HVSR Site&#34;
        Site name as designated by user for ease of reference. Used for plotting titles, filenames, etc.
    network : str, default=&#39;AM&#39;
        The network designation of the seismometer. This is necessary for data from Raspberry Shakes. &#39;AM&#39; is for Amateur network, which fits Raspberry Shakes.
    station : str, default=&#39;RAC84&#39;
        The station name of the seismometer. This is necessary for data from Raspberry Shakes.
    loc : str, default=&#39;00&#39;
        Location information of the seismometer.
    channels : list, default=[&#39;EHZ&#39;, &#39;EHN&#39;, &#39;EHE&#39;]
        The three channels used in this analysis, as a list of strings. Preferred that Z component is first, but not necessary
    acq_date : str, int, date object, or datetime object
        If string, preferred format is &#39;YYYY-MM-DD&#39;. 
        If int, this will be interpreted as the time_int of year of current year (e.g., 33 would be Feb 2 of current year)
        If date or datetime object, this will be the date. Make sure to account for time change when converting to UTC (if UTC is the following time_int, use the UTC time_int).
    starttime : str, time object, or datetime object, default=&#39;00:00:00.00&#39;
        Start time of data stream. This is necessary for Raspberry Shake data in &#39;raw&#39; form, or for trimming data. Format can be either &#39;HH:MM:SS.micros&#39; or &#39;HH:MM&#39; at minimum.
    endtime : str, time obejct, or datetime object, default=&#39;23:59:99.99&#39;
        End time of data stream. This is necessary for Raspberry Shake data in &#39;raw&#39; form, or for trimming data. Same format as starttime.
    tzone : str or int, default = &#39;UTC&#39;
        Timezone of input data. If string, &#39;UTC&#39; will use the time as input directly. Any other string value needs to be a TZ identifier in the IANA database, a wikipedia page of these is available here: https://en.wikipedia.org/wiki/List_of_tz_database_time_zones.
        If int, should be the int value of the UTC offset (e.g., for American Eastern Standard Time: -5). 
        This is necessary for Raspberry Shake data in &#39;raw&#39; format.
    xcoord : float, default=-88.2290526
        Longitude (or easting, or, generally, X coordinate) of data point, in Coordinate Reference System (CRS) designated by input_crs. Currently only used in csv output, but will likely be used in future for mapping/profile purposes.
    ycoord : float, default=40.1012122
        Latitute (or northing, or, generally, X coordinate) of data point, in Coordinate Reference System (CRS) designated by input_crs. Currently only used in csv output, but will likely be used in future for mapping/profile purposes.
    input_crs : str or other format read by pyproj, default=&#39;EPSG:4326&#39;
        Coordinate reference system of input data, as used by pyproj.CRS.from_user_input()
    output_crs : str or other format read by pyproj, default=&#39;EPSG:4326&#39;
        Coordinate reference system to which input data will be transformed, as used by pyproj.CRS.from_user_input()
    elevation : float, default=755
        Surface elevation of data point. Not currently used (except in csv output), but will likely be used in the future.
    depth : float, default=0
        Depth of seismometer. Not currently used, but will likely be used in the future.
    instrument : str or list {&#39;Raspberry Shake&#39;)
        Instrument from which the data was acquired. 
    metapath : str or pathlib.Path object, default=&#39;&#39;
        Filepath of metadata, in format supported by obspy.read_inventory. If default value of &#39;&#39;, will read from resources folder of repository (only supported for Raspberry Shake).
    hvsr_band : list, default=[0.4, 40]
        Two-element list containing low and high &#34;corner&#34; frequencies (in Hz) for processing. This can specified again later.
    peak_freq_range : list or tuple, default=[0.4, 40]
        Two-element list or tuple containing low and high frequencies (in Hz) that are used to check for HVSR Peaks. This can be a tigher range than hvsr_band, but if larger, it will still only use the hvsr_band range.
    verbose : bool, default=False
        Whether to print output and results to terminal

    Returns
    -------
    params : sprit.HVSRData
        sprit.HVSRData class containing input parameters, including data file path and metadata path. This will be used as an input to other functions. If batch processing, params will be converted to batch type in fetch_data() step.

    &#34;&#34;&#34;
    orig_args = locals().copy() #Get the initial arguments

    #Declare obspy here instead of at top of file for (for example) colab, where obspy first needs to be installed on environment
    global obspy
    import obspy
    if verbose:
        print(&#39;Gathering input parameters (input_params())&#39;)
        for key, value in orig_args.items():
            print(&#39;\t  {}={}&#39;.format(key, value))
        print()

    #Make Sure metapath is all good
    if not pathlib.Path(metapath).exists() or metapath==&#39;&#39;:
        if metapath == &#39;&#39;:
            pass
        else:
            print(&#39;Specified metadata file cannot be read!&#39;)
        repoDir = pathlib.Path(os.path.dirname(__file__))
        metapath = pathlib.Path(pkg_resources.resource_filename(__name__, &#39;resources/rs3dv5plus_metadata.inv&#39;))
        #print(&#39;Using default metadata file for Raspberry Shake v.7 distributed with package&#39;)
    else:
        if isinstance(metapath, pathlib.PurePath):
            metapath = metapath.as_posix()
        else:
            metapath = pathlib.Path(metapath).as_posix()

    #Reformat times
    if type(acq_date) is datetime.datetime:
        date = str(acq_date.date())
    elif type(acq_date) is datetime.date:
        date=str(acq_date)
    elif type(acq_date) is str:
        monthStrs = {&#39;jan&#39;:1, &#39;january&#39;:1,
                    &#39;feb&#39;:2, &#39;february&#39;:2,
                    &#39;mar&#39;:3, &#39;march&#39;:3,
                    &#39;apr&#39;:4, &#39;april&#39;:4,
                    &#39;may&#39;:5,
                    &#39;jun&#39;:6, &#39;june&#39;:6,
                    &#39;jul&#39;:7, &#39;july&#39;:7,
                    &#39;aug&#39;:8, &#39;august&#39;:8,
                    &#39;sep&#39;:9, &#39;sept&#39;:9, &#39;september&#39;:9,
                    &#39;oct&#39;:10,&#39;october&#39;:10, 
                    &#39;nov&#39;:11,&#39;november&#39;:11,
                    &#39;dec&#39;:12,&#39;december&#39;:12}

        spelledMonth = False
        for m in monthStrs.keys():
            acq_date = acq_date.lower()
            if m in acq_date:
                spelledMonth = True
                break

        if spelledMonth is not False:
            month = monthStrs[m]

        if &#39;/&#39; in acq_date:
            sep = &#39;/&#39;
        elif &#39;.&#39; in acq_date:
            sep=&#39;.&#39;
        elif &#39; &#39; in acq_date:
            sep = &#39; &#39;
            acq_date = acq_date.replace(&#39;,&#39;, &#39;&#39;)
        else:
            sep = &#39;-&#39;

        acq_date = acq_date.split(sep)
        if len(acq_date[2]) &gt; 2: #American format
            date = &#39;{}-{}-{}&#39;.format(acq_date[2], acq_date[0], acq_date[1])
        else: #international format, one we&#39;re going to use
            date = &#39;{}-{}-{}&#39;.format(acq_date[0], acq_date[1], acq_date[2])     

    elif type(acq_date) is int:
        year=datetime.datetime.today().year
        date = str((datetime.datetime(year, 1, 1) + datetime.timedelta(acq_date - 1)).date())
    
    if type(starttime) is str:
        if &#39;T&#39; in starttime:
            #date=starttime.split(&#39;T&#39;)[0]
            starttime = starttime.split(&#39;T&#39;)[1]
        else:
            pass
            #starttime = date+&#39;T&#39;+starttime
    elif type(starttime) is datetime.datetime:
        #date = str(starttime.date())
        starttime = str(starttime.time())
        ###HERE IS NEXT
    elif type(starttime) is datetime.time():
        starttime = str(starttime)
    
    starttime = date+&#34;T&#34;+starttime
    starttime = obspy.UTCDateTime(sprit_utils.format_time(starttime, tzone=tzone))
    
    if type(endtime) is str:
        if &#39;T&#39; in endtime:
            date=endtime.split(&#39;T&#39;)[0]
            endtime = endtime.split(&#39;T&#39;)[1]
    elif type(endtime) is datetime.datetime:
        date = str(endtime.date())
        endtime = str(endtime.time())
    elif type(endtime) is datetime.time():
        endtime = str(endtime)

    endtime = date+&#34;T&#34;+endtime
    endtime = obspy.UTCDateTime(sprit_utils.format_time(endtime, tzone=tzone))

    acq_date = datetime.date(year=int(date.split(&#39;-&#39;)[0]), month=int(date.split(&#39;-&#39;)[1]), day=int(date.split(&#39;-&#39;)[2]))
    raspShakeInstNameList = [&#39;raspberry shake&#39;, &#39;shake&#39;, &#39;raspberry&#39;, &#39;rs&#39;, &#39;rs3d&#39;, &#39;rasp. shake&#39;, &#39;raspshake&#39;]
    
    #Raspberry shake stationxml is in the resources folder, double check we have right path
    if instrument.lower() in raspShakeInstNameList:
        if metapath == r&#39;resources/rs3dv7_metadata.inv&#39;:
            metapath = pathlib.Path(pkg_resources.resource_filename(__name__, &#39;resources/rs3dv7_metadata.inv&#39;))
            #metapath = pathlib.Path(os.path.realpath(__file__)).parent.joinpath(&#39;/resources/rs3dv7_metadata.inv&#39;)

    if output_crs is None:
        output_crs=&#39;EPSG:4326&#39;

    if input_crs is None:
        input_crs = &#39;EPSG:4326&#39;#Default to WGS84
    else:        
        input_crs = CRS.from_user_input(input_crs)
        output_crs = CRS.from_user_input(output_crs)

        coord_transformer = Transformer.from_crs(input_crs, output_crs, always_xy=True)
        xcoord, ycoord = coord_transformer.transform(xcoord, ycoord)

    #Add key/values to input parameter dictionary
    inputParamDict = {&#39;site&#39;:site, &#39;net&#39;:network,&#39;sta&#39;:station, &#39;loc&#39;:loc, &#39;cha&#39;:channels, &#39;instrument&#39;:instrument,
                    &#39;acq_date&#39;:acq_date,&#39;starttime&#39;:starttime,&#39;endtime&#39;:endtime, &#39;timezone&#39;:&#39;UTC&#39;, #Will be in UTC by this point
                    &#39;longitude&#39;:xcoord,&#39;latitude&#39;:ycoord,&#39;elevation&#39;:elevation,&#39;input_crs&#39;:input_crs, &#39;output_crs&#39;:output_crs,
                    &#39;depth&#39;:depth, &#39;datapath&#39;: datapath, &#39;metapath&#39;:metapath, &#39;hvsr_band&#39;:hvsr_band, &#39;peak_freq_range&#39;:peak_freq_range,
                    &#39;ProcessingStatus&#39;:{&#39;InputStatus&#39;:True, &#39;OverallStatus&#39;:True}
                    }
    
    params = sprit_utils.make_it_classy(inputParamDict)
    params[&#39;ProcessingStatus&#39;][&#39;InputParams&#39;] = True
    params = _check_processing_status(params)
    return params

#Plot Obspy Trace in axis using matplotlib
def plot_stream(stream, params, fig=None, axes=None, show_plot=False, ylim_std=0.75, return_fig=True):
    &#34;&#34;&#34;Function to plot a stream of data with Z, E, N components using matplotlib. Similar to obspy.Stream.Plot(), but will be formatted differently and eventually more customizable.
    This is also used in various functions throughout the package.

    Parameters
    ----------
    stream : obspy.core.Stream.stream
        Obpsy stream of data with Z, E, N componenents
    params : HVSRData or HVSRBatch
        Data object with parameters relevant for creating plot
    fig : matplotlib.Figure, default=None
        Optional: if not None, matplotlib.Figure in which to plot the resulting figure (i.e., can be plotted in existing figure)
    axes : matplotlib.Axis, default=None
        Optional: if not None, matplotlib.Axis in which to plot the resulting figure (i.e., can be plotted in existing axis)
    show_plot : bool, default=False
        Whether to do matplotlib.pylot.show(), by default False
    ylim_std : float, default = 0.75
        Optional: the standard deviation of the data at which to clip the chart, by default 0.75
    return_fig : bool, default=True
        Optional: whether to return the figure, by default True

    Returns
    -------
    (matplotlib.Figure, matplotlib.Axes)
        Tuple containing the figure and axes of the resulting plot, only returned if return_fig = True
    &#34;&#34;&#34;
    if fig is None and axes is None:
        fig, axes = plt.subplot_mosaic([[&#39;Z&#39;],[&#39;N&#39;],[&#39;E&#39;]], sharex=True, sharey=False)

    new_stream = stream.copy()
    #axis.plot(trace.times, trace.data)
    
    sTime = stream[0].stats.starttime
    timeList = {}
    mplTimes = {}

    #In case data is masked, need to split, decimate, then merge back together
    if isinstance(new_stream[0].data, np.ma.masked_array):
        new_stream = new_stream.split()
    new_stream.decimate(10)
    new_stream.merge()

    zStream = new_stream.select(component=&#39;Z&#39;)#[0]
    eStream = new_stream.select(component=&#39;E&#39;)#[0]
    nStream = new_stream.select(component=&#39;N&#39;)#[0]
    streams = [zStream, nStream, eStream]

    for st in streams:
        key = st[0].stats.component
        timeList[key] = []
        mplTimes[key] = []
        for tr in st:
            for t in np.ma.getdata(tr.times()):
                newt = sTime + t
                timeList[key].append(newt)
                mplTimes[key].append(newt.matplotlib_date)

    #Ensure that the min and max times for each component are the same
    for i, k in enumerate(mplTimes.keys()):
        currMin = np.min(list(map(np.min, mplTimes[k])))
        currMax = np.max(list(map(np.max, mplTimes[k])))

        if i == 0:
            xmin = currMin
            xmax = currMax
        else:
            if xmin &gt; currMin:
                xmin = currMin
            if xmax &lt; currMax:
                xmax = currMax

    axes[&#39;Z&#39;].xaxis_date()
    axes[&#39;N&#39;].xaxis_date()
    axes[&#39;E&#39;].xaxis_date()

    #tTicks = mdates.MinuteLocator(interval=5)
    #axis.xaxis.set_major_locator(tTicks)
    axes[&#39;E&#39;].xaxis.set_major_locator(mdates.MinuteLocator(byminute=range(0,60,5)))
    axes[&#39;E&#39;].xaxis.set_major_formatter(mdates.DateFormatter(&#39;%H:%M&#39;))
    axes[&#34;E&#34;].xaxis.set_minor_locator(mdates.MinuteLocator(interval=1))
    axes[&#34;E&#34;].tick_params(axis=&#39;x&#39;, labelsize=8)
    

    streams = [zStream.merge(method=1), 
               nStream.merge(method=1), 
               eStream.merge(method=1)]

    for st in streams:
        for i, tr in enumerate(st):
            key = tr.stats.component
            if key == &#39;Z&#39;:
                C=&#39;k&#39;
            elif key==&#39;N&#39;:
                C=&#39;r&#39;
            else:
                C=&#39;b&#39;
            axes[key].plot(mplTimes[key], tr.data, color=C, linewidth=0.15)


    axes[&#39;Z&#39;].set_ylabel(&#39;Z&#39;)
    axes[&#39;N&#39;].set_ylabel(&#39;N&#39;)
    axes[&#39;E&#39;].set_ylabel(&#39;E&#39;)
    
    #stDz = np.abs(np.nanstd(stream.select(component=&#39;Z&#39;)[0].data))
    #stDn = np.abs(np.nanstd(stream.select(component=&#39;N&#39;)[0].data))
    #stDe = np.abs(np.nanstd(stream.select(component=&#39;E&#39;)[0].data))
    #stD = max([stDz, stDn, stDe])
    
    for i, comp in enumerate(list(mplTimes.keys())):
        stD = np.abs(np.nanstd(np.ma.getdata(stream.select(component=comp)[0].data)))
        dmed = np.nanmedian(np.ma.getdata(stream.select(component=comp)[0].data))

        axes[comp].set_ylim([dmed-ylim_std*stD, dmed+ylim_std*stD])
        if xmin &lt; 0:
            xmin=params[&#39;hvsr_band&#39;][0]
        axes[comp].set_xlim([xmin, xmax])

    fig.suptitle(params[&#39;site&#39;])
    
    day = &#34;{}-{}-{}&#34;.format(stream[0].stats.starttime.year, stream[0].stats.starttime.month, stream[0].stats.starttime.day)
    axes[&#39;E&#39;].set_xlabel(&#39;UTC Time \n&#39;+ day)

    #plt.rcParams[&#39;figure.dpi&#39;] = 100
    #plt.rcParams[&#39;figure.figsize&#39;] = (5,4)
    
    #fig.tight_layout()
    fig.canvas.draw()

    if show_plot:
        plt.show()

    if return_fig:
        return fig, axes
    return                 

#Main function for processing HVSR Curve
def process_hvsr(params, method=3, smooth=True, freq_smooth=&#39;konno ohmachi&#39;, f_smooth_width=40, resample=True, outlier_curve_std=1.75, verbose=False):
    &#34;&#34;&#34;Process the input data and get HVSR data
    
    This is the main function that uses other (private) functions to do 
    the bulk of processing of the HVSR data and the data quality checks.

    Parameters
    ----------
    params  : HVSRData or HVSRBatch
        Data object containing all the parameters input and generated by the user (usually, during sprit.input_params(), sprit.fetch_data(), sprit.generate_ppsds() and/or sprit.remove_noise()).
    method  : int or str, default=3
        Method to use for combining the horizontal components
            0) (not used)
            1) Diffuse field assumption, or &#39;DFA&#39; (not currently implemented)
            2) &#39;Arithmetic Mean&#39;: H â‰¡ (HN + HE)/2
            3) &#39;Geometric Mean&#39;: H â‰¡ âˆšHN Â· HE, recommended by the SESAME project (2004)
            4) &#39;Vector Summation&#39;: H â‰¡ âˆšH2 N + H2 E
            5) &#39;Quadratic Mean&#39;: H â‰¡ âˆš(H2 N + H2 E )/2
            6) &#39;Maximum Horizontal Value&#39;: H â‰¡ max {HN, HE}
    smooth  : bool, default=True
        bool or int may be used. 
            If True, default to smooth H/V curve to using savgoy filter with window length of 51 (works well with default resample of 1000 pts)
            If int, the length of the window in the savgoy filter.
    freq_smooth : str {&#39;konno ohmachi&#39;, &#39;constant&#39;, &#39;proportional&#39;}
        Which frequency smoothing method to use. By default, uses the &#39;konno ohmachi&#39; method.
            - The Konno &amp; Ohmachi method uses the obspy.signal.konnoohmachismoothing.konno_ohmachi_smoothing() function: https://docs.obspy.org/packages/autogen/obspy.signal.konnoohmachismoothing.konno_ohmachi_smoothing.html
            - The constant method uses a window of constant length f_smooth_width
            - The proportional method uses a window the percentage length of the frequncy steps/range (f_smooth_width now refers to percentage)
        See here for more information: https://www.geopsy.org/documentation/geopsy/hv-processing.html
    f_smooth_width : int, default = 40
        - For &#39;konno ohmachi&#39;: passed directly to the bandwidth parameter of the konno_ohmachi_smoothing() function, determines the width of the smoothing peak, with lower values resulting in broader peak. Must be &gt; 0.
        - For &#39;constant&#39;: the size of a triangular smoothing window in the number of frequency steps
        - For &#39;proportional&#39;: the size of a triangular smoothing window in percentage of the number of frequency steps (e.g., if 1000 frequency steps/bins and f_smooth_width=40, window would be 400 steps wide)
    resample  : bool, default = True
        bool or int. 
            If True, default to resample H/V data to include 1000 frequency values for the rest of the analysis
            If int, the number of data points to interpolate/resample/smooth the component psd/HV curve data to.
    outlier_curve_std : float, default = 1.75
        Standard deviation of mean of each H/V curve to use as cuttoff for whether an H/V curve is considered an &#39;outlier&#39;
    verbose : bool, defualt=False
        Whether to print output to terminal

    Returns
    -------
        hvsr_out    : dict
            Dictionary containing all the information about the data, including input parameters

    &#34;&#34;&#34;
    orig_args = locals().copy() #Get the initial arguments
    if (verbose and isinstance(params, HVSRBatch)) or (verbose and not params[&#39;batch&#39;]):
        if isinstance(params, HVSRData) and params[&#39;batch&#39;]:
            pass
        else:
            print(&#39;\nCalculating Horizontal/Vertical Ratios at all frequencies/time steps (process_hvsr())&#39;)
            print(&#39;\tUsing the following parameters:&#39;)
            for key, value in orig_args.items():
                if key==&#39;params&#39;:
                    pass
                else:
                    print(&#39;\t  {}={}&#39;.format(key, value))
            print()

    #First, divide up for batch or not
    #Site is in the keys anytime it&#39;s not batch
    if isinstance(params, HVSRBatch):
        #If running batch, we&#39;ll loop through each site
        hvsr_out = {}
        for site_name in params.keys():
            args = orig_args.copy() #Make a copy so we don&#39;t accidentally overwrite
            args[&#39;params&#39;] = params[site_name] #Get what would normally be the &#34;params&#34; variable for each site
            if params[site_name][&#39;ProcessingStatus&#39;][&#39;OverallStatus&#39;]:
                try:
                    hvsr_out[site_name] = _process_hvsr_batch(**args) #Call another function, that lets us run this function again
                except:
                    hvsr_out = params
                    hvsr_out[site_name][&#39;ProcessingStatus&#39;][&#39;HVStatus&#39;]=False
                    hvsr_out[site_name][&#39;ProcessingStatus&#39;][&#39;OverallStatus&#39;] = False                    
            else:
                hvsr_out = params
                hvsr_out[site_name][&#39;ProcessingStatus&#39;][&#39;HVStatus&#39;]=False
                hvsr_out[site_name][&#39;ProcessingStatus&#39;][&#39;OverallStatus&#39;] = False
        hvsr_out = HVSRBatch(hvsr_out)
    else:
        ppsds = params[&#39;ppsds&#39;].copy()#[k][&#39;psd_values&#39;]
        ppsds = sprit_utils.check_xvalues(ppsds)

        methodList = [&#39;&lt;placeholder_0&gt;&#39;, &#39;Diffuse Field Assumption&#39;, &#39;Arithmetic Mean&#39;, &#39;Geometric Mean&#39;, &#39;Vector Summation&#39;, &#39;Quadratic Mean&#39;, &#39;Maximum Horizontal Value&#39;]
        x_freqs = {}
        x_periods = {}

        psdValsTAvg = {}
        stDev = {}
        stDevValsP = {}
        stDevValsM = {}
        psdRaw={}
        currTimesUsed={}
        hvsrDF = params[&#39;hvsr_df&#39;]

        for k in ppsds.keys():
            #input_ppsds = ppsds[k][&#39;psd_values&#39;] #original, not used anymore
            input_ppsds = np.stack(hvsrDF[&#39;psd_values_&#39;+k].values)

            currPPSDs = hvsrDF[&#39;psd_values_&#39;+k][hvsrDF[&#39;Use&#39;]].values
            used_ppsds = np.stack(currPPSDs)
            

            #if reasmpling has been selected
            if resample is True or type(resample) is int:
                if resample is True:
                    resample = 1000 #Default smooth value

                xValMin = min(ppsds[k][&#39;period_bin_centers&#39;])
                xValMax = max(ppsds[k][&#39;period_bin_centers&#39;])

                #Resample period bin values
                x_periods[k] = np.logspace(np.log10(xValMin), np.log10(xValMax), num=resample)

                if smooth or type(smooth) is int:
                    if smooth:
                        smooth = 51 #Default smoothing window
                    elif smooth % 2==0:
                        smooth = smooth+1

                #Resample raw ppsd values
                for i, ppsd_t in enumerate(input_ppsds):
                    if i==0:
                        psdRaw[k] = np.interp(x_periods[k], ppsds[k][&#39;period_bin_centers&#39;], ppsd_t)
                        if smooth is not False:
                            psdRaw[k] = scipy.signal.savgol_filter(psdRaw[k], smooth, 3)
                    else:
                        psdRaw[k] = np.vstack((psdRaw[k], np.interp(x_periods[k], ppsds[k][&#39;period_bin_centers&#39;], ppsd_t)))
                        if smooth is not False:
                            psdRaw[k][i] = scipy.signal.savgol_filter(psdRaw[k][i], smooth, 3)

            else:
                #If no resampling desired
                x_periods[k] = np.array(ppsds[k][&#39;period_bin_centers&#39;])
                psdRaw[k] = np.array(input_ppsds)

            hvsrDF[&#39;psd_values_&#39;+k] = list(psdRaw[k])

            #Get average psd value across time for each channel (used to calc main H/V curve)
            psdValsTAvg[k] = np.nanmean(np.array(psdRaw[k]), axis=0)
            x_freqs[k] = np.divide(np.ones_like(x_periods[k]), x_periods[k]) 

            stDev[k] = np.std(psdRaw[k], axis=0)
            stDevValsM[k] = np.array(psdValsTAvg[k] - stDev[k])
            stDevValsP[k] = np.array(psdValsTAvg[k] + stDev[k])

            currTimesUsed[k] = np.array(hvsrDF[&#39;TimesProcessed_Obspy&#39;][hvsrDF[&#39;Use&#39;]].values)
            #currTimesUsed[k] = ppsds[k][&#39;current_times_used&#39;] #original one
            
        #Get string of method type
        if type(method) is int:
            methodInt = method
            method = methodList[method]
        params[&#39;method&#39;] = method

        #This gets the main hvsr curve averaged from all time steps
        anyK = list(x_freqs.keys())[0]
        hvsr_curve, _ = __get_hvsr_curve(x=x_freqs[anyK], psd=psdValsTAvg, method=methodInt, hvsr_data=params, verbose=verbose)
        origPPSD = params[&#39;ppsds_obspy&#39;].copy()

        #Add some other variables to our output dictionary
        hvsr_data = {&#39;input_params&#39;:params,
                    &#39;x_freqs&#39;:x_freqs,
                    &#39;hvsr_curve&#39;:hvsr_curve,
                    &#39;x_period&#39;:x_periods,
                    &#39;psd_raw&#39;:psdRaw,
                    &#39;current_times_used&#39;: currTimesUsed,
                    &#39;psd_values_tavg&#39;:psdValsTAvg,
                    &#39;ppsd_std&#39;:stDev,
                    &#39;ppsd_std_vals_m&#39;:stDevValsM,
                    &#39;ppsd_std_vals_p&#39;:stDevValsP,
                    &#39;method&#39;:method,
                    &#39;ppsds&#39;:ppsds,
                    &#39;ppsds_obspy&#39;:origPPSD,
                    &#39;tsteps_used&#39;: params[&#39;tsteps_used&#39;].copy(),
                    &#39;hvsr_df&#39;:params[&#39;hvsr_df&#39;]
                    }
        
        hvsr_out = HVSRData(hvsr_data)

        #This is if manual editing was used (should probably be updated at some point to just use masks)
        if &#39;xwindows_out&#39; in params.keys():
            hvsr_out[&#39;xwindows_out&#39;] = params[&#39;xwindows_out&#39;]
        else:
            hvsr_out[&#39;xwindows_out&#39;] = []

        #These are in other places in the hvsr_out dict, so are redudant
        #del dir(hvsr_out[&#39;input_params&#39;])[&#39;_ppsds_obspy&#39;]
        #del hvsr_out[&#39;input_params&#39;][&#39;_ppsds&#39;]
        #del hvsr_out[&#39;input_params&#39;][&#39;tsteps_used&#39;]

        freq_smooth_ko = [&#39;konno ohmachi&#39;, &#39;konno-ohmachi&#39;, &#39;konnoohmachi&#39;, &#39;konnohmachi&#39;, &#39;ko&#39;, &#39;k&#39;]
        freq_smooth_constant = [&#39;constant&#39;, &#39;const&#39;, &#39;c&#39;]
        freq_smooth_proport = [&#39;proportional&#39;, &#39;proportion&#39;, &#39;prop&#39;, &#39;p&#39;]

        #Frequency Smoothing
        if freq_smooth is False:
            if verbose:
                warnings.warn(&#39;No frequency smoothing is being applied. This is not recommended for noisy datasets.&#39;)
        elif freq_smooth is True or freq_smooth.lower() in freq_smooth_ko:
            from obspy.signal import konnoohmachismoothing
            for k in hvsr_out[&#39;psd_raw&#39;]:
                colName = f&#39;psd_values_{k}&#39;

                ppsd_data = np.stack(hvsr_out[&#39;hvsr_df&#39;][colName])
                ppsd_data = hvsr_out[&#39;psd_raw&#39;][k]

                freqs = hvsr_out[&#39;x_freqs&#39;][k]
                with warnings.catch_warnings():
                    warnings.simplefilter(&#39;ignore&#39;, category=UserWarning) #Filter out UserWarning for just this method, since it throws up a UserWarning that doesn&#39;t really matter about dtypes often
                    smoothed_ppsd_data = konnoohmachismoothing.konno_ohmachi_smoothing(ppsd_data, freqs, bandwidth=f_smooth_width, normalize=True)
                hvsr_out[&#39;psd_raw&#39;][k] = smoothed_ppsd_data
                hvsr_out[&#39;hvsr_df&#39;][colName] = pd.Series(list(smoothed_ppsd_data), index=hvsr_out[&#39;hvsr_df&#39;].index)

        elif freq_smooth.lower() in freq_smooth_constant:
            hvsr_out = __freq_smooth_window(hvsr_out, f_smooth_width, kind_freq_smooth=&#39;constant&#39;)
        elif freq_smooth.lower() in freq_smooth_proport:
            hvsr_out = __freq_smooth_window(hvsr_out, f_smooth_width, kind_freq_smooth=&#39;proportional&#39;)
        else:
            if verbose:
                warnings.warn(f&#39;You indicated no frequency smoothing should be applied (freq_smooth = {freq_smooth}). This is not recommended for noisy datasets.&#39;)

        #Get hvsr curve from three components at each time step
        anyK = list(hvsr_out[&#39;psd_raw&#39;].keys())[0]
        if method==1 or method ==&#39;dfa&#39; or method ==&#39;Diffuse Field Assumption&#39;:
            pass ###UPDATE HERE NEXT???__get_hvsr_curve(x=hvsr_out[&#39;x_freqs&#39;][anyK], psd=tStepDict, method=methodInt, hvsr_data=hvsr_out, verbose=verbose)
        else:
            hvsr_tSteps = []
            for tStep in range(len(hvsr_out[&#39;psd_raw&#39;][anyK])):
                tStepDict = {}
                for k in hvsr_out[&#39;psd_raw&#39;]:
                    tStepDict[k] = hvsr_out[&#39;psd_raw&#39;][k][tStep]
                hvsr_tstep, _ = __get_hvsr_curve(x=hvsr_out[&#39;x_freqs&#39;][anyK], psd=tStepDict, method=methodInt, hvsr_data=hvsr_out, verbose=verbose)
                hvsr_tSteps.append(np.float32(hvsr_tstep)) #Add hvsr curve for each time step to larger list of arrays with hvsr_curves

        hvsr_out[&#39;hvsr_df&#39;][&#39;HV_Curves&#39;] = hvsr_tSteps

        hvsr_out[&#39;ind_hvsr_curves&#39;] = np.stack(hvsr_out[&#39;hvsr_df&#39;][&#39;HV_Curves&#39;][hvsr_out[&#39;hvsr_df&#39;][&#39;Use&#39;]])
        #hvsr_out[&#39;ind_hvsr_curves&#39;] = np.array(hvsr_tSteps)

        #Initialize array based only on the curves we are currently using
        indHVCurvesArr = np.stack(hvsr_out[&#39;hvsr_df&#39;][&#39;HV_Curves&#39;][hvsr_out[&#39;hvsr_df&#39;][&#39;Use&#39;]])
        #indHVCurvesArr = hvsr_out[&#39;ind_hvsr_curves&#39;]

        if outlier_curve_std:
            #use the standard deviation of each individual curve to determine if it overlapped
            stdT = np.nanstd(indHVCurvesArr, axis=1)
            std_stdT= np.nanstd(stdT)
            avg_stdT= np.nanmean(stdT)
            bool_col=&#39;Use&#39;
            eval_col=&#39;HV_Curves&#39;
    
            testCol = hvsr_out[&#39;hvsr_df&#39;].loc[hvsr_out[&#39;hvsr_df&#39;][bool_col], eval_col].apply(np.nanstd).gt((avg_stdT + (std_stdT * outlier_curve_std)))
            low_std_val = avg_stdT - (std_stdT * outlier_curve_std)
            hi_std_val = avg_stdT + (std_stdT * outlier_curve_std)

            #First, do pandas version of it
            updateUseCol = hvsr_out[&#39;hvsr_df&#39;].loc[hvsr_out[&#39;hvsr_df&#39;][bool_col], eval_col].apply(np.nanstd).between(low_std_val, hi_std_val, inclusive=&#39;both&#39;)
            hvsr_out[&#39;hvsr_df&#39;].loc[hvsr_out[&#39;hvsr_df&#39;][bool_col], bool_col] = updateUseCol

            #Find psds to get rid of based on standard deviation of each curve (i.e., how curvy is the curve)
            psds_to_rid = []
            for i,t in enumerate(indHVCurvesArr):
                if stdT[i] &lt; avg_stdT - std_stdT*outlier_curve_std or stdT[i] &gt; avg_stdT + std_stdT*outlier_curve_std:
                    psds_to_rid.append(i)

            for i, r in enumerate(psds_to_rid):
                index = int(r-i)
                indHVCurvesArr = np.delete(indHVCurvesArr, index, axis=0)

                for k in hvsr_out[&#39;ppsds&#39;]:
                    hvsr_out[&#39;psd_raw&#39;][k] = np.delete(hvsr_out[&#39;psd_raw&#39;][k], index, axis=0)         
                    hvsr_out[&#39;current_times_used&#39;][k] = np.delete(hvsr_out[&#39;current_times_used&#39;][k], index)
            hvsr_out[&#39;tsteps_used&#39;][0] = hvsr_out[&#39;ppsds&#39;][k][&#39;current_times_used&#39;].shape[0]

        hvsr_out[&#39;ind_hvsr_stdDev&#39;] = np.nanstd(indHVCurvesArr, axis=0)

        #Get peaks for each time step
        tStepPeaks = []
        for tStepHVSR in hvsr_tSteps:
            tStepPeaks.append(__find_peaks(tStepHVSR))
        hvsr_out[&#39;ind_hvsr_peak_indices&#39;] = tStepPeaks
        hvsr_out[&#39;hvsr_df&#39;][&#39;CurvesPeakIndices&#39;] = tStepPeaks

        tStepPFList = []
        for tPeaks in tStepPeaks:
            tStepPFs = []
            for pInd in tPeaks:
                tStepPFs.append(np.float32(hvsr_out[&#39;x_freqs&#39;][anyK][pInd]))
            tStepPFList.append(tStepPFs)
        hvsr_out[&#39;hvsr_df&#39;][&#39;CurvesPeakFreqs&#39;] = tStepPFList

        #Get peaks of main HV curve
        hvsr_out[&#39;hvsr_peak_indices&#39;] = __find_peaks(hvsr_out[&#39;hvsr_curve&#39;])
        
        #Get frequency values at HV peaks in main curve
        hvsrPF=[]
        for p in hvsr_out[&#39;hvsr_peak_indices&#39;]:
            hvsrPF.append(hvsr_out[&#39;x_freqs&#39;][anyK][p])
        hvsr_out[&#39;hvsr_peak_freqs&#39;] = np.array(hvsrPF)


        #Get other HVSR parameters (i.e., standard deviations, etc.)
        hvsr_out = __gethvsrparams(hvsr_out)

        #Include the original obspy stream in the output
        hvsr_out[&#39;input_stream&#39;] = params[&#39;input_stream&#39;] #input_stream

        hvsr_out = sprit_utils.make_it_classy(hvsr_out)

        hvsr_out[&#39;ProcessingStatus&#39;][&#39;HVStatus&#39;] = True
    hvsr_out = _check_processing_status(hvsr_out)

    return hvsr_out

#Function to remove noise windows from data
def remove_noise(hvsr_data, remove_method=&#39;auto&#39;, sat_percent=0.995, noise_percent=0.80, sta=2, lta=30, stalta_thresh=[0.5,5], warmup_time=0, cooldown_time=0, min_win_size=1, remove_raw_noise=False, verbose=False):
    &#34;&#34;&#34;Function to remove noisy windows from data, using various methods.
    
    Methods include 
    - Manual window selection (by clicking on a chart with spectrogram and stream data), 
    - Auto window selection, which does the following two in sequence (these can also be done indepently):
        - A sta/lta &#34;antitrigger&#34; method (using stalta values to automatically remove triggered windows where there appears to be too much noise)
        - A noise threshold method, that cuts off all times where the noise threshold equals more than (by default) 80% of the highest amplitude noise sample for the length specified by lta (in seconds)
        - A saturation threshold method, that cuts off all times where the noise threshold equals more than (by default) 99.5% of the highest amplitude noise sample.

    Parameters
    ----------
    hvsr_data : dict, obspy.Stream, or obspy.Trace
        Dictionary containing all the data and parameters for the HVSR analysis
    remove_method : str, {&#39;auto&#39;, &#39;manual&#39;, &#39;stalta&#39;/&#39;antitrigger&#39;, &#39;saturation threshold&#39;, &#39;noise threshold&#39;, &#39;warmup&#39;/&#39;cooldown&#39;/&#39;buffer&#39;/&#39;warm_cool&#39;}
        The different methods for removing noise from the dataset. A list of strings will also work, in which case, it should be a list of the above strings. See descriptions above for what how each method works. By default &#39;auto.&#39;
        If remove_method=&#39;auto&#39;, this is the equivalent of remove_method=[&#39;noise threshold&#39;, &#39;antitrigger&#39;, &#39;saturation threshold&#39;, &#39;warm_cool&#39;]
    sat_percent : float, default=0.995
        Percentage (between 0 and 1), to use as the threshold at which to remove data. This is used in the saturation method. By default 0.995. 
        If a value is passed that is greater than 1, it will be divided by 100 to obtain the percentage.
    noise_percent : float, default = 0.8
        Percentage (between 0 and 1), to use as the threshold at which to remove data, if it persists for longer than time (in seconds (specified by min_win_size)). This is used in the noise threshold method. By default 0.8. 
        If a value is passed that is greater than 1, it will be divided by 100 to obtain the percentage.
    sta : int, optional
        Short term average (STA) window (in seconds), by default 2. For use with sta/lta antitrigger method.
    lta : int, optional
        Long term average (STA) window (in seconds), by default 30. For use with sta/lta antitrigger method.
    stalta_thresh : list, default=[0.5,5]
        Two-item list or tuple with the thresholds for the stalta antitrigger. The first value (index [0]) is the lower threshold, the second value (index [1] is the upper threshold), by default [0.5,5]
    warmup_time : int, default=0
        Time in seconds to allow for warmup of the instrument (or while operator is still near instrument). This will renove any data before this time, by default 0.
    cooldown_time : int, default=0
        Time in seconds to allow for cooldown of the instrument (or for when operator is nearing instrument). This will renove any data before this time, by default 0.
    min_win_size : float, default=1
        The minumum size a window must be over specified threshold (in seconds) for it to be removed
    remove_raw_noise : bool, default=False
        If remove_raw_noise=True, will perform operation on raw data (&#39;input_stream&#39;), rather than potentially already-modified data (&#39;stream&#39;).
    verbose : bool, default=False
        Whether to print status of remove_noise

    Returns
    -------
    output : dict
        Dictionary similar to hvsr_data, but containing modified data with &#39;noise&#39; removed
    &#34;&#34;&#34;
    orig_args = locals().copy() #Get the initial arguments

    if (verbose and isinstance(hvsr_data, HVSRBatch)) or (verbose and not hvsr_data[&#39;batch&#39;]):
        if isinstance(hvsr_data, HVSRData) and hvsr_data[&#39;batch&#39;]:
            pass
        else:
            print(&#39;\nRemoving noisy data windows (remove_noise())&#39;)
            print(&#39;\tUsing the following parameters:&#39;)
            for key, value in orig_args.items():
                if key==&#39;hvsr_data&#39;:
                    pass
                else:
                    print(&#39;\t  {}={}&#39;.format(key, value))

    #Setup lists
    manualList = [&#39;manual&#39;, &#39;man&#39;, &#39;m&#39;, &#39;window&#39;, &#39;windows&#39;, &#39;w&#39;]
    autoList = [&#39;auto&#39;, &#39;automatic&#39;, &#39;all&#39;, &#39;a&#39;]
    antitrigger = [&#39;stalta&#39;, &#39;anti&#39;, &#39;antitrigger&#39;, &#39;trigger&#39;, &#39;at&#39;]
    saturationThresh = [&#39;saturation threshold&#39;, &#39;saturation&#39;, &#39;sat&#39;, &#39;s&#39;]
    noiseThresh = [&#39;noise threshold&#39;, &#39;noise&#39;, &#39;threshold&#39;, &#39;n&#39;]
    warmup_cooldown=[&#39;warmup&#39;, &#39;cooldown&#39;, &#39;warm&#39;, &#39;cool&#39;, &#39;buffer&#39;, &#39;warmup-cooldown&#39;, &#39;warmup_cooldown&#39;, &#39;wc&#39;, &#39;warm_cool&#39;, &#39;warm-cool&#39;]

    #Get Stream from hvsr_data
    if isinstance(hvsr_data, HVSRBatch):
        #If running batch, we&#39;ll loop through each site
        hvsr_out = {}
        for site_name in hvsr_data.keys():
            args = orig_args.copy() #Make a copy so we don&#39;t accidentally overwrite
            args[&#39;hvsr_data&#39;] = hvsr_data[site_name] #Get what would normally be the &#34;hvsr_data&#34; variable for each site
            if hvsr_data[site_name][&#39;ProcessingStatus&#39;][&#39;OverallStatus&#39;]:
                try:
                   hvsr_out[site_name] = __remove_noise_batch(**args) #Call another function, that lets us run this function again
                except:
                    hvsr_out[site_name][&#39;ProcessingStatus&#39;][&#39;RemoveNoiseStatus&#39;]=False
                    hvsr_out[site_name][&#39;ProcessingStatus&#39;][&#39;OverallStatus&#39;]=False
            else:
                hvsr_data[site_name][&#39;ProcessingStatus&#39;][&#39;RemoveNoiseStatus&#39;]=False
                hvsr_data[site_name][&#39;ProcessingStatus&#39;][&#39;OverallStatus&#39;]=False
                hvsr_out = hvsr_data
                
        output = HVSRBatch(hvsr_out)
        return output

    elif isinstance(hvsr_data, (HVSRData, dict)):
        if remove_raw_noise:
            inStream = hvsr_data[&#39;input_stream&#39;].copy()
        else:
            inStream = hvsr_data[&#39;stream&#39;].copy()
        output = hvsr_data#.copy()
    elif isinstance(hvsr_data, (obspy.core.stream.Stream, obspy.core.trace.Trace)):
        inStream = hvsr_data.copy()
        output = inStream.copy()
    else:
        RuntimeError(f&#34;Input of type type(hvsr_data)={type(hvsr_data)} cannot be used.&#34;)
    
    outStream = inStream
    
    if isinstance(remove_method, str):
        if &#39;,&#39; in remove_method:
            remove_method = remove_method.split(&#39;,&#39;)
        else:
            remove_method = [remove_method]
    elif isinstance(remove_method, (list, tuple)):
        pass
    elif not remove_method:
        remove_method=[None]
    else:
        warnings.warn(f&#34;Input value remove_method={remove_method} must be either string, list of strings, None, or False. No noise removal will be carried out. Please choose one of the following: &#39;manual&#39;, &#39;auto&#39;, &#39;antitrigger&#39;, &#39;noise threshold&#39;, &#39;warmup_cooldown&#39;.&#34;)
        return output
        
    #Reorder list so manual is always first
    if len(set(remove_method).intersection(manualList)) &gt; 0:
        manInd = list(set(remove_method).intersection(manualList))[0]
        remove_method.remove(manInd)
        remove_method.insert(0, manInd)
        
    #Go through each type of removal and remove
    for rem_kind in remove_method:
        if not rem_kind:
            break
        elif rem_kind.lower() in manualList:
            if isinstance(output, (HVSRData, dict)):
                if &#39;xwindows_out&#39; in output.keys():
                    pass
                else:
                    output = _select_windows(output)
                window_list = output[&#39;xwindows_out&#39;]
            if isinstance(outStream, obspy.core.stream.Stream):
                if window_list is not None:
                    output[&#39;stream&#39;] = __remove_windows(inStream, window_list, warmup_time)
                else:
                    output = _select_windows(output)
            elif isinstance(output, (HVSRData, dict)):
                pass
            else:
                RuntimeError(&#34;Only obspy.core.stream.Stream data type is currently supported for manual noise removal method.&#34;)     
        elif rem_kind.lower() in autoList:
            outStream = __remove_noise_thresh(outStream, noise_percent=noise_percent, lta=lta, min_win_size=min_win_size)
            outStream = __remove_anti_stalta(outStream, sta=sta, lta=lta, thresh=stalta_thresh)
            outStream = __remove_noise_saturate(outStream, sat_percent=sat_percent, min_win_size=min_win_size)
            outStream = __remove_warmup_cooldown(stream=outStream, warmup_time=warmup_time, cooldown_time=cooldown_time)
        elif rem_kind.lower() in antitrigger:
            outStream = __remove_anti_stalta(outStream, sta=sta, lta=lta, thresh=stalta_thresh)
        elif rem_kind.lower() in saturationThresh:
            outStream = __remove_noise_saturate(outStream, sat_percent=sat_percent, min_win_size=min_win_size)
        elif rem_kind.lower() in noiseThresh:
            outStream = __remove_noise_thresh(outStream, noise_percent=noise_percent, lta=lta, min_win_size=min_win_size)
        elif rem_kind.lower() in warmup_cooldown:
            outStream = __remove_warmup_cooldown(stream=outStream, warmup_time=warmup_time, cooldown_time=cooldown_time)
        else:
            if len(remove_method)==1:
                warnings.warn(f&#34;Input value remove_method={remove_method} is not recognized. No noise removal will be carried out. Please choose one of the following: &#39;manual&#39;, &#39;auto&#39;, &#39;antitrigger&#39;, &#39;noise threshold&#39;, &#39;warmup_cooldown&#39;.&#34;)
                break
            warnings.warn(f&#34;Input value remove_method={remove_method} is not recognized. Continuing with other noise removal methods.&#34;)

    #Add output
    if isinstance(output, (HVSRData, dict)):
        output[&#39;stream&#39;] = outStream
        output[&#39;input_stream&#39;] = hvsr_data[&#39;input_stream&#39;]
        output[&#39;ProcessingStatus&#39;][&#39;RemoveNoiseStatus&#39;] = True
        output = _check_processing_status(output)

        if &#39;hvsr_df&#39; in output.keys():
            hvsrDF = output[&#39;hvsr_df&#39;]
            
            outStream = output[&#39;stream&#39;].split()
            for i, trace in enumerate(outStream):
                if i ==0:
                    trEndTime = trace.stats.endtime
                    comp_end = trace.stats.component
                    continue
                trStartTime = trace.stats.starttime
                comp_start = trace.stats.component
                
                if trEndTime &lt; trStartTime and comp_end==comp_start:
                    gap = [trEndTime,trStartTime]
                    output[&#39;hvsr_df&#39;][&#39;Use&#39;] = (hvsrDF[&#39;TimesProcessed_Obspy&#39;].gt(gap[0]) &amp; hvsrDF[&#39;TimesProcessed_Obspy&#39;].gt(gap[1]) )| \
                                    (hvsrDF[&#39;TimesProcessed_ObspyEnd&#39;].lt(gap[0]) &amp; hvsrDF[&#39;TimesProcessed_ObspyEnd&#39;].lt(gap[1]))# | \
                
                trEndTime = trace.stats.endtime
            
            outStream.merge()
            output[&#39;stream&#39;] = outStream        
    elif isinstance(hvsr_data, obspy.core.stream.Stream) or isinstance(hvsr_data, obspy.core.trace.Trace):
        output = outStream
    else:
        warnings.warn(f&#34;Output of type {type(output)} for this function will likely result in errors in other processing steps. Returning hvsr_data data.&#34;)
        return hvsr_data


    return output

#Remove outlier ppsds
def remove_outlier_curves(params, outlier_std=3, ppsd_length=30):
    &#34;&#34;&#34;Function used in generate_ppsds() to remove outliers. May also be used independently.
    
    This uses the mean value of the entirety of each ppsd curve. This is not very robust, but it is intended only to remove curves who are well outside of the what would be expected.
    These abberant curves often occur due to the remove_noise() function.

    Parameters
    ----------
    params : dict
        Input dictionary containing all the values and parameters of interest
    outlier_std :  float, default=3
        The standard deviation value to use as a threshold for determining whether a curve is an outlier. 
        This averages over each individual entire curve so that curves with very abberant data (often occurs when using the remove_noise() method), can be identified.
    ppsd_length : float, optional
        Length of data segments passed to psd in seconds, by default 60.

    Returns
    -------
    params : dict
        Input dictionary with values modified based on work of function.
    &#34;&#34;&#34;
    
    ppsds = params[&#39;ppsds&#39;]
    newPPsds = {}
    stds = {}
    psds_to_rid = []

    for k in ppsds:
        #Get the average ppsd curve value
        psdVals = np.array(ppsds[k][&#39;psd_values&#39;])
        meanArr = np.nanmean(psdVals, axis=1)
        medArr = np.nanmedian(psdVals, axis=1)
        
        newPPsds[k] = []
        totMean = np.nanmean(meanArr)
        totMed = np.nanmedian(medArr)

        stds[k] = np.std(meanArr)

        for i, m in enumerate(meanArr):
            if m &gt; totMean + outlier_std*stds[k] or m &lt; totMean - outlier_std*stds[k]:
                psds_to_rid.append(i)

        curr_times_mpl = []
        for i, t in enumerate(ppsds[k][&#39;current_times_used&#39;]):
            curr_times_mpl.append(t.matplotlib_date)

        #Get ppsd length in seconds in matplotlib format
        ppsd_length_mpl = ppsd_length/86400

        ##UPDATE THIS NOT TO USE xWindows_out (calculate from mask)
        #Check if any times fall in excluded zone
        for i, t in enumerate(curr_times_mpl):
            nextT = t + ppsd_length_mpl
            for w, win in enumerate(params[&#39;xwindows_out&#39;]):
                if t &gt; win[0] and t &lt; win[1]:
                    psds_to_rid.append(i)
                elif nextT &gt; win[0] and nextT &lt; win[1]:
                    psds_to_rid.append(i)
    
        #Use dataframe
        hvsrDF = params[&#39;hvsr_df&#39;]
        psdVals = hvsrDF[&#39;psd_values_&#39;+k]
        params[&#39;hvsr_df&#39;][k+&#39;_CurveMedian&#39;] = psdVals.apply(np.nanmedian)
        params[&#39;hvsr_df&#39;][k+&#39;_CurveMean&#39;] = psdVals.apply(np.nanmean)

        totMean = np.nanmean(params[&#39;hvsr_df&#39;][k+&#39;_CurveMean&#39;])
        stds[k] = np.nanstd(params[&#39;hvsr_df&#39;][k+&#39;_CurveMean&#39;])

        meanArr = params[&#39;hvsr_df&#39;][k+&#39;_CurveMean&#39;]
        params[&#39;hvsr_df&#39;][&#39;Use&#39;] = meanArr &lt; (totMean + outlier_std * stds[k])

    psds_to_rid = np.unique(psds_to_rid)

    for k in params[&#39;ppsds&#39;]:
        for i, r in enumerate(psds_to_rid):
            index = int(r-i)
            params[&#39;ppsds&#39;][k][&#39;psd_values&#39;] = np.delete(params[&#39;ppsds&#39;][k][&#39;psd_values&#39;], index, axis=0)
            params[&#39;ppsds&#39;][k][&#39;current_times_used&#39;] = np.delete(params[&#39;ppsds&#39;][k][&#39;current_times_used&#39;], index, axis=0)
    return params

#Read data as batch
def batch_data_read(input_data, batch_type=&#39;table&#39;, param_col=None, batch_params=None, verbose=False, **readcsv_getMeta_fetch_kwargs):
    &#34;&#34;&#34;Function to read data in data as a batch of multiple data files. This is best used through sprit.fetch_data(*args, source=&#39;batch&#39;, **other_kwargs).

    Parameters
    ----------
    input_data : filepath or list
        Input data information for how to read in data as batch
    batch_type : str, optional
        Type of batch read, only &#39;table&#39; and &#39;filelist&#39; accepted. If &#39;table&#39;, will read data from a file read in using pandas.read_csv(), by default &#39;table&#39;
    param_col : None or str, optional
        Name of parameter column from batch information file. Only used if a batch_type=&#39;table&#39; and single parameter column is used, rather than one column per parameter (for single parameter column, parameters are formatted with = between keys/values and , between item pairs), by default None
    batch_params : list, dict, or None, default = None
        Parameters to be used if batch_type=&#39;filelist&#39;. If it is a list, needs to be the same length as input_data. If it is a dict, will be applied to all files in input_data and will combined with extra keyword arguments caught by **readcsv_getMeta_fetch_kwargs.
    verbose : bool, optional
        Whether to print information to terminal during batch read, by default False
    **readcsv_getMeta_fetch_kwargs
        Keyword arguments that will be read into pandas.read_csv(), sprit.input_params, sprit.get_metadata(), and/or sprit.fetch_data()

    Returns
    -------
    dict
        Dictionary with each item representing a different file read in, and which consists of its own parameter dictionary to be used by the rest of the processing steps

    Raises
    ------
    IndexError
        _description_
    &#34;&#34;&#34;
    #First figure out columns
    input_params_params = input_params.__code__.co_varnames
    get_metadata_params = get_metadata.__code__.co_varnames
    fetch_data_params = fetch_data.__code__.co_varnames

    if batch_type==&#39;sample&#39;:
        sample_data=True
        batch_type=&#39;table&#39;
    else:
        sample_data = False
    # Dictionary to store the stream objects
    stream_dict = {}
    data_dict = {}
    if batch_type == &#39;table&#39;:
        if isinstance(input_data, pd.DataFrame):
            dataReadInfoDF = input_data
        elif isinstance(input_data, dict):
            #For params input
            pass
        else:#Read csv
            read_csv_kwargs = {k: v for k, v in locals()[&#39;readcsv_getMeta_fetch_kwargs&#39;].items() if k in pd.read_csv.__code__.co_varnames}
            dataReadInfoDF = pd.read_csv(input_data, **read_csv_kwargs)
            if &#39;datapath&#39; in dataReadInfoDF.columns:
                filelist = list(dataReadInfoDF[&#39;datapath&#39;])
            #dataReadInfoDF = dataReadInfoDF.replace(np.nan, None)

        #If this is sample data, we need to create absolute paths to the filepaths
        if sample_data:
            sample_data_dir = pathlib.Path(pkg_resources.resource_filename(__name__, &#39;resources/sample_data/&#39;))
            for index, row in dataReadInfoDF.iterrows():
                dataReadInfoDF.loc[index, &#39;datapath&#39;] = sample_data_dir.joinpath(row.loc[&#39;datapath&#39;])

        default_dict = {&#39;site&#39;:&#39;HVSR Site&#39;,
                    &#39;network&#39;:&#39;AM&#39;, 
                    &#39;station&#39;:&#39;RAC84&#39;, 
                    &#39;loc&#39;:&#39;00&#39;, 
                    &#39;channels&#39;:[&#39;EHZ&#39;, &#39;EHN&#39;, &#39;EHE&#39;],
                    &#39;acq_date&#39;:str(datetime.datetime.now().date()),
                    &#39;starttime&#39; : &#39;00:00:00.00&#39;,
                    &#39;endtime&#39; : &#39;23:59:59.999&#39;,
                    &#39;tzone&#39; : &#39;UTC&#39;,
                    &#39;xcoord&#39; : -88.2290526,
                    &#39;ycoord&#39; :  40.1012122,
                    &#39;elevation&#39; : 755,
                    &#39;input_crs&#39;:&#39;EPSG:4326&#39;,#4269 is NAD83, defautling to WGS
                    &#39;output_crs&#39;:&#39;EPSG:4326&#39;,
                    &#39;elev_unit&#39; : &#39;feet&#39;,
                    &#39;depth&#39; : 0,
                    &#39;instrument&#39; : &#39;Raspberry Shake&#39;,
                    &#39;metapath&#39; : &#39;&#39;,
                    &#39;hvsr_band&#39; : [0.4, 40],
                    &#39;write_path&#39;:&#39;&#39;,
                    &#39;source&#39;:&#39;file&#39;, 
                    &#39;export_format&#39;:&#39;mseed&#39;, 
                    &#39;detrend&#39;:&#39;spline&#39;, 
                    &#39;detrend_order&#39;:2, 
                    &#39;verbose&#39;:False}

        print(f&#34;\t{dataReadInfoDF.shape[0]} sites found: {list(dataReadInfoDF[&#39;site&#39;])}&#34;)
        if verbose:
            maxLength = 25
            maxColWidth = 12
            if dataReadInfoDF.shape[0] &gt; maxLength:
                print(f&#39;\t Showing information for first {maxLength} files only:&#39;)
            print()
            #Print nicely formated df
            #Print column names
            print(&#39;\t&#39;, end=&#39;&#39;)
            for col in dataReadInfoDF.columns:
                print(str(col)[:maxColWidth].ljust(maxColWidth), end=&#39;  &#39;)
            print(&#39;\n\t&#39;, end=&#39;&#39;)

            #Print separator
            tableLen = (maxColWidth+2)*len(dataReadInfoDF.columns)
            for r in range(tableLen):
                print(&#39;-&#39;, end=&#39;&#39;)
            print()

            #Print columns/rows
            for index, row in dataReadInfoDF.iterrows():
                print(&#39;\t&#39;, end=&#39;&#39;)
                for col in row:
                    if len(str(col)) &gt; maxColWidth:
                        print((str(col)[:maxColWidth-3]+&#39;...&#39;).ljust(maxColWidth), end=&#39;  &#39;)
                    else:
                        print(str(col)[:maxColWidth].ljust(maxColWidth), end=&#39;  &#39;)
                print()
            if dataReadInfoDF.shape[0] &gt; maxLength:
                endline = f&#39;\t...{dataReadInfoDF.shape[0]-maxLength} more rows in file.\n&#39;
            else:
                endline = &#39;\n&#39;
            print(endline)

            print(&#39;Fetching the following files:&#39;)
        param_dict_list = []
        verboseStatement = []
        if param_col is None: #Not a single parameter column, each col=parameter
            for row_ind in range(dataReadInfoDF.shape[0]):
                param_dict = {}
                verboseStatement.append([])
                for col in dataReadInfoDF.columns:
                    if col in input_params_params or col in get_metadata_params or col in fetch_data_params:
                        currParam = dataReadInfoDF.loc[row_ind, col]
                        if pd.isna(currParam) or currParam == &#39;nan&#39;:
                            if col in default_dict.keys():
                                param_dict[col] = default_dict[col] #Get default value
                                if verbose:
                                    if type(default_dict[col]) is str:
                                        verboseStatement[row_ind].append(&#34;\t\t&#39;{}&#39; parameter not specified in batch file. Using {}=&#39;{}&#39;&#34;.format(col, col, default_dict[col]))
                                    else:
                                        verboseStatement[row_ind].append(&#34;\t\t&#39;{}&#39; parameter not specified in batch file. Using {}={}&#34;.format(col, col, default_dict[col]))
                            else:
                                param_dict[col] = None
                        else:
                            param_dict[col] = dataReadInfoDF.loc[row_ind, col]
                param_dict_list.append(param_dict)
        else:
            if param_col not in dataReadInfoDF.columns:
                raise IndexError(&#39;{} is not a column in {} (columns are: {})&#39;.format(param_col, input_data, dataReadInfoDF.columns))
            for row in dataReadInfoDF[param_col]:
                param_dict = {}
                splitRow = str(row).split(&#39;,&#39;)
                for item in splitRow:
                    param_dict[item.split(&#39;=&#39;)[0]] = item.split(&#39;=&#39;)[1]
                param_dict_list.append(param_dict)
        #input_params(datapath,site,network,station,loc,channels, acq_date,starttime, endtime, tzone, xcoord, ycoord, elevation, depth, instrument, metapath, hvsr_band)
        #fetch_data(params, inv, source, trim_dir, export_format, detrend, detrend_order, verbose)
        #get_metadata(params, write_path)
    elif batch_type == &#39;filelist&#39;:
        if isinstance(batch_params, list):
            if len(batch_params) != len(input_data):
                raise RuntimeError(&#39;If batch_params is list, it must be the same length as input_data. len(batch_params)={} != len(input_data)={}&#39;.format(len(batch_params), len(input_data)))
            param_dict_list = batch_params
        elif isinstance(batch_params, dict):
            batch_params.update(readcsv_getMeta_fetch_kwargs)
            param_dict_list = []
            for i in range(len(input_data)):
                param_dict_list.append(batch_params)
        
        # Read and process each MiniSEED file
        for i, file in enumerate(input_data):
            if isinstance(file, obspy.core.stream.Stream):
                warnings.warn(&#39;Reading in a list of Obspy streams is not currently supported, but may be implemented in the future&#39;, FutureWarning)
                pass 
            else:
                param_dict_list[i][&#39;datapath&#39;] = file

    hvsr_metaDict = {}
    zfillDigs = len(str(len(param_dict_list))) #Get number of digits of length of param_dict_list
    i=0
    for i, param_dict in enumerate(param_dict_list):
        # Read the data file into a Stream object
        input_params_kwargs = {k: v for k, v in locals()[&#39;readcsv_getMeta_fetch_kwargs&#39;].items() if k in input_params.__code__.co_varnames}
        input_params_kwargs2 = {k: v for k, v in param_dict.items() if k in input_params.__code__.co_varnames}
        input_params_kwargs.update(input_params_kwargs2)

        params = input_params(**input_params_kwargs)

        fetch_data_kwargs = {k: v for k, v in locals()[&#39;readcsv_getMeta_fetch_kwargs&#39;].items() if k in fetch_data.__code__.co_varnames}
        fetch_data_kwargs2 = {k: v for k, v in param_dict.items() if k in fetch_data.__code__.co_varnames[0:7]}
        fetch_data_kwargs.update(fetch_data_kwargs2)
        
        try:
            params = fetch_data(params=params, **fetch_data_kwargs)
        except:
            params[&#39;ProcessingStatus&#39;][&#39;FetchDataStatus&#39;]=False
            params[&#39;ProcessingStatus&#39;][&#39;OverallStatus&#39;] = False            
        
        if verbose and params[&#39;ProcessingStatus&#39;][&#39;FetchDataStatus&#39;]:
            print(&#34;\t  {}&#34;.format(params[&#39;site&#39;]))
            if verboseStatement !=[]:
                for item in verboseStatement[i]:
                    print(item)
        elif verbose and not params[&#39;ProcessingStatus&#39;][&#39;FetchDataStatus&#39;]:
            print(&#34;\t  {} not read correctly. Processing will not be carried out.&#34;.format(params[&#39;site&#39;]))
                
        params[&#39;batch&#39;] = True

        if params[&#39;site&#39;] == default_dict[&#39;site&#39;]: #If site was not designated
            params[&#39;site&#39;] = &#34;{}_{}&#34;.format(params[&#39;site&#39;], str(i).zfill(zfillDigs))
            i+=1
        hvsr_metaDict[params[&#39;site&#39;]] = params

    hvsr_metaDict = HVSRBatch(hvsr_metaDict)

    return hvsr_metaDict

#Just for testing
def test_function():
    print(&#39;is this working?&#39;)

#BATCH FUNCTIONS: various functions that are used to help the regular functions handle batch data
#Helper function for batch processing of check_peaks
def _check_peaks_batch(**check_peaks_kwargs):
    try:
        hvsr_data = check_peaks(**check_peaks_kwargs)
        if check_peaks_kwargs[&#39;verbose&#39;]:
            print(&#39;\t{} succesfully completed check_peaks()&#39;.format(hvsr_data[&#39;input_params&#39;][&#39;site&#39;]))    
    except:
        warnings.warn(f&#34;Error in check_peaks({check_peaks_kwargs[&#39;hvsr_data&#39;][&#39;input_params&#39;][&#39;site&#39;]}, **check_peaks_kwargs)&#34;, RuntimeWarning)
        hvsr_data = check_peaks_kwargs[&#39;hvsr_data&#39;]
        
    return hvsr_data

#Support function for running batch
def _generate_ppsds_batch(**generate_ppsds_kwargs):
    try:
        params = generate_ppsds(**generate_ppsds_kwargs)
        if generate_ppsds_kwargs[&#39;verbose&#39;]:
            print(&#39;\t{} successfully completed generate_ppsds()&#39;.format(params[&#39;site&#39;]))
    except Exception as e:
        print(e)
        warnings.warn(f&#34;Error in generate_ppsds({generate_ppsds_kwargs[&#39;params&#39;][&#39;site&#39;]}, **generate_ppsds_kwargs)&#34;, RuntimeWarning)
        params = generate_ppsds_kwargs[&#39;params&#39;]
        
    return params

#Helper function for batch processing of get_report
def _get_report_batch(**get_report_kwargs):

    try:
        hvsr_results = get_report(**get_report_kwargs)
        #Print if verbose, but selected report_format was not print
        print(&#39;\n\n\n&#39;) #add some &#39;whitespace&#39;
        if get_report_kwargs[&#39;verbose&#39;]:
            if &#39;print&#39; in get_report_kwargs[&#39;report_format&#39;]:
                pass
            else:
                get_report_kwargs[&#39;report_format&#39;] = &#39;print&#39;
                get_report(**get_report_kwargs)
        
    except:
        warnMsg = f&#34;Error in get_report({get_report_kwargs[&#39;hvsr_results&#39;][&#39;input_params&#39;][&#39;site&#39;]}, **get_report_kwargs)&#34;
        if get_report_kwargs[&#39;verbose&#39;]:
            print(&#39;\t&#39;+warnMsg)
        else:
            warnings.warn(warnMsg, RuntimeWarning)
        hvsr_results = get_report_kwargs[&#39;hvsr_results&#39;]
        
    return hvsr_results

#Helper function for batch procesing of remove_noise
def __remove_noise_batch(**remove_noise_kwargs):
    try:
        hvsr_data = remove_noise(**remove_noise_kwargs)

        if remove_noise_kwargs[&#39;verbose&#39;]:
            if &#39;input_params&#39; in hvsr_data.keys():
                print(&#39;\t{} successfully completed remove_noise()&#39;.format(hvsr_data[&#39;input_params&#39;][&#39;site&#39;]))
            elif &#39;site&#39; in hvsr_data.keys():
                print(&#39;\t{} successfully completed remove_noise()&#39;.format(hvsr_data[&#39;site&#39;]))
    except:
        warnings.warn(f&#34;Error in remove_noise({remove_noise_kwargs[&#39;input&#39;][&#39;site&#39;]}, **remove_noise_kwargs)&#34;, RuntimeWarning)

    return hvsr_data

#Batch function for plot_hvsr()
def _hvsr_plot_batch(**hvsr_plot_kwargs):
    try:
        hvsr_data = plot_hvsr(**hvsr_plot_kwargs)
    except:
        warnings.warn(f&#34;Error in plotting ({hvsr_plot_kwargs[&#39;hvsr_data&#39;][&#39;input_params&#39;][&#39;site&#39;]}, **hvsr_plot_kwargs)&#34;, RuntimeWarning)
        hvsr_data = hvsr_plot_kwargs[&#39;hvsr_data&#39;]
        
    return hvsr_data

#Helper function for batch version of process_hvsr()
def _process_hvsr_batch(**process_hvsr_kwargs):
    try:
        hvsr_data = process_hvsr(**process_hvsr_kwargs)
        if process_hvsr_kwargs[&#39;verbose&#39;]:
            print(&#39;\t{} successfully completed process_hvsr()&#39;.format(hvsr_data[&#39;input_params&#39;][&#39;site&#39;]))
    except:
        errMsg=f&#34;Error in process_hvsr({process_hvsr_kwargs[&#39;params&#39;][&#39;site&#39;]}, **process_hvsr_kwargs)&#34;
        if process_hvsr_kwargs[&#39;verbose&#39;]:
            print(&#39;\t&#39;+errMsg)
        else:
            warnings.warn(errMsg, RuntimeWarning)
        hvsr_data = process_hvsr_kwargs[&#39;params&#39;]
        
    return hvsr_data

#Special helper function that checks the processing status at each stage of processing to help determine if any processing steps were skipped
def _check_processing_status(hvsr_data):
    &#34;&#34;&#34;Internal function to check processing status, used primarily in the sprit.run() function to allow processing to continue if one site is bad.

    Parameters
    ----------
    hvsr_data : sprit.HVSRData
        Data being processed

    Returns
    -------
    sprit.HVSRData
        Data being processed, with updated the &#39;OverallStatus&#39; key of the attribute ProcessingStatus updated.
    &#34;&#34;&#34;
    
    if isinstance(hvsr_data, HVSRData):
        siteName = hvsr_data[&#39;site&#39;]
        hvsr_interim = {siteName: hvsr_data}
    else:
        hvsr_interim = hvsr_data
        
    for sitename in hvsr_interim.keys():
        statusOK = True
        for status_type, status_value in hvsr_interim[sitename][&#39;ProcessingStatus&#39;].items():
            if not status_value and status_type != &#39;RemoveNoiseStatus&#39;:
                statusOK = False
                
        if statusOK:
            hvsr_interim[sitename][&#39;ProcessingStatus&#39;][&#39;OverallStatus&#39;] = True
        else:
            hvsr_interim[sitename][&#39;ProcessingStatus&#39;][&#39;OverallStatus&#39;] = False

    if isinstance(hvsr_data, HVSRData):
        hvsr_data = hvsr_interim[siteName]
    return hvsr_data    

#HELPER functions for fetch_data() and get_metadata()
#Read in metadata .inv file, specifically for RaspShake
def _update_shake_metadata(filepath, params, write_path=&#39;&#39;):
    &#34;&#34;&#34;Reads static metadata file provided for Rasp Shake and updates with input parameters. Used primarily in the get_metadata() function.

        PARAMETERS
        ----------
        filepath : str or pathlib.Path object
            Filepath to metadata file. Should be a file format supported by obspy.read_inventory().
        params : dict
            Dictionary containing necessary keys/values for updating, currently only supported for STATIONXML with Raspberry Shakes.
                Necessary keys: &#39;net&#39;, &#39;sta&#39;, 
                Optional keys: &#39;longitude&#39;, &#39;latitude&#39;, &#39;elevation&#39;, &#39;depth&#39;
        write_path   : str, default=&#39;&#39;
            If specified, filepath to write to updated inventory file to.

        Returns
        -------
        params : dict
            Updated params dict with new key:value pair with updated updated obspy.inventory object (key=&#34;inv&#34;)
    &#34;&#34;&#34;

    network = params[&#39;net&#39;]
    station = params[&#39;sta&#39;]
    optKeys = [&#39;longitude&#39;, &#39;latitude&#39;, &#39;elevation&#39;, &#39;depth&#39;]
    for k in optKeys:
        if k not in params.keys():
            params[k] = &#39;0&#39;
    xcoord = str(params[&#39;longitude&#39;])
    ycoord = str(params[&#39;latitude&#39;])
    elevation = str(params[&#39;elevation&#39;])
    depth = str(params[&#39;depth&#39;])
    
    startdate = str(datetime.datetime(year=2023, month=2, day=15)) #First day sprit code worked :)
    enddate=str(datetime.datetime.today())

    filepath = sprit_utils.checkifpath(filepath)

    tree = ET.parse(str(filepath))
    root = tree.getroot()

    prefix= &#34;{http://www.fdsn.org/xml/station/1}&#34;

    for item in root.iter(prefix+&#39;Channel&#39;):
        item.attrib[&#39;startDate&#39;] = startdate
        item.attrib[&#39;endDate&#39;] = enddate

    for item in root.iter(prefix+&#39;Station&#39;):
        item.attrib[&#39;code&#39;] = station
        item.attrib[&#39;startDate&#39;] = startdate
        item.attrib[&#39;endDate&#39;] = enddate

    for item in root.iter(prefix+&#39;Network&#39;):
        item.attrib[&#39;code&#39;] = network
        
    for item in root.iter(prefix+&#39;Latitude&#39;):
        item.text = ycoord

    for item in root.iter(prefix+&#39;Longitude&#39;):
        item.text = xcoord

    for item in root.iter(prefix+&#39;Created&#39;):
        nowTime = str(datetime.datetime.now())
        item.text = nowTime

    for item in root.iter(prefix+&#39;Elevation&#39;):
        item.text= elevation

    for item in root.iter(prefix+&#39;Depth&#39;):
        item.text=depth

    #Set up (and) export
    #filetag = &#39;_&#39;+str(datetime.datetime.today().date())
    #outfile = str(parentPath)+&#39;\\&#39;+filename+filetag+&#39;.inv&#39;

    if write_path != &#39;&#39;:
        try:
            write_path = pathlib.Path(write_path)
            if write_path.isdir():
                fname = params[&#39;network&#39;]+&#39;_&#39;+params[&#39;station&#39;]+&#39;_&#39;+params[&#39;site&#39;]
                fname = fname + &#39;_response.xml&#39;
                write_file = write_path.joinpath(fname)
            else:
                write_file=write_path
            tree.write(write_file, xml_declaration=True, method=&#39;xml&#39;,encoding=&#39;UTF-8&#39;)
            inv = obspy.read_inventory(write_file, format=&#39;STATIONXML&#39;, level=&#39;response&#39;)
        except:
            warnings.warn(f&#39;write_path={write_path} is not recognized as a filepath, updated metadata file will not be written&#39;)
            write_path=&#39;&#39;
    else:
        #Create temporary file for reading into obspy
        tpf = tempfile.NamedTemporaryFile(delete=False)
        stringRoot = ET.tostring(root, encoding=&#39;UTF-8&#39;, method=&#39;xml&#39;)
        tpf.write(stringRoot)

        inv = obspy.read_inventory(tpf.name, format=&#39;STATIONXML&#39;, level=&#39;response&#39;)
        tpf.close()

        os.remove(tpf.name)
    params[&#39;inv&#39;] = inv
    params[&#39;params&#39;][&#39;inv&#39;] = inv
    return params

#Support function for get_metadata()
def _read_RS_Metadata(params, source=None):
    &#34;&#34;&#34;Function to read the metadata from Raspberry Shake using the StationXML file provided by the company.
    Intended to be used within the get_metadata() function.

    Parameters
    ----------
    params : dict
        The parameter dictionary output from input_params() and read into get_metadata()

    Returns
    -------
    params : dict
        Further modified parameter dictionary
    &#34;&#34;&#34;
    if &#39;inv&#39; in params.keys():
        inv = params[&#39;inv&#39;]
    else:
        sprit_utils.checkifpath(params[&#39;metapath&#39;])
        inv = obspy.read_inventory(params[&#39;metapath&#39;], format=&#39;STATIONXML&#39;, level=&#39;response&#39;)
        params[&#39;inv&#39;] = inv

    station = params[&#39;sta&#39;]
    network = params[&#39;net&#39;]
    channels = params[&#39;cha&#39;]

    if isinstance(inv, obspy.core.inventory.inventory.Inventory):
        #Create temporary file from inventory object
        tpf = tempfile.NamedTemporaryFile(delete=False)
        inv.write(tpf.name, format=&#39;STATIONXML&#39;)

        #Read data into xmlTree
        tree = ET.parse(tpf.name)
        root = tree.getroot()

        #Close and remove temporary file
        tpf.close()
        os.remove(tpf.name)
    else:
        inv = sprit_utils.checkifpath(inv)
        inv = obspy.read_inventory(params[&#39;metapath&#39;], format=&#39;STATIONXML&#39;, level=&#39;response&#39;)
        params[&#39;inv&#39;] = inv
        tree = ET.parse(inv)
        root = tree.getroot()

    #if write_path != &#39;&#39;:
    #    inv.write(write_path, format=&#39;STATIONXML&#39;)

    #This is specific to RaspShake
    c=channels[0]
    pzList = [str(n) for n in list(range(7))]
    s=pzList[0]

    prefix= &#34;{http://www.fdsn.org/xml/station/1}&#34;

    sensitivityPath = &#34;./&#34;+prefix+&#34;Network[@code=&#39;&#34;+network+&#34;&#39;]/&#34;+prefix+&#34;Station[@code=&#39;&#34;+station+&#34;&#39;]/&#34;+prefix+&#34;Channel[@code=&#39;&#34;+c+&#34;&#39;]/&#34;+prefix+&#34;Response/&#34;+prefix+&#34;InstrumentSensitivity/&#34;+prefix+&#34;Value&#34;
    gainPath = &#34;./&#34;+prefix+&#34;Network[@code=&#39;&#34;+network+&#34;&#39;]/&#34;+prefix+&#34;Station[@code=&#39;&#34;+station+&#34;&#39;]/&#34;+prefix+&#34;Channel[@code=&#39;&#34;+c+&#34;&#39;]/&#34;+prefix+&#34;Response/&#34;+prefix+&#34;Stage[@number=&#39;1&#39;]/&#34;+prefix+&#34;StageGain/&#34;+prefix+&#34;Value&#34;

    #paz = []
    rsCList = [&#39;EHZ&#39;, &#39;EHN&#39;, &#39;EHE&#39;]
    paz = {}
    for c in channels:
        channelPaz = {}
        #channelPaz[&#39;channel&#39;] = c
        for item in root.findall(sensitivityPath):
            channelPaz[&#39;sensitivity&#39;]=float(item.text)

        for item in root.findall(gainPath):
            channelPaz[&#39;gain&#39;]=float(item.text)
        
        poleList = []
        zeroList = []
        for s in pzList:
            if int(s) &lt; 4:
                polePathReal = &#34;./&#34;+prefix+&#34;Network[@code=&#39;&#34;+network+&#34;&#39;]/&#34;+prefix+&#34;Station[@code=&#39;&#34;+station+&#34;&#39;]/&#34;+prefix+&#34;Channel[@code=&#39;&#34;+c+&#34;&#39;]/&#34;+prefix+&#34;Response/&#34;+prefix+&#34;Stage[@number=&#39;1&#39;]/&#34;+prefix+&#34;PolesZeros/&#34;+prefix+&#34;Pole[@number=&#39;&#34;+s+&#34;&#39;]/&#34;+prefix+&#34;Real&#34;
                polePathImag = &#34;./&#34;+prefix+&#34;Network[@code=&#39;&#34;+network+&#34;&#39;]/&#34;+prefix+&#34;Station[@code=&#39;&#34;+station+&#34;&#39;]/&#34;+prefix+&#34;Channel[@code=&#39;&#34;+c+&#34;&#39;]/&#34;+prefix+&#34;Response/&#34;+prefix+&#34;Stage[@number=&#39;1&#39;]/&#34;+prefix+&#34;PolesZeros/&#34;+prefix+&#34;Pole[@number=&#39;&#34;+s+&#34;&#39;]/&#34;+prefix+&#34;Imaginary&#34;
                for poleItem in root.findall(polePathReal):
                    poleReal = poleItem.text
                for poleItem in root.findall(polePathImag):
                    pole = complex(float(poleReal), float(poleItem.text))
                    poleList.append(pole)
                    channelPaz[&#39;poles&#39;] = poleList
                    #channelPaz[&#39;poles&#39;] = list(set(poleList))
            else:
                zeroPathReal = &#34;./&#34;+prefix+&#34;Network[@code=&#39;&#34;+network+&#34;&#39;]/&#34;+prefix+&#34;Station[@code=&#39;&#34;+station+&#34;&#39;]/&#34;+prefix+&#34;Channel[@code=&#39;&#34;+c+&#34;&#39;]/&#34;+prefix+&#34;Response/&#34;+prefix+&#34;Stage[@number=&#39;1&#39;]/&#34;+prefix+&#34;PolesZeros/&#34;+prefix+&#34;Zero[@number=&#39;&#34;+s+&#34;&#39;]/&#34;+prefix+&#34;Real&#34;
                zeroPathImag = &#34;./&#34;+prefix+&#34;Network[@code=&#39;&#34;+network+&#34;&#39;]/&#34;+prefix+&#34;Station[@code=&#39;&#34;+station+&#34;&#39;]/&#34;+prefix+&#34;Channel[@code=&#39;&#34;+c+&#34;&#39;]/&#34;+prefix+&#34;Response/&#34;+prefix+&#34;Stage[@number=&#39;1&#39;]/&#34;+prefix+&#34;PolesZeros/&#34;+prefix+&#34;Zero[@number=&#39;&#34;+s+&#34;&#39;]/&#34;+prefix+&#34;Imaginary&#34;
                for zeroItem in root.findall(zeroPathReal):
                    zeroReal = zeroItem.text
                
                for zeroItem in root.findall(zeroPathImag):
                    zero = complex(float(zeroReal), float(zeroItem.text))
                    #zero = zeroReal + &#34;+&#34; + zeroItem.text+&#39;j&#39;
                    zeroList.append(zero)
                    #channelPaz[&#39;zeros&#39;] = list(set(zeroList))
                    channelPaz[&#39;zeros&#39;] = zeroList
        if str(c).upper() in rsCList:
            c = str(c)[-1].upper()
        paz[str(c)] = channelPaz
    params[&#39;paz&#39;] = paz
    params[&#39;params&#39;][&#39;paz&#39;] = paz

    return params

#Helper function to sort channels
def _sort_channels(input, source, verbose):
    if source!=&#39;batch&#39;:
        input = {&#39;SITENAME&#39;: {&#39;stream&#39;:input}} #Make same structure as batch

    for site in input.keys():
        rawDataIN = input[site][&#39;stream&#39;]

        if rawDataIN is None:
            if verbose:
                raise RuntimeError(&#34;No data was read using specified parameters {}&#34;.format(input[site]))
            else:
                raise RuntimeError(&#34;No data was read using specified parameters&#34;)

        elif isinstance(rawDataIN, obspy.core.stream.Stream):
            #Make sure z component is first
            dataIN = rawDataIN.sort([&#39;channel&#39;], reverse=True) #z, n, e order
        else:
            #Not usually used anymore, retained just in case
            dataIN = []
            for i, st in enumerate(rawDataIN):
                if &#39;Z&#39; in st[0].stats[&#39;channel&#39;]:#).split(&#39;.&#39;)[3]:#[12:15]:
                    dataIN.append(rawDataIN[i])
                else:
                    dataIN.append(rawDataIN[i].sort([&#39;channel&#39;], reverse=True)) #z, n, e order            

        input[site][&#39;stream&#39;] = dataIN
            
    if source==&#39;batch&#39;:
        #Return a dict
        output = input
    else:
        #Return a stream otherwise
        output = input[site][&#39;stream&#39;]
    return output

#Trim data 
def _trim_data(input, stream=None, export_dir=None, export_format=None, source=None, **kwargs):
    &#34;&#34;&#34;Function to trim data to start and end time

        Trim data to start and end times so that stream being analyzed only contains wanted data.
        Can also export data to specified directory using a specified site name and/or export_format

        Parameters
        ----------
            input  : HVSRData
                HVSR Data class containing input parameters for trimming
            stream  : obspy.stream object  
                Obspy stream to be trimmed
            export_dir: str or pathlib obj   
                Output filepath to export trimmed data to. If not specified, does not export. 
            export_format  : str or None, default=None  
                If None, and export_dir is specified, format defaults to .mseed. Otherwise, exports trimmed stream using obspy.core.stream.Stream.write() method, with export_format being passed to the format argument. 
                https://docs.obspy.org/packages/autogen/obspy.core.stream.Stream.write.html#obspy.core.stream.Stream.write
            **kwargs
                Keyword arguments passed directly to obspy.core.stream.Stream.trim() method.
                
        Returns
        -------
            st_trimmed  : obspy.stream object 
                Obpsy Stream trimmed to start and end times
    &#34;&#34;&#34;
    #if source!=&#39;batch&#39;:
    #    #input = {&#39;SITENAME&#39;: {&#39;stream&#39;:input}} #Make same structure as batch
    #    pass

    if &#39;starttime&#39; in kwargs.keys():
        start = kwargs[&#39;starttime&#39;]
    elif isinstance(input, (HVSRData, dict)):
        start = input[&#39;starttime&#39;]
    
    if &#39;endtime&#39; in kwargs.keys():
        end = kwargs[&#39;endtime&#39;]
    else:
        end = input[&#39;endtime&#39;]
        
    if &#39;site&#39; in kwargs.keys():
        site = kwargs[&#39;site&#39;]
    else:
        site = input[&#39;site&#39;]

    if stream is not None:
        st_trimmed = stream.copy()
    elif &#39;stream&#39; in input.keys():
        st_trimmed = input[&#39;stream&#39;].copy()
    else:
        raise UnboundLocalError(&#34;stream not specified. Must either be specified using stream parameter or as a key in the input parameters (input[&#39;stream&#39;])&#34;)
        
    trimStart = obspy.UTCDateTime(start)
    trimEnd = obspy.UTCDateTime(end)

    #If data is contained in a masked array, split to undo masked array
    if isinstance(st_trimmed[0].data, np.ma.masked_array):
        st_trimmed = st_trimmed.split()
        #This split is undone with the .merge() method a few lines down

    for tr in st_trimmed:
        if trimStart &gt; tr.stats.endtime or trimEnd &lt; tr.stats.starttime:
            pass
        else:
            st_trimmed.trim(starttime=trimStart, endtime=trimEnd, **kwargs)

    st_trimmed.merge(method=1)

    if export_format is None:
        export_format = &#39;.mseed&#39;

    #Format export filepath, if exporting
    if export_dir is not None:
        if site is None:
            site=&#39;&#39;
        else:
            site = site+&#39;_&#39;
        if &#39;.&#39; not in export_format:
            export_format = &#39;.&#39;+export_format
        net = st_trimmed[0].stats.network
        sta = st_trimmed[0].stats.station
        loc = st_trimmed[0].stats.location
        yr = str(st_trimmed[0].stats.starttime.year)
        strtD=str(st_trimmed[0].stats.starttime.date)
        strtT=str(st_trimmed[0].stats.starttime.time)[0:2]
        strtT=strtT+str(st_trimmed[0].stats.starttime.time)[3:5]
        endT = str(st_trimmed[0].stats.endtime.time)[0:2]
        endT = endT+str(st_trimmed[0].stats.endtime.time)[3:5]
        doy = str(st_trimmed[0].stats.starttime.utctimetuple().tm_yday).zfill(3)

        export_dir = sprit_utils.checkifpath(export_dir)
        export_dir = str(export_dir)
        export_dir = export_dir.replace(&#39;\\&#39;, &#39;/&#39;)
        export_dir = export_dir.replace(&#39;\\&#39;[0], &#39;/&#39;)

        if type(export_format) is str:
            filename = site+net+&#39;.&#39;+sta+&#39;.&#39;+loc+&#39;.&#39;+yr+&#39;.&#39;+doy+&#39;_&#39;+strtD+&#39;_&#39;+strtT+&#39;-&#39;+endT+export_format
        elif type(export_format) is bool:
            filename = site+net+&#39;.&#39;+sta+&#39;.&#39;+loc+&#39;.&#39;+yr+&#39;.&#39;+doy+&#39;_&#39;+strtD+&#39;_&#39;+strtT+&#39;-&#39;+endT+&#39;.mseed&#39;

        if export_dir[-1]==&#39;/&#39;:
            export_dir=export_dir[:-1]
        
        exportFile = export_dir+&#39;/&#39;+filename

        #Take care of masked arrays for writing purposes
        if &#39;fill_value&#39; in kwargs.keys():
            for tr in st_trimmed:
                if isinstance(tr.data, np.ma.masked_array):
                    tr.data = tr.data.filled(kwargs[&#39;fill_value&#39;])
        else:
            st_trimmed = st_trimmed.split()
        
        st_trimmed.write(filename=exportFile)
    else:
        pass

    return st_trimmed

#Helper function to detrend data
def __detrend_data(input, detrend, detrend_order, verbose, source):
    &#34;&#34;&#34;Helper function to detrend data, specifically formatted for the HVSRData and HVSRBatch objects&#34;&#34;&#34;
    if source!=&#39;batch&#39;:
        input = {&#39;SITENAME&#39;: {&#39;stream&#39;:input}} #Make same structure as batch


    for key in input.keys():
        dataIN = input[key][&#39;stream&#39;]
        if detrend==False:
            pass
        elif detrend==True:
            #By default, do a spline removal
            for tr in dataIN:
                tr.detrend(type=&#39;spline&#39;, order=detrend_order, dspline=1000)        
        else:
            data_undetrended = dataIN.copy()
            try:
                if detrend==&#39;simple&#39;:
                    for tr in dataIN:
                        tr.detrend(type=detrend)
                if detrend==&#39;linear&#39;:
                    for tr in dataIN:
                        tr.detrend(type=detrend)
                if detrend==&#39;constant&#39; or detrend==&#39;demean&#39;:
                    for tr in dataIN:
                        tr.detrend(type=detrend)                
                if detrend==&#39;polynomial&#39;:
                    for tr in dataIN:
                        tr.detrend(type=detrend, order=detrend_order)   
                if detrend==&#39;spline&#39;:
                    for tr in dataIN:
                        tr.detrend(type=detrend, order=detrend_order, dspline=1000)       
            except:
                dataIN = data_undetrended
                if verbose:
                    warnings.warn(&#34;Detrend error, data not detrended&#34;, UserWarning)
        
        input[key][&#39;stream&#39;] = dataIN

    if source==&#39;batch&#39;:
        #Return a dict
        output = input
    else:
        #Return a stream otherwise
        output = input[key][&#39;stream&#39;]
    return output

#Read data from raspberry shake
def __read_RS_file_struct(datapath, source, year, doy, inv, params, verbose=False):
    &#34;&#34;&#34;&#34;Private function used by fetch_data() to read in Raspberry Shake data&#34;&#34;&#34;
    from obspy.core import UTCDateTime
    fileList = []
    folderPathList = []
    filesinfolder = False
    datapath = sprit_utils.checkifpath(datapath)
    #Read RS files
    if source==&#39;raw&#39;: #raw data with individual files per trace
        if datapath.is_dir():
            for child in datapath.iterdir():
                if child.is_file() and child.name.startswith(&#39;AM&#39;) and str(doy).zfill(3) in child.name and str(year) in child.name:
                    filesinfolder = True
                    folderPathList.append(datapath)
                    fileList.append(child)
                elif child.is_dir() and child.name.startswith(&#39;EH&#39;) and not filesinfolder:
                    folderPathList.append(child)
                    for c in child.iterdir():
                        if c.is_file() and c.name.startswith(&#39;AM&#39;) and c.name.endswith(str(doy).zfill(3)) and str(year) in c.name:
                            fileList.append(c)


            if len(fileList) == 0:
                doyList = []
                printList= []
                for j, folder in enumerate(folderPathList):
                    for i, file in enumerate(folder.iterdir()):
                        if j ==0:
                            doyList.append(str(year) + &#39; &#39; + str(file.name[-3:]))
                            printList.append(f&#34;{datetime.datetime.strptime(doyList[i], &#39;%Y %j&#39;).strftime(&#39;%b %d&#39;)} | Day of year: {file.name[-3:]}&#34;)
                if len(printList) == 0:
                    warnings.warn(&#39;No files found matching Raspberry Shake data structure or files in specified directory.&#39;)
                else:
                    warnings.warn(f&#39;No file found for specified date: {params[&#34;acq_date&#34;]}. The following days/files exist for specified year in this directory&#39;)
                    for p in printList:
                        print(&#39;\t&#39;,p)
                return None
            elif len(fileList) !=3:
                warnings.warn(&#39;3 channels needed! {} found.&#39;.format(len(folderPathList)), UserWarning)
            else:
                fileList.sort(reverse=True) # Puts z channel first
                folderPathList.sort(reverse=True)
                if verbose:
                    print(&#39;Reading files: \n\t{}\n\t{}\n\t{}&#39;.format(fileList[0].name, fileList[1].name, fileList[2].name))

            traceList = []
            for i, f in enumerate(fileList):
                with warnings.catch_warnings():
                    warnings.filterwarnings(action=&#39;ignore&#39;, message=&#39;^readMSEEDBuffer()&#39;)
                    st = obspy.read(str(f), starttime=UTCDateTime(params[&#39;starttime&#39;]), endtime=UTCDateTime(params[&#39;endtime&#39;]), nearest_sample=False)
                    st.merge()
                    tr = (st[0])
                    #tr= obspy.Trace(tr.data,header=meta)
                    traceList.append(tr)
            rawDataIN = obspy.Stream(traceList)
            with warnings.catch_warnings():
                warnings.filterwarnings(action=&#39;ignore&#39;, message=&#39;Found more than one matching response.*&#39;)
                rawDataIN.attach_response(inv)
        else:
            rawDataIN = obspy.read(str(datapath), starttime=UTCDateTime(params[&#39;starttime&#39;]), endttime=UTCDateTime(params[&#39;endtime&#39;]), nearest_sample=True)
            rawDataIN.attach_response(inv)
    elif source==&#39;dir&#39;: #files with 3 traces, but may be several in a directory or only directory name provided
        obspyFormats = [&#39;AH&#39;,&#39;ALSEP_PSE&#39;,&#39;ALSEP_WTH&#39;,&#39;ALSEP_WTN&#39;,&#39;CSS&#39;,&#39;DMX&#39;,&#39;GCF&#39;,&#39;GSE1&#39;,&#39;GSE2&#39;,&#39;KINEMETRICS_EVT&#39;,&#39;MSEED&#39;,&#39;NNSA_KB_CORE&#39;,&#39;PDAS&#39;,&#39;PICKLE&#39;,&#39;Q&#39;,&#39;REFTEK130&#39;,&#39;RG16&#39;,&#39;SAC&#39;,&#39;SACXY&#39;,&#39;SEG2&#39;,&#39;SEGY&#39;,&#39;SEISAN&#39;,&#39;SH_ASC&#39;,&#39;SLIST&#39;,&#39;SU&#39;,&#39;TSPAIR&#39;,&#39;WAV&#39;,&#39;WIN&#39;,&#39;Y&#39;]
        for file in datapath.iterdir():
            ext = file.suffix[1:]
            rawFormat = False
            if ext.isnumeric():
                if float(ext) &gt;= 0 and float(ext) &lt; 367:
                    rawFormat=True
            
            if ext.upper() in obspyFormats or rawFormat:
                filesinfolder = True
                folderPathList.append(datapath)
                fileList.append(file.name)
                        
        filepaths = []
        rawDataIN = obspy.Stream()
        for i, f in enumerate(fileList):
            filepaths.append(folderPathList[i].joinpath(f))
            #filepaths[i] = pathlib.Path(filepaths[i])
            currData = obspy.read(filepaths[i])
            currData.merge()
            #rawDataIN.append(currData)
            #if i == 0:
            #    rawDataIN = currData.copy()
            if isinstance(currData, obspy.core.stream.Stream):
                rawDataIN += currData.copy()
        #rawDataIN = obspy.Stream(rawDataIN)
        rawDataIN.attach_response(inv)  
        if type(rawDataIN) is list and len(rawDataIN)==1:
            rawDataIN = rawDataIN[0]
    elif source==&#39;file&#39;:
        rawDataIN = obspy.read(str(datapath), starttime=UTCDateTime(params[&#39;starttime&#39;]), endttime=UTCDateTime(params[&#39;endtime&#39;]), nearest=True)
        rawDataIN.merge()   
        rawDataIN.attach_response(inv)
    elif type(source) is list or type(datapath) is list:
        pass #Eventually do something
        rawDataIN.attach_response(inv)

    return rawDataIN

##Helper functions for remove_noise()
#Helper function for removing gaps
def __remove_gaps(stream, window_gaps_obspy):
    &#34;&#34;&#34;Helper function for removing gaps&#34;&#34;&#34;
    #combine overlapping windows
    overlapList = []
    for i in range(len(window_gaps_obspy)-2):
        if window_gaps_obspy[i][1] &gt; window_gaps_obspy[i+1][0]:
            overlapList.append(i)

    for i, t in enumerate(overlapList):
        if i &lt; len(window_gaps_obspy)-2:
            window_gaps_obspy[i][1] = window_gaps_obspy[i+1][1]
            window_gaps_obspy.pop(i+1)

    #Add streams
    window_gaps_s = []
    for w, win in enumerate(window_gaps_obspy):
        if w == 0:
            pass
        elif w == len(window_gaps_obspy)-1:
            pass
        else:
            window_gaps_s.append(win[1]-win[0])

    if len(window_gaps_s) &gt; 0:
        stream_windows = []
        j = 0
        for i, window in enumerate(window_gaps_s):
            j=i
            newSt = stream.copy()
            stream_windows.append(newSt.trim(starttime=window_gaps_obspy[i][1], endtime=window_gaps_obspy[i+1][0]))
        i = j + 1
        newSt = stream.copy()
        stream_windows.append(newSt.trim(starttime=window_gaps_obspy[i][1], endtime=window_gaps_obspy[i+1][0]))

        for i, st in enumerate(stream_windows):
            if i == 0:
                outStream = st.copy()
            else:
                newSt = st.copy()
                gap = window_gaps_s[i-1]
                outStream = outStream + newSt.trim(starttime=st[0].stats.starttime - gap, pad=True, fill_value=None)       
        outStream.merge()
    else:
        outStream = stream.copy()

    return outStream

#Helper function for getting windows to remove noise using stalta antitrigger method
def __remove_anti_stalta(stream, sta, lta, thresh, show_plot=False):
    &#34;&#34;&#34;Helper function for getting windows to remove noise using stalta antitrigger method

    Parameters
    ----------
    stream : obspy.core.stream.Stream object
        Input stream on which to perform noise removal
    sta : int
        Number of seconds to use as short term window, reads from remove_noise() function.
    lta : int
        Number of seconds to use as long term window, reads from remove_noise() function.
    thresh : list
        Two-item list or tuple with the thresholds for the stalta antitrigger. Reads from remove_noise() function. The first value (index [0]) is the lower threshold, the second value (index [1] is the upper threshold), by default [0.5,5]
    show_plot : bool
        If True, will plot the trigger and stalta values. Reads from remove_noise() function, by default False.

    Returns
    -------
    outStream : obspy.core.stream.Stream object
        Stream with a masked array for the data where &#39;noise&#39; has been removed

    &#34;&#34;&#34;
    from obspy.signal.trigger import classic_sta_lta

    sampleRate = float(stream[0].stats.delta)

    sta_samples = sta / sampleRate #Convert to samples
    lta_samples = lta / sampleRate #Convert to samples
    staltaStream = stream.copy()

    for tr in staltaStream:
        with warnings.catch_warnings():
            warnings.filterwarnings(&#39;ignore&#39;, category=UserWarning)
            characteristic_fun = classic_sta_lta(tr, nsta=sta_samples, nlta=lta_samples)

    if show_plot:
        obspy.signal.trigger.plot_trigger(tr, characteristic_fun, thresh[1], thresh[0])
    windows_samples = obspy.signal.trigger.trigger_onset(characteristic_fun, thresh[1], thresh[0])
    
    startT = stream[0].stats.starttime
    endT = stream[0].stats.endtime
    window_UTC = []
    window_MPL = []
    window_UTC.append([startT, startT])
    for w, win in enumerate(windows_samples):
        for i, t in enumerate(win):
            if i == 0:
                window_UTC.append([])
                window_MPL.append([])
            trigShift = sta
            if trigShift &gt; t * sampleRate:
                trigShift = 0
            tSec = t * sampleRate - trigShift
            window_UTC[w+1].append(startT+tSec)
            window_MPL[w].append(window_UTC[w][i].matplotlib_date)
    
    window_UTC.append([endT, endT])
    #window_MPL[w].append(window_UTC[w][i].matplotlib_date)
    outStream = __remove_gaps(stream, window_UTC)
    return outStream

#Remove noise saturation
def __remove_noise_saturate(stream, sat_percent, min_win_size):
    &#34;&#34;&#34;Function to remove &#34;saturated&#34; data points that exceed a certain percent (sat_percent) of the maximum data value in the stream.  

    Parameters
    ----------
    stream : obspy.Stream
        Obspy Stream of interest
    sat_percent : float
        Percentage of the maximum amplitude, which will be used as the saturation threshold above which data points will be excluded
    min_win_size : float
        The minumum size a window must be (in seconds) for it to be removed

    Returns
    -------
    obspy.Stream
        Stream with masked array (if data removed) with &#34;saturated&#34; data removed
    &#34;&#34;&#34;
    if sat_percent &gt; 1:
        sat_percent = sat_percent / 100

    removeInd = np.array([], dtype=int)
    for trace in stream:
        dataArr = trace.data.copy()

        sample_rate = trace.stats.delta

        #Get max amplitude value
        maxAmp = np.max(np.absolute(dataArr, where = not None))
        thresholdAmp = maxAmp * sat_percent
        cond = np.nonzero(np.absolute(dataArr, where=not None) &gt; thresholdAmp)[0]
        removeInd = np.hstack([removeInd, cond])
        #trace.data = np.ma.where(np.absolute(data, where = not None) &gt; (noise_percent * maxAmp), None, data)
    #Combine indices from all three traces
    removeInd = np.unique(removeInd)
    
    removeList = []  # initialize
    min_win_samples = int(min_win_size / sample_rate)

    if len(removeInd) &gt; 0:
        startInd = removeInd[0]
        endInd = removeInd[0]

        for i in range(0, len(removeInd)):             
            if removeInd[i] - removeInd[i-1] &gt; 1:
                if endInd - startInd &gt;= min_win_samples:
                    removeList.append([int(startInd), int(endInd)])
                startInd = removeInd[i]
            endInd = removeInd[i]



    removeList.append([-1, -1]) #figure out a way to get rid of this

    #Convert removeList from samples to seconds after start to UTCDateTime
    sampleRate = stream[0].stats.delta
    startT = stream[0].stats.starttime
    endT = stream[0].stats.endtime
    removeSec = []
    removeUTC = []
    removeUTC.append([startT, startT])
    for i, win in enumerate(removeList):
        removeSec.append(list(np.round(sampleRate * np.array(win),6)))
        removeUTC.append(list(np.add(startT, removeSec[i])))
    removeUTC[-1][0] = removeUTC[-1][1] = endT
    
    outstream  = __remove_gaps(stream, removeUTC)
    return outstream

#Helper function for removing data using the noise threshold input from remove_noise()
def __remove_noise_thresh(stream, noise_percent=0.8, lta=30, min_win_size=1):
    &#34;&#34;&#34;Helper function for removing data using the noise threshold input from remove_noise()

    The purpose of the noise threshold method is to remove noisy windows (e.g., lots of traffic all at once). 
    
    This function uses the lta value (which can be specified here), and finds times where the lta value is at least at the noise_percent level of the max lta value for at least a specified time (min_win_size)

    Parameters
    ----------
    stream : obspy.core.stream.Stream object
        Input stream from which to remove windows. Passed from remove_noise().
    noise_percent : float, default=0.995
        Percentage (between 0 and 1), to use as the threshold at which to remove data. This is used in the noise threshold method. By default 0.995. 
        If a value is passed that is greater than 1, it will be divided by 100 to obtain the percentage. Passed from remove_noise().
    lta : int, default = 30
        Length of lta to use (in seconds)
    min_win_size : int, default = 1
        Minimum amount of time (in seconds) at which noise is above noise_percent level.
    
    Returns
    -------
    outStream : obspy.core.stream.Stream object
        Stream with a masked array for the data where &#39;noise&#39; has been removed. Passed to remove_noise().
    &#34;&#34;&#34;
    if noise_percent &gt; 1:
        noise_percent = noise_percent / 100

    removeInd = np.array([], dtype=int)
    for trace in stream:
        dataArr = trace.data.copy()

        sample_rate = trace.stats.delta
        lta_samples = int(lta / sample_rate)

        #Get lta values across traces data
        window_size = lta_samples
        if window_size == 0:
            window_size = 1
        kernel = np.ones(window_size) / window_size
        maskedArr = np.ma.array(dataArr, dtype=float, fill_value=None)
        ltaArr = np.convolve(maskedArr, kernel, mode=&#39;same&#39;)
        #Get max lta value
        maxLTA = np.max(ltaArr, where = not None)
        cond = np.nonzero(np.absolute(ltaArr, where=not None) &gt; (noise_percent * maxLTA))[0]
        removeInd = np.hstack([removeInd, cond])
        #trace.data = np.ma.where(np.absolute(data, where = not None) &gt; (noise_percent * maxAmp), None, data)
    #Combine indices from all three traces
    removeInd = np.unique(removeInd)

    #Make sure we&#39;re not removing single indices (we only want longer than min_win_size)
    removeList = []  # initialize    
    min_win_samples = int(min_win_size / sample_rate)

    if len(removeInd) &gt; 0:
        startInd = removeInd[0]
        endInd = removeInd[0]

        for i in range(0, len(removeInd)):
            #If indices are non-consecutive... 
            if removeInd[i] - removeInd[i-1] &gt; 1:
                #If the indices are non-consecutive and the 
                if endInd - startInd &gt;= min_win_samples:
                    removeList.append([int(startInd), int(endInd)])
                    
                #Set startInd as the current index
                startInd = removeInd[i]
            endInd = removeInd[i]
            
    removeList.append([-1, -1])

    sampleRate = stream[0].stats.delta
    startT = stream[0].stats.starttime
    endT = stream[0].stats.endtime
    removeSec = []
    removeUTC = []

    removeUTC.append([startT, startT])
    for i, win in enumerate(removeList):
        removeSec.append(list(np.round(sampleRate * np.array(win),6)))
        removeUTC.append(list(np.add(startT, removeSec[i])))
    removeUTC[-1][0] = removeUTC[-1][1] = endT

    outstream  = __remove_gaps(stream, removeUTC)

    return outstream

#Helper function for removing data during warmup (when seismometers are still initializing) and &#34;cooldown&#34; (when there may be noise from deactivating seismometer) time, if desired
def __remove_warmup_cooldown(stream, warmup_time = 0, cooldown_time = 0):
    sampleRate = float(stream[0].stats.delta)
    outStream = stream.copy()

    warmup_samples = int(warmup_time / sampleRate) #Convert to samples
    windows_samples=[]
    for tr in stream:
        totalSamples = len(tr.data)-1#float(tr.stats.endtime - tr.stats.starttime) / tr.stats.delta
        cooldown_samples = int(totalSamples - (cooldown_time / sampleRate)) #Convert to samples
    windows_samples = [[0, warmup_samples],[cooldown_samples, totalSamples]]
    if cooldown_time==0:
        windows_samples.pop(1)
    if warmup_time==0:
        windows_samples.pop(0)

    if windows_samples == []:
        pass
    else:
        startT = stream[0].stats.starttime
        endT = stream[-1].stats.endtime
        window_UTC = []
        window_MPL = []
        window_UTC.append([startT, startT])

        for w, win in enumerate(windows_samples):
            for j, tm in enumerate(win):

                if j == 0:
                    window_UTC.append([])
                    window_MPL.append([])
                tSec = tm * sampleRate
                window_UTC[w+1].append(startT+tSec)
                window_MPL[w].append(window_UTC[w][j].matplotlib_date)
        window_UTC.append([endT, endT])

        #window_MPL[w].append(window_UTC[w][i].matplotlib_date)
        outStream = __remove_gaps(stream, window_UTC)
    return outStream

#Plot noise windows
def _plot_noise_windows(hvsr_data, fig=None, ax=None, clear_fig=False, fill_gaps=None,
                         do_stalta=False, sta=5, lta=30, stalta_thresh=[0.5,5], 
                         do_pctThresh=False, sat_percent=0.8, min_win_size=1, 
                         do_noiseWin=False, noise_percent=0.995, 
                         do_warmup=False, warmup_time=0, cooldown_time=0, 
                         return_dict=False, use_tkinter=False):
    
    if clear_fig: #Intended use for tkinter
        #Clear everything
        for key in ax:
            ax[key].clear()
        fig.clear()

        #Really make sure it&#39;s out of memory
        fig = []
        ax = []
        try:
            fig.get_children()
        except:
            pass
        try:
            ax.get_children()
        except:
            pass

    if use_tkinter:
        try:
            pass #Don&#39;t think this is being used anymore, defined in sprit_gui separately
            #ax=ax_noise #self.ax_noise #?
            #fig=fig_noise
        except:
            pass

    #Reset axes, figure, and canvas widget
    noise_mosaic = [[&#39;spec&#39;],[&#39;spec&#39;],[&#39;spec&#39;],
            [&#39;spec&#39;],[&#39;spec&#39;],[&#39;spec&#39;],
            [&#39;signalz&#39;],[&#39;signalz&#39;], [&#39;signaln&#39;], [&#39;signale&#39;]]
    fig, ax = plt.subplot_mosaic(noise_mosaic, sharex=True)  
    #self.noise_canvas = FigureCanvasTkAgg(fig, master=canvasFrame_noise)
    #self.noise_canvasWidget.destroy()
    #self.noise_canvasWidget = self.noise_canvas.get_tk_widget()#.pack(side=tk.TOP, fill=tk.BOTH, expand=1)
    #self.noise_canvasWidget.pack(fill=&#39;both&#39;)#.grid(row=0, column=0, sticky=&#39;nsew&#39;)
    fig.canvas.draw()
    
    fig, ax = _plot_specgram_stream(stream=hvsr_data[&#39;stream&#39;], params=hvsr_data, fig=fig, ax=ax, component=&#39;Z&#39;, stack_type=&#39;linear&#39;, detrend=&#39;mean&#39;, fill_gaps=fill_gaps, dbscale=True, return_fig=True, cmap_per=[0.1,0.9])
    fig.canvas.draw()

    #Set initial input
    input = hvsr_data[&#39;stream&#39;]

    if do_stalta:
        hvsr_data[&#39;stream&#39;] = remove_noise(hvsr_data=input, remove_method=&#39;stalta&#39;, sta=sta, lta=lta, stalta_thresh=stalta_thresh)
        input = hvsr_data[&#39;stream&#39;]

    if do_pctThresh:
        hvsr_data[&#39;stream&#39;] = remove_noise(hvsr_data=input, remove_method=&#39;saturation&#39;,  sat_percent=sat_percent, min_win_size=min_win_size)
        input = hvsr_data[&#39;stream&#39;]

    if do_noiseWin:
        hvsr_data[&#39;stream&#39;] = remove_noise(hvsr_data=input, remove_method=&#39;noise&#39;, noise_percent=noise_percent, lta=lta, min_win_size=min_win_size)
        input = hvsr_data[&#39;stream&#39;]

    if do_warmup:
        hvsr_data[&#39;stream&#39;] = remove_noise(hvsr_data=input, remove_method=&#39;warmup&#39;, warmup_time=warmup_time, cooldown_time=cooldown_time)

    fig, ax, noise_windows_line_artists, noise_windows_window_artists = _get_removed_windows(input=hvsr_data, fig=fig, ax=ax, time_type=&#39;matplotlib&#39;)
    
    fig.canvas.draw()
    plt.show()
    if return_dict:
        hvsr_data[&#39;Windows_Plot&#39;] = (fig, ax)
        return hvsr_data
    return 

#Helper function for manual window selection 
def __draw_boxes(event, clickNo, xWindows, pathList, windowDrawn, winArtist, lineArtist, x0, fig, ax):
    &#34;&#34;&#34;Helper function for manual window selection to draw boxes to show where windows have been selected for removal&#34;&#34;&#34;
    #Create an axis dictionary if it does not already exist so all functions are the same

    if isinstance(ax, np.ndarray) or isinstance(ax, dict):
        ax = ax
    else:
        ax = {&#39;a&#39;:ax}

    
    if len(ax) &gt; 1:
        if type(ax) is not dict:
            axDict = {}
            for i, a in enumerate(ax):
                axDict[str(i)] = a
            ax = axDict
    #else:
    #    ax = {&#39;a&#39;:ax}
    
    #if event.inaxes!=ax: return
    #y0, y1 = ax.get_ylim()
    y0 = []
    y1 = []
    kList = []
    for k in ax.keys():
        kList.append(k)
        y0.append(ax[k].get_ylim()[0])
        y1.append(ax[k].get_ylim()[1])
    #else:
    #    y0 = [ax.get_ylim()[0]]
    #    y1 = [ax.get_ylim()[1]]

    if clickNo == 0:
        #y = np.linspace(ax.get_ylim()[0], ax.get_ylim()[1], 2)
        x0 = event.xdata
        clickNo = 1   
        lineArtist.append([])
        winNums = len(xWindows)
        for i, k in enumerate(ax.keys()):
            linArt = ax[k].axvline(x0, 0, 1, color=&#39;k&#39;, linewidth=1, zorder=100)
            lineArtist[winNums].append([linArt, linArt])
        #else:
        #    linArt = plt.axvline(x0, y0[i], y1[i], color=&#39;k&#39;, linewidth=1, zorder=100)
        #    lineArtist.append([linArt, linArt])
    else:
        x1 = event.xdata
        clickNo = 0

        windowDrawn.append([])
        winArtist.append([])  
        pathList.append([])
        winNums = len(xWindows)
        for i, key in enumerate(kList):
            path_data = [
                (matplotlib.path.Path.MOVETO, (x0, y0[i])),
                (matplotlib.path.Path.LINETO, (x1, y0[i])),
                (matplotlib.path.Path.LINETO, (x1, y1[i])),
                (matplotlib.path.Path.LINETO, (x0, y1[i])),
                (matplotlib.path.Path.LINETO, (x0, y0[i])),
                (matplotlib.path.Path.CLOSEPOLY, (x0, y0[i])),
            ]
            codes, verts = zip(*path_data)
            path = matplotlib.path.Path(verts, codes)

            windowDrawn[winNums].append(False)
            winArtist[winNums].append(None)

            pathList[winNums].append(path)
            __draw_windows(event=event, pathlist=pathList, ax_key=key, windowDrawn=windowDrawn, winArtist=winArtist, xWindows=xWindows, fig=fig, ax=ax)
            linArt = plt.axvline(x1, 0, 1, color=&#39;k&#39;, linewidth=0.5, zorder=100)

            [lineArtist[winNums][i].pop(-1)]
            lineArtist[winNums][i].append(linArt)
        x_win = [x0, x1]
        x_win.sort() #Make sure they are in the right order
        xWindows.append(x_win)
    fig.canvas.draw()
    return clickNo, x0

#Helper function for manual window selection to draw boxes to deslect windows for removal
def __remove_on_right(event, xWindows, pathList, windowDrawn, winArtist,  lineArtist, fig, ax):
    &#34;&#34;&#34;Helper function for manual window selection to draw boxes to deslect windows for removal&#34;&#34;&#34;

    if xWindows is not None:
        for i, xWins in enumerate(xWindows):
            if event.xdata &gt; xWins[0] and event.xdata &lt; xWins[1]:
                linArtists = lineArtist[i]
                pathList.pop(i)
                for j, a in enumerate(linArtists):
                    winArtist[i][j].remove()#.pop(i)
                    lineArtist[i][j][0].remove()#.pop(i)#[i].pop(j)
                    lineArtist[i][j][1].remove()
                windowDrawn.pop(i)
                lineArtist.pop(i)#[i].pop(j)
                winArtist.pop(i)#[i].pop(j)
                xWindows.pop(i)
    fig.canvas.draw() 

#Helper function for updating the canvas and drawing/deleted the boxes
def __draw_windows(event, pathlist, ax_key, windowDrawn, winArtist, xWindows, fig, ax):
    &#34;&#34;&#34;Helper function for updating the canvas and drawing/deleted the boxes&#34;&#34;&#34;
    for i, pa in enumerate(pathlist):
        for j, p in enumerate(pa): 
            if windowDrawn[i][j]:
                pass
            else:
                patch = matplotlib.patches.PathPatch(p, facecolor=&#39;k&#39;, alpha=0.75)                            
                winArt = ax[ax_key].add_patch(patch)
                windowDrawn[i][j] = True
                winArtist[i][j] = winArt

    if event.button is MouseButton.RIGHT:
        fig.canvas.draw()

#Helper function for getting click event information
def __on_click(event):
    &#34;&#34;&#34;Helper function for getting click event information&#34;&#34;&#34;
    global clickNo
    global x0
    if event.button is MouseButton.RIGHT:
        __remove_on_right(event, xWindows, pathList, windowDrawn, winArtist, lineArtist, fig, ax)

    if event.button is MouseButton.LEFT:            
        clickNo, x0 = __draw_boxes(event, clickNo, xWindows, pathList, windowDrawn, winArtist, lineArtist, x0, fig, ax)    

#Function to select windows using original stream specgram/plots
def _select_windows(input):
    &#34;&#34;&#34;Function to manually select windows for exclusion from data.

    Parameters
    ----------
    input : dict
        Dictionary containing all the hvsr information.

    Returns
    -------
    xWindows : list
        List of two-item lists containing start and end times of windows to be removed.
    &#34;&#34;&#34;
    from matplotlib.backend_bases import MouseButton
    import matplotlib.pyplot as plt
    import matplotlib
    import time
    global fig
    global ax

    if isinstance(input, (HVSRData, dict)):
        if &#39;hvsr_curve&#39; in input.keys():
            fig, ax = plot_hvsr(hvsr_data=input, plot_type=&#39;spec&#39;, returnfig=True, cmap=&#39;turbo&#39;)
        else:
            params = input.copy()
            input = input[&#39;stream&#39;]
    
    if isinstance(input, obspy.core.stream.Stream):
        fig, ax = _plot_specgram_stream(input, component=[&#39;Z&#39;])
    elif isinstance(input, obspy.core.trace.Trace):
        fig, ax = _plot_specgram_stream(input)

    global lineArtist
    global winArtist
    global windowDrawn
    global pathList
    global xWindows
    global clickNo
    global x0
    x0=0
    clickNo = 0
    xWindows = []
    pathList = []
    windowDrawn = []
    winArtist = []
    lineArtist = []

    global fig_closed
    fig_closed = False
    while fig_closed is False:
        fig.canvas.mpl_connect(&#39;button_press_event&#39;, __on_click)#(clickNo, xWindows, pathList, windowDrawn, winArtist, lineArtist, x0, fig, ax))
        fig.canvas.mpl_connect(&#39;close_event&#39;, _on_fig_close)#(clickNo, xWindows, pathList, windowDrawn, winArtist, lineArtist, x0, fig, ax))
        plt.pause(1)

    params[&#39;xwindows_out&#39;] = xWindows
    params[&#39;fig&#39;] = fig
    params[&#39;ax&#39;] = ax
    return params

#Support function to help select_windows run properly
def _on_fig_close(event):
    global fig_closed
    fig_closed = True
    return

#Shows windows with None on input plot
def _get_removed_windows(input, fig=None, ax=None, lineArtist =[], winArtist = [], existing_lineArtists=[], existing_xWindows=[], exist_win_format=&#39;matplotlib&#39;, keep_line_artists=True, time_type=&#39;matplotlib&#39;,show_plot=False):
    &#34;&#34;&#34;This function is for getting Nones from masked arrays and plotting them as windows&#34;&#34;&#34;
    if fig is None and ax is None:
        fig, ax = plt.subplots()

    if isinstance(input, (dict, HVSRData)):
        stream = input[&#39;stream&#39;].copy()
    elif isinstance(input, (obspy.core.Trace.trace, obspy.core.stream.Stream)):
        stream = input.copy()
    else:
        pass #Warning?
        
    samplesList = [&#39;sample&#39;, &#39;samples&#39;, &#39;s&#39;]
    utcList = [&#39;utc&#39;, &#39;utcdatetime&#39;, &#39;obspy&#39;, &#39;u&#39;, &#39;o&#39;]
    matplotlibList = [&#39;matplotlib&#39;, &#39;mpl&#39;, &#39;m&#39;]    
    
    #Get masked indices of trace(s)
    trace = stream.merge()[0]
    sample_rate = trace.stats.delta
    windows = []
    #windows.append([0,np.nan])
    #mask = np.isnan(trace.data)  # Create a mask for None values
    #masked_array = np.ma.array(trace.data, mask=mask).copy()
    masked_array = trace.data.copy()
    if isinstance(masked_array, np.ma.MaskedArray):
        masked_array = masked_array.mask.nonzero()[0]
        lastMaskInd = masked_array[0]-1
        wInd = 0
        for i in range(0, len(masked_array)-1):
            maskInd = masked_array[i]
            if maskInd-lastMaskInd &gt; 1 or i==0:
                windows.append([np.nan, np.nan])
                if i==0:
                    windows[wInd][0] = masked_array[i]
                else:
                    windows[wInd-1][1] = masked_array[i - 1]
                windows[wInd][0] = masked_array[i]
                wInd += 1
            lastMaskInd = maskInd
        windows[wInd-1][1] = masked_array[-1] #Fill in last masked value (wInd-1 b/c wInd+=1 earlier)
    winTypeList = [&#39;gaps&#39;] * len(windows)

    #Check if the windows are just gaps
    if len(existing_xWindows) &gt; 0:
        existWin = []
        #Check if windows are already being taken care of with the gaps
        startList = []
        endList = []
        for start, end in windows:
            startList.append((trace.stats.starttime + start*sample_rate).matplotlib_date)
            endList.append((trace.stats.starttime + end*sample_rate).matplotlib_date)
        for w in existing_xWindows:
            removed=False
            if w[0] in startList and w[1] in endList:
                existing_xWindows.remove(w)

                removed=True                    
            if exist_win_format.lower() in matplotlibList and not removed:
                sTimeMPL = trace.stats.starttime.matplotlib_date #Convert time to samples from starttime
                existWin.append(list(np.round((w - sTimeMPL)*3600*24/sample_rate)))
                                    
        windows = windows + existWin
        existWinTypeList = [&#39;removed&#39;] * len(existWin)
        winTypeList = winTypeList + existWinTypeList

    #Reformat ax as needed
    if isinstance(ax, np.ndarray):
        origAxes = ax.copy()
        newAx = {}
        for i, a in enumerate(ax):
            newAx[i] = a
        axes = newAx
    elif isinstance(ax, dict):
        origAxes = ax
        axes = ax
    else:
        origAxes = ax
        axes = {&#39;ax&#39;:ax}

    for i, a in enumerate(axes.keys()):
        ax = axes[a]
        pathList = []
        
        windowDrawn = []
        winArtist = []
        if existing_lineArtists == []:
            lineArtist = []
        elif len(existing_lineArtists)&gt;=1 and keep_line_artists:
            lineArtist = existing_lineArtists
        else:
            lineArtist = []

        for winNums, win in enumerate(windows):
            if time_type.lower() in samplesList:
                x0 = win[0]
                x1 = win[1]
            elif time_type.lower() in utcList or time_type.lower() in matplotlibList:
                #sample_rate = trace.stats.delta

                x0 = trace.stats.starttime + (win[0] * sample_rate)
                x1 = trace.stats.starttime + (win[1] * sample_rate)

                if time_type.lower() in matplotlibList:
                    x0 = x0.matplotlib_date
                    x1 = x1.matplotlib_date
            else:
                warnings.warn(f&#39;time_type={time_type} not recognized. Defaulting to matplotlib time formatting&#39;)
                x0 = trace.stats.starttime + (win[0] * sample_rate)
                x1 = trace.stats.starttime + (win[1] * sample_rate)
                
                x0 = x0.matplotlib_date
                x1 = x1.matplotlib_date
            
            y0, y1 = ax.get_ylim()

            path_data = [
                        (matplotlib.path.Path.MOVETO, (x0, y0)),
                        (matplotlib.path.Path.LINETO, (x1, y0)),
                        (matplotlib.path.Path.LINETO, (x1, y1)),
                        (matplotlib.path.Path.LINETO, (x0, y1)),
                        (matplotlib.path.Path.LINETO, (x0, y0)),
                        (matplotlib.path.Path.CLOSEPOLY, (x0, y0)),
                    ]
            
            codes, verts = zip(*path_data)
            path = matplotlib.path.Path(verts, codes)

            #
            windowDrawn.append(False)
            winArtist.append(None)
            lineArtist.append([])
            
            if winTypeList[winNums] == &#39;gaps&#39;:
                clr = &#39;#b13d41&#39;
            elif winTypeList[winNums] == &#39;removed&#39;:
                clr = &#39;k&#39;
            else:
                clr = &#39;yellow&#39;

            linArt0 = ax.axvline(x0, y0, y1, color=clr, linewidth=0.5, zorder=100)
            linArt1 = plt.axvline(x1, y0, y1, color=clr, linewidth=0.5, zorder=100)
            lineArtist[winNums].append([linArt0, linArt1])
            #
            
            pathList.append(path)

        for i, pa in enumerate(pathList):
            if windowDrawn[i]:
                pass
            else:
                patch = matplotlib.patches.PathPatch(pa, facecolor=clr, alpha=0.75)                            
                winArt = ax.add_patch(patch)
                windowDrawn[i] = True
                winArtist[i] = winArt
        
        #Reformat ax as needed
        if isinstance(origAxes, np.ndarray):
            origAxes[i] = ax
        elif isinstance(origAxes, dict):
            origAxes[a] = ax
        else:
            origAxes = ax

        ax = origAxes

        fig.canvas.draw()
    
    if show_plot:
        plt.show()
    return fig, ax, lineArtist, winArtist

#Helper function for removing windows from data, leaving gaps
def __remove_windows(stream, window_list, warmup_time):
    &#34;&#34;&#34;Helper function that actually does the work in obspy to remove the windows calculated in the remove_noise function
s
    Parameters
    ----------
    stream : obspy.core.stream.Stream object
        Input stream from which to remove windows
    window_list : list
        A list of windows with start and end times for the windows to be removed
    warmup_time : int, default = 0
        Passed from remove_noise, the amount of time in seconds to allow for warmup. Anything before this is removed as &#39;noise&#39;.

    Returns
    -------
    outStream : obspy.core.stream.Stream object
        Stream with a masked array for the data where &#39;noise&#39; has been removed
    &#34;&#34;&#34;
    og_stream = stream.copy()

    #Find the latest start time and earliest endtime of all traces (in case they aren&#39;t consistent)
    maxStartTime = obspy.UTCDateTime(-1e10) #Go back pretty far (almost 400 years) to start with
    minEndTime = obspy.UTCDateTime(1e10)
    for comp in [&#39;E&#39;, &#39;N&#39;, &#39;Z&#39;]:
        tr = stream.select(component=comp).copy()
        if tr[0].stats.starttime &gt; maxStartTime:
            maxStartTime = tr[0].stats.starttime
        if tr[0].stats.endtime &lt; minEndTime:
            minEndTime = tr[0].stats.endtime

    #Trim all traces to the same start/end time
    stream.trim(starttime=maxStartTime, endtime=minEndTime)      

    #Sort windows by the start of the window
    sorted_window_list = []
    windowStart = []
    for i, window in enumerate(window_list):
        windowStart.append(window[0])
    windowStart_og = windowStart.copy()
    windowStart.sort()
    sorted_start_list = windowStart
    ranks = [windowStart_og.index(item) for item in sorted_start_list]
    for r in ranks:
        sorted_window_list.append(window_list[r])

    for i, w in enumerate(sorted_window_list):
        if i &lt; len(sorted_window_list) - 1:
            if w[1] &gt; sorted_window_list[i+1][0]:
                warnings.warn(f&#34;Warning: Overlapping windows. Please start over and reselect windows to be removed or use a different noise removal method: {w[1]} &#39;&gt;&#39; {sorted_window_list[i+1][0]}&#34;)
                return
                
    window_gaps_obspy = []
    window_gaps = []

    buffer_time = np.ceil((stream[0].stats.endtime-stream[0].stats.starttime)*0.01)

    #Get obspy.UTCDateTime objects for the gap times
    window_gaps_obspy.append([stream[0].stats.starttime + warmup_time, stream[0].stats.starttime + warmup_time])
    for i, window in enumerate(sorted_window_list):
        for j, item in enumerate(window):
            if j == 0:
                window_gaps_obspy.append([0,0])
            window_gaps_obspy[i+1][j] = obspy.UTCDateTime(matplotlib.dates.num2date(item))
        window_gaps.append((window[1]-window[0])*86400)
    window_gaps_obspy.append([stream[0].stats.endtime-buffer_time, stream[0].stats.endtime-buffer_time])
    #Note, we added start and endtimes to obpsy list to help with later functionality

    #Clean up stream windows (especially, start and end)
    for i, window in enumerate(window_gaps):
        newSt = stream.copy()
        #Check if first window starts before end of warmup time
        #If the start of the first exclusion window is before the warmup_time is over
        if window_gaps_obspy[i+1][0] - newSt[0].stats.starttime &lt; warmup_time:
            #If the end of first exclusion window is also before the warmup_time is over
            if window_gaps_obspy[i+1][1] - newSt[0].stats.starttime &lt; warmup_time:
                #Remove that window completely, it is unnecessary
                window_gaps.pop(i)
                window_gaps_obspy.pop(i+1)
                #...and reset the entire window to start at the warmup_time end
                window_gaps_obspy[0][0] = window_gaps_obspy[0][1] = newSt[0].stats.starttime + warmup_time
                continue
            else: #if window overlaps the start of the stream after warmup_time
                #Remove that window
                window_gaps.pop(i)
                #...and reset the start of the window to be the end of warm up time
                #...and  remove that first window from the obspy list
                window_gaps_obspy[0][0] = window_gaps_obspy[0][1] =  window_gaps_obspy[i+1][1]#newSt[0].stats.starttime + warmup_time
                window_gaps_obspy.pop(i+1)


        if stream[0].stats.endtime - window_gaps_obspy[i+1][1] &gt; stream[0].stats.endtime - buffer_time:        
            if stream[0].stats.endtime - window_gaps_obspy[i+1][0] &gt; stream[0].stats.endtime - buffer_time:
                window_gaps.pop(i)
                window_gaps_obspy.pop(i+1)
            else:  #if end of window overlaps the buffer time, just end it at the start of the window (always end with stream, not gap)
                window_gaps.pop(i)
                window_gaps_obspy[-1][0] = window_gaps_obspy[-1][1] = newSt[0].stats.endtime - buffer_time
   
    #Add streams
    stream_windows = []
    j = 0
    for i, window in enumerate(window_gaps):
        j=i
        newSt = stream.copy()
        stream_windows.append(newSt.trim(starttime=window_gaps_obspy[i][1], endtime=window_gaps_obspy[i+1][0]))
    i = j + 1
    newSt = stream.copy()
    stream_windows.append(newSt.trim(starttime=window_gaps_obspy[i][1], endtime=window_gaps_obspy[i+1][0]))

    for i, st in enumerate(stream_windows):
        if i == 0:
            outStream = st.copy()
        else:
            newSt = st.copy()
            gap = window_gaps[i-1]
            outStream = outStream + newSt.trim(starttime=st[0].stats.starttime - gap, pad=True, fill_value=None)       
    outStream.merge()
    return outStream

##Helper functions for process_hvsr()
#Get diffuse field assumption data
def _dfa(params, verbose=False):#, equal_interval_energy, median_daily_psd, verbose=False):
    &#34;&#34;&#34;Function for performing Diffuse Field Assumption (DFA) analysis
    
        This feature is not yet implemented.
    &#34;&#34;&#34;
    # Use equal energy for daily PSDs to give small &#39;events&#39; a chance to contribute
    # the same as large ones, so that P1+P2+P3=1

    x_values=params[&#39;ppsds&#39;][&#39;Z&#39;][&#39;period_bin_centers&#39;]

    method=params[&#39;method&#39;]

    methodList = [&#39;&lt;placeholder&gt;&#39;, &#39;Diffuse Field Assumption&#39;, &#39;Arithmetic Mean&#39;, &#39;Geometric Mean&#39;, &#39;Vector Summation&#39;, &#39;Quadratic Mean&#39;, &#39;Maximum Horizontal Value&#39;]
    dfaList = [&#39;dfa&#39;, &#39;diffuse field&#39;, &#39;diffuse field assumption&#39;]
    if type(method) is int:
        method = methodList[method]

    if method.lower() in dfaList:
        if verbose:
            print(&#39;[Using Diffuuse Field Assumption (DFA)&#39;, flush=True)
        params[&#39;dfa&#39;] = {}
        params[&#39;dfa&#39;][&#39;sum_ns_power&#39;] = list()
        params[&#39;dfa&#39;][&#39;sum_ew_power&#39;] = list()
        params[&#39;dfa&#39;][&#39;sum_z_power&#39;] = list()
        params[&#39;dfa&#39;][&#39;time_int_psd&#39;] = {&#39;Z&#39;:{}, &#39;E&#39;:{}, &#39;N&#39;:{}}
        params[&#39;dfa&#39;][&#39;time_values&#39;] = list()
        params[&#39;dfa&#39;][&#39;equal_interval_energy&#39;] = {&#39;Z&#39;:{}, &#39;E&#39;:{}, &#39;N&#39;:{}}

        # Make sure we have all 3 components for every time sample
        for i, t_int in enumerate(params[&#39;ppsds&#39;][&#39;Z&#39;][&#39;current_times_used&#39;]):#day_time_values):
            #if day_time not in (day_time_psd[0].keys()) or day_time not in (day_time_psd[1].keys()) or day_time not in (day_time_psd[2].keys()):
            #    continue
            
            #Currently the same as day_time, and probably notneeded to redefine
            time_int = str(t_int)#day_time.split(&#39;T&#39;)[0]
            if time_int not in params[&#39;dfa&#39;][&#39;time_values&#39;]:
                params[&#39;dfa&#39;][&#39;time_values&#39;].append(time_int)

            # Initialize the PSDs.
            if time_int not in params[&#39;dfa&#39;][&#39;time_int_psd&#39;][&#39;Z&#39;].keys():
                params[&#39;dfa&#39;][&#39;time_int_psd&#39;][&#39;Z&#39;][time_int] = list()
                params[&#39;dfa&#39;][&#39;time_int_psd&#39;][&#39;E&#39;][time_int] = list()
                params[&#39;dfa&#39;][&#39;time_int_psd&#39;][&#39;N&#39;][time_int] = list()

            params[&#39;dfa&#39;][&#39;time_int_psd&#39;][&#39;Z&#39;][time_int].append(params[&#39;ppsds&#39;][&#39;Z&#39;][&#39;psd_values&#39;][i])
            params[&#39;dfa&#39;][&#39;time_int_psd&#39;][&#39;E&#39;][time_int].append(params[&#39;ppsds&#39;][&#39;E&#39;][&#39;psd_values&#39;][i])
            params[&#39;dfa&#39;][&#39;time_int_psd&#39;][&#39;N&#39;][time_int].append(params[&#39;ppsds&#39;][&#39;N&#39;][&#39;psd_values&#39;][i])

        # For each time_int equalize energy
        for time_int in params[&#39;dfa&#39;][&#39;time_values&#39;]:

            # Each PSD for the time_int
            for k in range(len(params[&#39;dfa&#39;][&#39;time_int_psd&#39;][&#39;Z&#39;][time_int])):
                Pz = list()
                P1 = list()
                P2 = list()
                sum_pz = 0
                sum_p1 = 0
                sum_p2 = 0

                # Each sample of the PSD , convert to power
                for j in range(len(x_values) - 1):
                    pz = __get_power([params[&#39;dfa&#39;][&#39;time_int_psd&#39;][&#39;Z&#39;][time_int][k][j], params[&#39;dfa&#39;][&#39;time_int_psd&#39;][&#39;Z&#39;][time_int][k][j + 1]], [x_values[j], x_values[j + 1]])
                    Pz.append(pz)
                    sum_pz += pz
                    p1 = __get_power([params[&#39;dfa&#39;][&#39;time_int_psd&#39;][&#39;E&#39;][time_int][k][j], params[&#39;dfa&#39;][&#39;time_int_psd&#39;][&#39;E&#39;][time_int][k][j + 1]], [x_values[j], x_values[j + 1]])
                    P1.append(p1)
                    sum_p1 += p1
                    p2 = __get_power([params[&#39;dfa&#39;][&#39;time_int_psd&#39;][&#39;N&#39;][time_int][k][j], params[&#39;dfa&#39;][&#39;time_int_psd&#39;][&#39;N&#39;][time_int][k][j + 1]], [x_values[j], x_values[j + 1]])
                    P2.append(p2)
                    sum_p2 += p2

                sum_power = sum_pz + sum_p1 + sum_p2  # total power

                # Mormalized power
                for j in range(len(x_values) - 1):
                    # Initialize if this is the first sample of the time_int
                    if k == 0:
                        params[&#39;dfa&#39;][&#39;sum_z_power&#39;].append(Pz[j] / sum_power)
                        params[&#39;dfa&#39;][&#39;sum_ns_power&#39;].append(P1[j] / sum_power)
                        params[&#39;dfa&#39;][&#39;sum_ew_power&#39;].append(P2[j] / sum_power)
                    else:
                        params[&#39;dfa&#39;][&#39;sum_z_power&#39;][j] += (Pz[j] / sum_power)
                        params[&#39;dfa&#39;][&#39;sum_ns_power&#39;][j] += (P1[j] / sum_power)
                        params[&#39;dfa&#39;][&#39;sum_ew_power&#39;][j] += (P2[j] / sum_power)
            # Average the normalized daily power
            for j in range(len(x_values) - 1):
                params[&#39;dfa&#39;][&#39;sum_z_power&#39;][j] /= len(params[&#39;dfa&#39;][&#39;time_int_psd&#39;][&#39;Z&#39;][time_int])
                params[&#39;dfa&#39;][&#39;sum_ns_power&#39;][j] /= len(params[&#39;dfa&#39;][&#39;time_int_psd&#39;][&#39;Z&#39;][time_int])
                params[&#39;dfa&#39;][&#39;sum_ew_power&#39;][j] /= len(params[&#39;dfa&#39;][&#39;time_int_psd&#39;][&#39;Z&#39;][time_int])

            params[&#39;dfa&#39;][&#39;equal_interval_energy&#39;][&#39;Z&#39;][time_int] = params[&#39;dfa&#39;][&#39;sum_z_power&#39;]
            params[&#39;dfa&#39;][&#39;equal_interval_energy&#39;][&#39;E&#39;][time_int] = params[&#39;dfa&#39;][&#39;sum_ns_power&#39;]
            params[&#39;dfa&#39;][&#39;equal_interval_energy&#39;][&#39;N&#39;][time_int] = params[&#39;dfa&#39;][&#39;sum_ew_power&#39;]

    return params

#Helper function for smoothing across frequencies
def __freq_smooth_window(hvsr_out, f_smooth_width, kind_freq_smooth):
    &#34;&#34;&#34;Helper function to smooth frequency if &#39;constant&#39; or &#39;proportional&#39; is passed to freq_smooth parameter of process_hvsr() function&#34;&#34;&#34;
    if kind_freq_smooth == &#39;constant&#39;:
        fwidthHalf = f_smooth_width//2
    elif kind_freq_smooth == &#39;proportional&#39;:
        anyKey = list(hvsr_out[&#39;psd_raw&#39;].keys())[0]
        freqLength = hvsr_out[&#39;psd_raw&#39;][anyKey].shape[1]
        if f_smooth_width &gt; 1:
            fwidthHalf = int(f_smooth_width/100 * freqLength)
        else:
            fwidthHalf = int(f_smooth_width * freqLength)
    else:
        warnings.warn(&#39;Oops, typo somewhere&#39;)


    for k in hvsr_out[&#39;psd_raw&#39;]:
        colName = f&#39;psd_values_{k}&#39;

        newTPSD = list(np.stack(hvsr_out[&#39;hvsr_df&#39;][colName]))
        #newTPSD = list(np.ones_like(hvsr_out[&#39;psd_raw&#39;][k]))

        for t, tPSD in enumerate(hvsr_out[&#39;psd_raw&#39;][k]):
            for i, fVal in enumerate(tPSD):
                if i &lt; fwidthHalf:
                    downWin = i
                    ind = -1*(fwidthHalf-downWin)
                    windMultiplier_down = np.linspace(1/fwidthHalf, 1-1/fwidthHalf, fwidthHalf)
                    windMultiplier_down = windMultiplier_down[:ind]
                else:
                    downWin = fwidthHalf
                    windMultiplier_down =  np.linspace(1/fwidthHalf, 1-1/fwidthHalf, fwidthHalf)
                if i + fwidthHalf &gt;= len(tPSD):
                    upWin = (len(tPSD) - i)
                    ind = -1 * (fwidthHalf-upWin+1)
                    windMultiplier_up = np.linspace(1-1/fwidthHalf, 0, fwidthHalf)
                    windMultiplier_up = windMultiplier_up[:ind]

                else:
                    upWin = fwidthHalf+1
                    windMultiplier_up = np.linspace(1 - 1/fwidthHalf, 0, fwidthHalf)
            
                windMultiplier = list(np.hstack([windMultiplier_down, windMultiplier_up]))
                midInd = np.argmax(windMultiplier)
                if i &gt; 0:
                    midInd+=1
                windMultiplier.insert(midInd, 1)
                smoothVal = np.divide(np.sum(np.multiply(tPSD[i-downWin:i+upWin], windMultiplier)), np.sum(windMultiplier))
                newTPSD[t][i] = smoothVal

        hvsr_out[&#39;psd_raw&#39;][k] = newTPSD
        hvsr_out[&#39;hvsr_df&#39;][colName] = pd.Series(list(newTPSD), index=hvsr_out[&#39;hvsr_df&#39;].index)


    return hvsr_out

#Get an HVSR curve, given an array of x values (freqs), and a dict with psds for three components
def __get_hvsr_curve(x, psd, method, hvsr_data, verbose=False):
    &#34;&#34;&#34; Get an HVSR curve from three components over the same time period/frequency intervals

    Parameters
    ----------
        x   : list or array_like
            x value (frequency or period)
        psd : dict
            Dictionary with psd values for three components. Usually read in as part of hvsr_data from process_hvsr
        method : int or str
            Integer or string, read in from process_hvsr method parameter
    
    Returns
    -------
        tuple
         (hvsr_curve, hvsr_tSteps), both np.arrays. hvsr_curve is a numpy array containing H/V ratios at each frequency/period in x.
         hvsr_tSteps only used with diffuse field assumption method. 

    &#34;&#34;&#34;
    hvsr_curve = []
    hvsr_tSteps = []

    params = hvsr_data
    if method==1 or method ==&#39;dfa&#39; or method ==&#39;Diffuse Field Assumption&#39;:
        warnings.warn(&#39;WARNING: DFA method is currently experimental and not supported&#39;)
        for j in range(len(x)-1):
            for time_interval in params[&#39;ppsds&#39;][&#39;Z&#39;][&#39;current_times_used&#39;]:
                hvsr_curve_tinterval = []
                params = _dfa(params, verbose=verbose)
                eie = params[&#39;dfa&#39;][&#39;equal_interval_energy&#39;]
                if time_interval in list(eie[&#39;Z&#39;].keys()) and time_interval in list(eie[&#39;E&#39;].keys()) and time_interval in list(eie[&#39;N&#39;].keys()):
                    hvsr = math.sqrt(
                        (eie[&#39;Z&#39;][str(time_interval)][j] + eie[&#39;N&#39;][str(time_interval)][j]) / eie[&#39;Z&#39;][str(time_interval)][j])
                    hvsr_curve_tinterval.append(hvsr)
                else:
                    if verbose &gt; 0:
                        print(&#39;WARNING: &#39;+ time_interval + &#39; missing component, skipped!&#39;)
                    continue
            #Average overtime
            hvsr_curve.append(np.mean(hvsr_curve_tinterval))
            hvsr_tSteps.append(hvsr_curve_tinterval)
    else:
        for j in range(len(x)-1):
            psd0 = [psd[&#39;Z&#39;][j], psd[&#39;Z&#39;][j + 1]]
            psd1 = [psd[&#39;E&#39;][j], psd[&#39;E&#39;][j + 1]]
            psd2 = [psd[&#39;N&#39;][j], psd[&#39;N&#39;][j + 1]]
            f =    [x[j], x[j + 1]]

            hvsr = __get_hvsr(psd0, psd1, psd2, f, use_method=method)
            
            #hvsr_curve.append(hvsr)
            hvsr_curve.append(hvsr)
        hvsr_tSteps= None #Only used for DFA

    return np.array(hvsr_curve), hvsr_tSteps

#Get HVSR
def __get_hvsr(_dbz, _db1, _db2, _x, use_method=4):
    &#34;&#34;&#34;
    _dbz : list
        Two item list with deciBel value of z component at either end of particular frequency step
    _db1 : list
        Two item list with deciBel value of either e or n component (does not matter which) at either end of particular frequency step
    _db2 : list
        Two item list with deciBel value of either e or n component (does not matter which) at either end of particular frequency step
    _x : list
        Two item list containing frequency values at either end of frequency step of interest
    use_method : int, default = 4
        H is computed based on the selected use_method see: https://academic.oup.com/gji/article/194/2/936/597415
            use_method:
            (1) Diffuse Field Assumption (DFA)
            (2) arithmetic mean, that is, H â‰¡ (HN + HE)/2
            (3) geometric mean, that is, H â‰¡ âˆšHN Â· HE, recommended by the SESAME project (2004)
            (4) vector summation, that is, H â‰¡ âˆšH2 N + H2 E
            (5) quadratic mean, that is, H â‰¡ âˆš(H2 N + H2 E )/2
            (6) maximum horizontal value, that is, H â‰¡ max {HN, HE}
        &#34;&#34;&#34;

    _pz = __get_power(_dbz, _x)
    _p1 = __get_power(_db1, _x)
    _p2 = __get_power(_db2, _x)
    
    _hz = math.sqrt(_pz)
    _h1 = math.sqrt(_p1)
    _h2 = math.sqrt(_p2)

    _h = {  2: (_h1 + _h2) / 2.0, #Arithmetic mean
            3: math.sqrt(_h1 * _h2), #Geometric mean
            4: math.sqrt(_p1 + _p2), #Vector summation
            5: math.sqrt((_p1 + _p2) / 2.0), #Quadratic mean
            6: max(_h1, _h2)} #Max horizontal value
    _hvsr = _h[use_method] / _hz
    return _hvsr

#For converting dB scaled data to power units
def __get_power(_db, _x):
    &#34;&#34;&#34;Calculate HVSR

    #FROM ORIGINAL (I think this is only step 6)
        Undo deciBel calculations as outlined below:
            1. Dividing the window into 13 segments having 75% overlap
            2. For each segment:
                2.1 Removing the trend and mean
                2.2 Apply a 10% sine taper
                2.3 FFT
            3. Calculate the normalized PSD
            4. Average the 13 PSDs &amp; scale to compensate for tapering
            5. Frequency-smooth the averaged PSD over 1-octave intervals at 1/8-octave increments
            6. Convert power to decibels
    #END FROM ORIGINAL

    Parameters
    ----------
    _db : list
        Two-item list with individual power values in decibels for specified freq step.
    _x : list
        Two-item list with Individual x value (either frequency or period)
    
    Returns
    -------
    _p : float
        Individual power value, converted from decibels

    NOTE
    ----
        PSD is equal to the power divided by the width of the bin
          PSD = P / W
          log(PSD) = Log(P) - log(W)
          log(P) = log(PSD) + log(W)  here W is width in frequency
          log(P) = log(PSD) - log(Wt) here Wt is width in period

    for each bin perform rectangular integration to compute power
    power is assigned to the point at the begining of the interval
         _   _
        | |_| |
        |_|_|_|

     Here we are computing power for individual ponts, so, no integration is necessary, just
     compute area.
    &#34;&#34;&#34;
    _dx = abs(np.diff(_x)[0])
    _p = np.multiply(np.mean(__remove_db(_db)), _dx)
    return _p

#Remove decibel scaling
def __remove_db(_db_value):
    &#34;&#34;&#34;convert dB power to power&#34;&#34;&#34;
    _values = list()
    for _d in _db_value:
        _values.append(10 ** (float(_d) / 10.0))
    #FIX THIS
    if _values[1]==0:
       _values[1]=10e-300
    return _values

#Find peaks in the hvsr ccruve
def __find_peaks(_y):
    &#34;&#34;&#34;Finds all possible peaks on hvsr curves
    Parameters
    ----------
    _y : list or array
        _y input is list or array of a curve.
          In this case, this is either main hvsr curve or individual time step curves
    &#34;&#34;&#34;
    _index_list = scipy.signal.argrelextrema(np.array(_y), np.greater)

    return _index_list[0]

#Get additional HVSR params for later calcualtions
def __gethvsrparams(hvsr_out):
    &#34;&#34;&#34;Private function to get HVSR parameters for later calculations (things like standard deviation, etc)&#34;&#34;&#34;

    hvsrp2 = {}
    hvsrm2 = {}
    
    peak_water_level_p=[]
    hvsrp2=[]
    hvsrm=[]
    peak_water_level_m=[]
    
    hvsr_log_std = {}


    hvsr=hvsr_out[&#39;hvsr_curve&#39;]
    hvsrDF = hvsr_out[&#39;hvsr_df&#39;]
    if hvsr_out[&#39;ind_hvsr_curves&#39;].shape[0] &gt; 0:
        #With arrays, original way of doing it
        hvsr_log_std = np.nanstd(np.log10(hvsr_out[&#39;ind_hvsr_curves&#39;]), axis=0)

        #With dataframe, updated way to use DF for all time-step tasks, still testing
        stackedData = np.stack(hvsr_out[&#39;hvsr_df&#39;][&#39;HV_Curves&#39;])
        logStackedata = np.log10(stackedData).tolist()
        for i, r in enumerate(logStackedata):
            logStackedata[i] = np.array(r)

        hvsr_out[&#39;hvsr_df&#39;][&#39;HV_Curves_Log10&#39;] = logStackedata
        hvsr_log_std = np.nanstd(np.stack(hvsr_out[&#39;hvsr_df&#39;][&#39;HV_Curves_Log10&#39;][hvsrDF[&#39;Use&#39;]]), axis=0)

        #The componenets are already calculated, don&#39;t need to recalculate aren&#39;t calculated at the time-step level
        hvsrp = np.add(hvsr_out[&#39;hvsr_curve&#39;], hvsr_out[&#39;ind_hvsr_stdDev&#39;])
        hvsrm = np.subtract(hvsr_out[&#39;hvsr_curve&#39;], hvsr_out[&#39;ind_hvsr_stdDev&#39;])
        hvsrp2 = np.multiply(hvsr, np.exp(hvsr_log_std))
        hvsrm2 = np.divide(hvsr, np.exp(hvsr_log_std))


    newKeys = [&#39;hvsr_log_std&#39;, &#39;hvsrp&#39;,&#39;hvsrm&#39;, &#39;hvsrp2&#39;,&#39;hvsrm2&#39;]
    newVals = [hvsr_log_std,    hvsrp,  hvsrm,   hvsrp2,  hvsrm2]
    for i, nk in enumerate(newKeys):
        hvsr_out[nk] = np.array(newVals[i])

    return hvsr_out

##Helper Functions for plotting
#Plot hvsr curve, private supporting function for plot_hvsr
def _plot_hvsr(hvsr_data, plot_type, xtype=&#39;frequency&#39;, fig=None, ax=None, save_dir=None, save_suffix=&#39;&#39;, show=True, **kwargs):
    &#34;&#34;&#34;Private function for plotting hvsr curve (or curves with components)
    &#34;&#34;&#34;
    if &#39;kwargs&#39; in kwargs.keys():
        kwargs = kwargs[&#39;kwargs&#39;]

    if fig is None and ax is None:
        fig, ax = plt.subplots()

    if &#39;xlim&#39; not in kwargs.keys():
        xlim = hvsr_data[&#39;hvsr_band&#39;]
    else:
        xlim = kwargs[&#39;xlim&#39;]
    
    if &#39;ylim&#39; not in kwargs.keys():
        ylim = [0, max(hvsr_data[&#39;hvsrp2&#39;])]
        if ylim[1] &gt; 20:
            ylim = [0, max(hvsr_data[&#39;hvsr_curve&#39;]+1)]

    else:
        ylim = kwargs[&#39;ylim&#39;]
    
    if &#39;grid&#39; in kwargs.keys():
        plt.grid(which=kwargs[&#39;grid&#39;], alpha=0.25)

    freqList = [&#39;x_freqs&#39;, &#39;freqs&#39;, &#39;freq&#39;, &#39;hz&#39;, &#39;f&#39;, &#39;frequency&#39;]
    if xtype.lower() in freqList:
        xlabel = &#39;Frequency [Hz]&#39;
    else:
        xlabel = &#39;Period [s]&#39;

    if save_dir is not None:
        filename = hvsr_data[&#39;input_params&#39;][&#39;site&#39;]
    else:
        filename = &#34;&#34;

    #ax = plt.gca()
    #fig = plt.gcf()

    anyKey = list(hvsr_data[xtype].keys())[0]
    x = hvsr_data[xtype][anyKey][:-1]
    y = hvsr_data[&#39;hvsr_curve&#39;]
    
    plotSuff=&#39;&#39;
    legendLoc = &#39;upper right&#39;
    
    plotHVSR = False
    for item in plot_type:
        if item.lower()==&#39;hvsr&#39;:
            plotHVSR=True
            ax.plot(x, y, color=&#39;k&#39;, label=&#39;H/V Ratio&#39;, zorder=1000)
            plotSuff=&#39;HVSRCurve_&#39;
            if &#39;-s&#39; not in plot_type:
                ax.fill_between(x, hvsr_data[&#39;hvsrm2&#39;], hvsr_data[&#39;hvsrp2&#39;], color=&#39;k&#39;, alpha=0.2, label=&#39;StDev&#39;,zorder=997)
                ax.plot(x, hvsr_data[&#39;hvsrm2&#39;], color=&#39;k&#39;, alpha=0.25, linewidth=0.5, zorder=998)
                ax.plot(x, hvsr_data[&#39;hvsrp2&#39;], color=&#39;k&#39;, alpha=0.25, linewidth=0.5, zorder=999)
            else:
                plotSuff = plotSuff+&#39;noStdDev_&#39;
            break

    ax.semilogx()
    ax.set_ylim(ylim)
    ax.set_xlim(xlim)
    ax.set_xlabel(xlabel)
    ax.set_ylabel(&#39;H/V Ratio&#39;+&#39;\n[&#39;+hvsr_data[&#39;method&#39;]+&#39;]&#39;)
    ax.set_title(hvsr_data[&#39;input_params&#39;][&#39;site&#39;])

    
    #print(&#34;label=&#39;comp&#39;&#34; in str(ax.__dict__[&#39;_axes&#39;]))
    for k in plot_type:   
        if k==&#39;p&#39; and &#39;all&#39; not in plot_type:
            plotSuff=plotSuff+&#39;BestPeak_&#39;
            
            bestPeakScore = 0
            for i, p in enumerate(hvsr_data[&#39;Peak Report&#39;]):
                if p[&#39;Score&#39;] &gt; bestPeakScore:
                    bestPeakScore = p[&#39;Score&#39;]
                    bestPeak = p

            ax.axvline(bestPeak[&#39;f0&#39;],color=&#39;k&#39;, linestyle=&#39;dotted&#39;, label=&#39;Peak&#39;)          
            if &#39;ann&#39; in plot_type:
                ax.annotate(&#39;Peak at &#39;+str(round(bestPeak[&#39;f0&#39;],2))+&#39;Hz&#39;, (bestPeak[&#39;f0&#39;], ax.get_ylim()[0]), xycoords=&#39;data&#39;, 
                                horizontalalignment=&#39;center&#39;, verticalalignment=&#39;bottom&#39;, 
                                bbox=dict(facecolor=&#39;w&#39;, edgecolor=&#39;none&#39;, alpha=0.8, pad=0.1))
                plotSuff = plotSuff+&#39;ann_&#39;

        elif k==&#39;p&#39;  and &#39;all&#39; in plot_type:
            plotSuff = plotSuff+&#39;allPeaks_&#39;

            ax.vlines(hvsr_data[&#39;hvsr_peak_freqs&#39;], ax.get_ylim()[0], ax.get_ylim()[1], colors=&#39;k&#39;, linestyles=&#39;dotted&#39;, label=&#39;Peak&#39;)          
            if &#39;ann&#39; in plot_type:
                for i, p in enumerate(hvsr_data[&#39;hvsr_peak_freqs&#39;]):
                    y = hvsr_data[&#39;hvsr_curve&#39;][hvsr_data[&#39;hvsr_peak_indices&#39;][i]]
                    ax.annotate(&#39;Peak at &#39;+str(round(p,2))+&#39;Hz&#39;, (p, 0.1), xycoords=&#39;data&#39;, 
                                    horizontalalignment=&#39;center&#39;, verticalalignment=&#39;bottom&#39;, 
                                    bbox=dict(facecolor=&#39;w&#39;, edgecolor=&#39;none&#39;, alpha=0.8, pad=0.1))
                plotSuff=plotSuff+&#39;ann_&#39;

        if &#39;t&#39; in k:
            plotSuff = plotSuff+&#39;allTWinCurves_&#39;

            if k==&#39;tp&#39;:
                for j, t in enumerate(hvsr_data[&#39;ind_hvsr_peak_indices&#39;]):
                    for i, v in enumerate(t):
                        v= x[v]
                        if i==0:
                            width = (x[i+1]-x[i])/16
                        else:
                            width = (x[i]-x[i-1])/16
                        if j == 0 and i==0:
                            ax.fill_betweenx(ylim,v-width,v+width, color=&#39;r&#39;, alpha=0.05, label=&#39;Individual H/V Peaks&#39;)
                        else:
                           ax.fill_betweenx(ylim,v-width,v+width, color=&#39;r&#39;, alpha=0.05)
            for t in hvsr_data[&#39;ind_hvsr_curves&#39;]:
                ax.plot(x, t, color=&#39;k&#39;, alpha=0.15, linewidth=0.8, linestyle=&#39;:&#39;)

        if &#39;c&#39; in k: #Spectrogram uses a different function, so c is unique to the component plot flag
            plotSuff = plotSuff+&#39;IndComponents_&#39;
            
            if &#39;c&#39; not in plot_type[0]:#This is part of the hvsr axis
                fig.tight_layout()
                axis2 = ax.twinx()
                compAxis = axis2
                #axis2 = plt.gca()
                #fig = plt.gcf()
                compAxis.set_ylabel(&#39;Amplitude&#39;+&#39;\n[m2/s4/Hz] [dB]&#39;)
                compAxis.set_facecolor([0,0,0,0])
                legendLoc2 = &#39;upper left&#39;
                
            else:
                ax.set_title(&#39;&#39;) #Remove title
                ax.sharex(kwargs[&#39;axes&#39;][&#39;hvsr&#39;])
                compAxis = ax
                legendLoc2 = &#39;upper right&#39;
                
            minY = []
            maxY = []
            for key in hvsr_data[&#39;psd_values_tavg&#39;]:
                minY.append(min(hvsr_data[&#39;ppsd_std_vals_m&#39;][key]))
                maxY.append(max(hvsr_data[&#39;ppsd_std_vals_p&#39;][key]))
            minY = min(minY)
            maxY = max(maxY)
            if maxY &gt; 20:
                maxY=20
            rng = maxY-minY
            pad = rng * 0.05
            ylim = [minY-pad, maxY+pad]
        
            compAxis.set_ylabel(&#39;COMPONENTS\nAmplitude\n[m2/s4/Hz] [dB]&#39;)
            compAxis.set_ylim(ylim)

            #Modify based on whether there are multiple charts
            if plotHVSR:
                linalpha = 0.2
                stdalpha = 0.05
            else:
                linalpha=1
                stdalpha=0.2
            
            #Plot individual components
            y={}
            for key in hvsr_data[&#39;psd_values_tavg&#39;]:
                y[key] = hvsr_data[&#39;psd_values_tavg&#39;][key][:-1]

                if key == &#39;Z&#39;:
                    pltColor = &#39;k&#39;
                elif key ==&#39;E&#39;:
                    pltColor = &#39;b&#39;
                elif key == &#39;N&#39;:
                    pltColor = &#39;r&#39;

                compAxis.plot(x, y[key], c=pltColor, label=key, alpha=linalpha)
                if &#39;-s&#39; not in plot_type:
                    compAxis.fill_between(x, hvsr_data[&#39;ppsd_std_vals_m&#39;][key][:-1], hvsr_data[&#39;ppsd_std_vals_p&#39;][key][:-1], color=pltColor, alpha=stdalpha)
                if plot_type[0] != &#39;c&#39;:
                    compAxis.legend(loc=legendLoc2)
            else:
                pass#ax.legend(loc=legendLoc)

    bbox = ax.get_window_extent()
    bboxStart = bbox.__str__().find(&#39;Bbox(&#39;,0,50)+5
    bboxStr = bbox.__str__()[bboxStart:].split(&#39;,&#39;)[:4]
    axisbox = []
    for i in bboxStr:
        i = i.split(&#39;=&#39;)[1]
        if &#39;)&#39; in i:
            i = i[:-1]
        axisbox.append(float(i))

    if kwargs[&#39;show_legend&#39;]:
        ax.legend(loc=legendLoc)

    __plot_current_fig(save_dir=save_dir, 
                        filename=filename, 
                        fig=fig, ax=ax,
                        plot_suffix=plotSuff, 
                        user_suffix=save_suffix, 
                        show=show)
    
    return fig, ax

#Private function to help for when to show and format and save plots
def __plot_current_fig(save_dir, filename, fig, ax, plot_suffix, user_suffix, show):
    &#34;&#34;&#34;Private function to support plot_hvsr, for plotting and showing plots&#34;&#34;&#34;
    #plt.gca()
    #plt.gcf()
    #fig.tight_layout() #May need to uncomment this

    #plt.subplots_adjust(top = 1, bottom = 0, right = 1, left = 0, hspace = 0, wspace = 0)

    if save_dir is not None:
        outFile = save_dir+&#39;/&#39;+filename+&#39;_&#39;+plot_suffix+str(datetime.datetime.today().date())+&#39;_&#39;+user_suffix+&#39;.png&#39;
        fig.savefig(outFile, bbox_inches=&#39;tight&#39;, pad_inches=0.2)
    if show:
        fig.canvas.draw()#.show()
        #fig.tight_layout()
        #plt.ion()
    return

#Plot specgtrogram, private supporting function for plot_hvsr
def _plot_specgram_hvsr(hvsr_data, fig=None, ax=None, save_dir=None, save_suffix=&#39;&#39;,**kwargs):
    &#34;&#34;&#34;Private function for plotting average spectrogram of all three channels from ppsds
    &#34;&#34;&#34;
    if fig is None and ax is None:
        fig, ax = plt.subplots()    

    if &#39;kwargs&#39; in kwargs.keys():
        kwargs = kwargs[&#39;kwargs&#39;]

    if &#39;peak_plot&#39; in kwargs.keys():
        peak_plot=True
        del kwargs[&#39;peak_plot&#39;]
    else:
        peak_plot=False
        

    if &#39;grid&#39; in kwargs.keys():
        ax.grid(which=kwargs[&#39;grid&#39;], alpha=0.25)
        del kwargs[&#39;grid&#39;]
        
    if &#39;ytype&#39; in kwargs:
        if kwargs[&#39;ytype&#39;]==&#39;freq&#39;:
            ylabel = &#39;Frequency [Hz]&#39;
            del kwargs[&#39;ytype&#39;]
        else:
            ylabel = &#39;Period [s]&#39;
        del kwargs[&#39;ytype&#39;]
    else:
        ylabel=&#39;Frequency [Hz]&#39;
        
    if &#39;detrend&#39; in kwargs.keys():
        detrend= kwargs[&#39;detrend&#39;]
        del kwargs[&#39;detrend&#39;]
    else:
        detrend=True

    if &#39;colorbar&#39; in kwargs.keys():
        colorbar = kwargs[&#39;colorbar&#39;]
        del kwargs[&#39;colorbar&#39;]
    else:
        colorbar=True

    if &#39;cmap&#39; in kwargs.keys():
        pass
    else:
        kwargs[&#39;cmap&#39;] = &#39;turbo&#39;

    ppsds = hvsr_data[&#39;ppsds&#39;]#[k][&#39;current_times_used&#39;]
    import matplotlib.dates as mdates
    anyKey = list(ppsds.keys())[0]

    psdHList =[]
    psdZList =[]
    for k in hvsr_data[&#39;psd_raw&#39;]:
        if &#39;z&#39; in k.lower():
            psdZList.append(hvsr_data[&#39;psd_raw&#39;][k])    
        else:
            psdHList.append(hvsr_data[&#39;psd_raw&#39;][k])
    
    #if detrend:
    #    psdArr = np.subtract(psdArr, np.median(psdArr, axis=0))
    psdArr = hvsr_data[&#39;ind_hvsr_curves&#39;].T

    xmin = min(hvsr_data[&#39;ppsds&#39;][anyKey][&#39;current_times_used&#39;][:-1]).matplotlib_date
    xmax = max(hvsr_data[&#39;ppsds&#39;][anyKey][&#39;current_times_used&#39;][:-1]).matplotlib_date
  
    tTicks = mdates.MinuteLocator(byminute=range(0,60,5))
    ax.xaxis.set_major_locator(tTicks)
    tTicks_minor = mdates.SecondLocator(bysecond=[0])
    ax.xaxis.set_minor_locator(tTicks_minor)

    tLabels = mdates.DateFormatter(&#39;%H:%M&#39;)
    ax.xaxis.set_major_formatter(tLabels)
    ax.tick_params(axis=&#39;x&#39;, labelsize=8)

    if hvsr_data[&#39;ppsds&#39;][anyKey][&#39;current_times_used&#39;][0].date != hvsr_data[&#39;ppsds&#39;][anyKey][&#39;current_times_used&#39;][-1].date:
        day = str(hvsr_data[&#39;ppsds&#39;][anyKey][&#39;current_times_used&#39;][0].date)+&#39; - &#39;+str(hvsr_data[&#39;ppsds&#39;][anyKey][&#39;current_times_used&#39;][1].date)
    else:
        day = str(hvsr_data[&#39;ppsds&#39;][anyKey][&#39;current_times_used&#39;][0].date)

    ymin = hvsr_data[&#39;input_params&#39;][&#39;hvsr_band&#39;][0]
    ymax = hvsr_data[&#39;input_params&#39;][&#39;hvsr_band&#39;][1]

    extList = [xmin, xmax, ymin, ymax]
  
    #ax = plt.gca()
    #fig = plt.gcf()

    freqticks = np.flip(hvsr_data[&#39;x_freqs&#39;][anyKey])
    yminind = np.argmin(np.abs(ymin-freqticks))
    ymaxind = np.argmin(np.abs(ymax-freqticks))
    freqticks = freqticks[yminind:ymaxind]

    #Set up axes, since data is already in semilog
    axy = ax.twinx()
    axy.set_yticks([])
    axy.zorder=0
    ax.zorder=1
    ax.set_facecolor(&#39;#ffffff00&#39;) #Create transparent background for front axis
    #plt.sca(axy)  
    im = ax.imshow(psdArr, origin=&#39;lower&#39;, extent=extList, aspect=&#39;auto&#39;, interpolation=&#39;nearest&#39;, **kwargs)
    ax.tick_params(left=False, right=False)
    #plt.sca(ax)
    if peak_plot:
        ax.hlines(hvsr_data[&#39;BestPeak&#39;][&#39;f0&#39;], xmin, xmax, colors=&#39;k&#39;, linestyles=&#39;dashed&#39;, alpha=0.5)

    #FreqTicks =np.arange(1,np.round(max(hvsr_data[&#39;x_freqs&#39;][anyKey]),0), 10)
    specTitle = ax.set_title(hvsr_data[&#39;input_params&#39;][&#39;site&#39;]+&#39;: Spectrogram&#39;)
    bgClr = (fig.get_facecolor()[0], fig.get_facecolor()[1], fig.get_facecolor()[2], 0.1)
    specTitle.set_color(bgClr)
    ax.set_xlabel(&#39;UTC Time \n&#39;+day)
    
    if colorbar:
        cbar = plt.colorbar(mappable=im)
        cbar.set_label(&#39;H/V Ratio&#39;)

    ax.set_ylabel(ylabel)
    ax.set_yticks(freqticks)
    ax.semilogy()
    ax.set_ylim(hvsr_data[&#39;input_params&#39;][&#39;hvsr_band&#39;])

    #fig.tight_layout()
    #plt.rcParams[&#39;figure.dpi&#39;] = 500
    #plt.rcParams[&#39;figure.figsize&#39;] = (12,4)
    fig.canvas.draw()
    #fig.tight_layout()
    #plt.show()
    return fig, ax

#Plot spectrogram from stream
def _plot_specgram_stream(stream, params=None, component=&#39;Z&#39;, stack_type=&#39;linear&#39;, detrend=&#39;mean&#39;, dbscale=True, fill_gaps=None,fig=None, ax=None, cmap_per=[0.1,0.9], ylimstd=5, show_plot=False, return_fig=True,  **kwargs):
    &#34;&#34;&#34;Function for plotting spectrogram in a nice matplotlib chart from an obspy.stream

    For more details on main function being called, see https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.specgram.html 

    Parameters
    ----------
    stream : obspy.core.stream.Stream object
        Stream for which to plot spectrogram
    params : dict, optional
        If dict, will read the hvsr_band from the a dictionary with a key [&#39;hvsr_band&#39;] (like the parameters dictionary). Otherwise, can read in the hvsr_band as a two-item list. Or, if None, defaults to [0.4,40], by default None.
    component : str or list, default=&#39;Z&#39;
        If string, should be one character long component, by default &#39;Z.&#39; If list, can contain &#39;E&#39;, &#39;N&#39;, &#39;Z&#39;, and will stack them per stack_type and stream.stack() method in obspy to make spectrogram.
    stack_type : str, default = &#39;linear&#39;
        Parameter to be read directly into stack_type parameter of Stream.stack() method of obspy streams, by default &#39;linear&#39;. See https://docs.obspy.org/packages/autogen/obspy.core.stream.Stream.stack.html
        Only matters if more than one component used.
    detrend : str, default = &#39;mean&#39;
        Parameter to be read directly into detrend parameter of matplotlib.pyplot.specgram, by default &#39;mean&#39;. See: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.specgram.html
    dbscale : bool, default = True
        If True, scale parameter of matplotlib.pyplot.specgram set to &#39;dB&#39;, by default True
    return_fig : bool, default = True
        Whether to return the figure from the function or just show it, by default True
    cmap_per : list, default = [0.1, 0.9]
        Two-item list wwith clip limits as percentage of values of colormap, so extremes do not taint colormap, by default [0.1,0.9]

    Returns
    -------
    fig
        If return_fig is True, matplotlib figure is returned
    ax
        If return_fig is True, matplotlib axis is returned
    &#34;&#34;&#34; 
    og_stream = stream.copy()

    #Get the latest start time and earliest end times of all components
    traceList = []
    maxStartTime = obspy.UTCDateTime(-1e10) #Go back pretty far (almost 400 years) to start with
    minEndTime = obspy.UTCDateTime(1e10)
    for comp in [&#39;E&#39;, &#39;N&#39;, &#39;Z&#39;]:
        #Get all traces from selected component in comp_st
        if isinstance(stream[0].data, np.ma.masked_array):
            stream = stream.split() 
        comp_st = stream.select(component=comp).copy()
        stream.merge()
        
        if comp in component:
            for tr in comp_st:
                #Get all traces specified for use in one list
                traceList.append(tr)

            if stream[0].stats.starttime &gt; maxStartTime:
                maxStartTime = stream[0].stats.starttime
            if stream[0].stats.endtime &lt; minEndTime:
                minEndTime = stream[0].stats.endtime

            if isinstance(comp_st[0].data, np.ma.masked_array):
                comp_st = comp_st.split()  

    #Trim all traces to the same start/end time for total
    for tr in traceList:
        tr.trim(starttime=maxStartTime, endtime=minEndTime)
    og_stream.trim(starttime=maxStartTime, endtime=minEndTime)      

    #Combine all traces into single, stacked trace/stream
    stream = obspy.Stream(traceList)
    stream.merge()
    if len(stream)&gt;1:
        stream.stack(group_by=&#39;all&#39;, npts_tol=200, stack_type=stack_type)  

    newFig= False
    if fig is None and ax is None:
        #Organize the chart layout
        mosaic = [[&#39;spec&#39;],
                  [&#39;spec&#39;],
                  [&#39;spec&#39;],
                  [&#39;spec&#39;],
                  [&#39;spec&#39;],
                  [&#39;spec&#39;],
                  [&#39;signalz&#39;],
                  [&#39;signalz&#39;], 
                  [&#39;signaln&#39;], 
                  [&#39;signale&#39;]]
        fig, ax = plt.subplot_mosaic(mosaic, sharex=True, gridspec_kw={&#39;hspace&#39;:0.3})
        #fig, ax = plt.subplots(nrows=2, ncols=1, sharex=True)
        newFig = True

    
    data = stream[0].data
    if isinstance(data, np.ma.MaskedArray) and fill_gaps is not None:
        data = data.filled(fill_gaps)
    sample_rate = stream[0].stats.sampling_rate

    if &#39;cmap&#39; in kwargs.keys():
        cmap=kwargs[&#39;cmap&#39;]
    else:
        cmap=&#39;turbo&#39;

    if params is None:
        hvsr_band = [0.4, 40]
    else:
        hvsr_band = params[&#39;hvsr_band&#39;]
    ymin = hvsr_band[0]
    ymax = hvsr_band[1]

    if dbscale:
        scale=&#39;dB&#39;
    else:
        scale=None
    with warnings.catch_warnings():
        warnings.simplefilter(&#39;ignore&#39;, category=RuntimeWarning)
        spec, freqs, times, im = ax[&#39;spec&#39;].specgram(x=data, Fs=sample_rate, detrend=detrend, scale_by_freq=True, scale=scale)
    im.remove()

    difference_array = freqs-ymin
    for i, d in enumerate(difference_array):
        if d &gt; 0:
            if i-1 &lt; 0:
                i=1
            minfreqInd = i-1
            break
            
    difference_array = freqs-ymax
    for i, d in enumerate(difference_array):
        if d &gt; 0:
            maxfreqInd = i-1
            break

    array_displayed = spec[minfreqInd:maxfreqInd,:]
    #freqs_displayed = freqs[minfreqInd:maxfreqInd]
    #im.set_data(array_displayed)
    vmin = np.nanpercentile(array_displayed, cmap_per[0]*100)
    vmax = np.nanpercentile(array_displayed, cmap_per[1]*100)
  
    
    decimation_factor = 10

    sTime = stream[0].stats.starttime
    timeList = {}
    mplTimes = {}
    
    if isinstance(og_stream[0].data, np.ma.masked_array):
        og_stream = og_stream.split()      
    og_stream.decimate(decimation_factor)
    og_stream.merge()

    for tr in og_stream:
        key = tr.stats.component
        timeList[key] = []
        mplTimes[key] = []
        for t in np.ma.getdata(tr.times()):
            newt = sTime + t
            timeList[key].append(newt)
            mplTimes[key].append(newt.matplotlib_date)
    
    #Ensure that the min and max times for each component are the same
    for i, k in enumerate(mplTimes.keys()):
        currMin = np.min(list(map(np.min, mplTimes[k])))
        currMax = np.max(list(map(np.max, mplTimes[k])))

        if i == 0:
            xmin = currMin
            xmax = currMax
        else:
            if xmin &gt; currMin:
                xmin = currMin
            if xmax &lt; currMax:
                xmax = currMax     
    
    norm = matplotlib.colors.Normalize(vmin=vmin, vmax=vmax)
    im = ax[&#39;spec&#39;].imshow(array_displayed, norm=norm, cmap=cmap, aspect=&#39;auto&#39;, interpolation=None, extent=[xmin,xmax,ymax,ymin])

    ax[&#39;spec&#39;].set_xlim([xmin, xmax])
    ax[&#39;spec&#39;].set_ylim([ymin, ymax])
    ax[&#39;spec&#39;].semilogy() 
    
    #cbar = plt.colorbar(mappable=im)
    #cbar.set_label(&#39;Power Spectral Density [dB]&#39;)
    #stream.spectrogram(samp_rate=sample_rate, axes=ax, per_lap=0.75, log=True, title=title, cmap=&#39;turbo&#39;, dbscale=dbscale, show=False)
    
    ax[&#39;spec&#39;].xaxis_date()
    ax[&#39;signalz&#39;].xaxis_date()
    ax[&#39;signaln&#39;].xaxis_date()
    ax[&#39;signale&#39;].xaxis_date()
    #tTicks = mdates.MinuteLocator(interval=5)
    #ax[0].xaxis.set_major_locator(tTicks)
    ax[&#39;signale&#39;].xaxis.set_major_locator(mdates.MinuteLocator(byminute=range(0,60,5)))
    ax[&#39;signale&#39;].xaxis.set_major_formatter(mdates.DateFormatter(&#39;%H:%M&#39;))
    ax[&#39;signale&#39;].xaxis.set_minor_locator(mdates.MinuteLocator(interval=1))
    ax[&#39;signale&#39;].tick_params(axis=&#39;x&#39;, labelsize=8)
    
    ax[&#39;signalz&#39;].plot(mplTimes[&#39;Z&#39;],og_stream.select(component=&#39;Z&#39;)[0].data, color=&#39;k&#39;, linewidth=0.25)
    ax[&#39;signaln&#39;].plot(mplTimes[&#39;N&#39;],og_stream.select(component=&#39;N&#39;)[0].data, color=&#39;k&#39;, linewidth=0.1)
    ax[&#39;signale&#39;].plot(mplTimes[&#39;E&#39;],og_stream.select(component=&#39;E&#39;)[0].data, color=&#39;k&#39;, linewidth=0.1)

    ax[&#39;spec&#39;].set_ylabel(&#39;Spectrogram: {}&#39;.format(component))
    ax[&#39;signalz&#39;].set_ylabel(&#39;Z&#39;)
    ax[&#39;signaln&#39;].set_ylabel(&#39;N&#39;)
    ax[&#39;signale&#39;].set_ylabel(&#39;E&#39;)
    
    for comp in mplTimes.keys():
        stD = np.abs(np.nanstd(np.ma.getdata(og_stream.select(component=comp)[0].data)))
        dmed = np.nanmedian(np.ma.getdata(og_stream.select(component=comp)[0].data))
        key = &#39;signal&#39;+comp.lower()
        ax[key].set_ylim([dmed-ylimstd*stD, dmed+ylimstd*stD])
    
    if params is None:
        fig.suptitle(&#39;HVSR Site: Spectrogram and Data&#39;)
    elif &#39;title&#39; in kwargs.keys():
        fig.suptitle(kwargs[&#39;title&#39;])
    else:
        if &#39;input_params&#39; in params.keys():
            sitename = params[&#39;input_params&#39;][&#39;site&#39;]
        else:
            sitename = params[&#39;site&#39;]
        fig.suptitle(&#39;{}\nSpectrogram and Data&#39;.format(sitename))
    
    day = &#34;{}-{}-{}&#34;.format(stream[0].stats.starttime.year, stream[0].stats.starttime.month, stream[0].stats.starttime.day)
    ax[&#39;signale&#39;].set_xlabel(&#39;UTC Time \n&#39;+day)

    if newFig:
        ogFigsize = matplotlib.rcParams[&#39;figure.figsize&#39;]
        fig = plt.gcf()
        matplotlib.rcParams[&#39;figure.figsize&#39;] = (40, 4)
        #plt.rcParams[&#39;figure.dpi&#39;] = 100
        #plt.rcParams[&#39;figure.figsize&#39;] = (5,4)
        #fig.tight_layout()
        plt.rcParams[&#39;figure.figsize&#39;] = ogFigsize
        
    fig.canvas.draw()

    if show_plot:
        plt.show()
    
    if return_fig:
        return fig, ax
    
    return

##Helper functions for checking peaks
#Initialize peaks
def __init_peaks(_x, _y, _index_list, _hvsr_band, peak_freq_range=[0.4, 40], _min_peak_amp=1):
    &#34;&#34;&#34; Initialize peaks.
        
        Creates dictionary with relevant information and removes peaks in hvsr curve that are not relevant for data analysis (outside HVSR_band)

        Parameters
        ----------
        x : list-like obj 
            List with x-values (frequency or period values)
        y : list-like obj 
            List with hvsr curve values
        index_list : list or array_like 
            List with indices of peaks
        _hvsr_band : list
            Two-item list with low and high frequency to limit frequency range of data analysis extent
        peak_freq_range : list
            Two-item list with low and high frequency to limit frequency range for checking for peaks
        _min_peak_amp : float
            Minimum amplitude to be used for peak selection (to limit number of meaningless peaks found)

        Returns
        -------
        _peak               : list 
            List of dictionaries, one for each input peak
    &#34;&#34;&#34;

    _peak = list()
    for _i in _index_list:
        if (_hvsr_band[0] &lt;= _x[_i] &lt;= _hvsr_band[1]) and (peak_freq_range[0] &lt;= _x[_i] &lt;= peak_freq_range[1]) and (_y[_i]&gt;_min_peak_amp):
            _peak.append({&#39;f0&#39;: float(_x[_i]), &#39;A0&#39;: float(_y[_i]), 
                          &#39;f-&#39;: None, &#39;f+&#39;: None, &#39;Sf&#39;: None, &#39;Sa&#39;: None,
                          &#39;Score&#39;: 0, 
                          &#39;Report&#39;: {&#39;Lw&#39;:&#39;&#39;, &#39;Nc&#39;:&#39;&#39;, &#39;Ïƒ_A(f)&#39;:&#39;&#39;, &#39;A(f-)&#39;:&#39;&#39;, &#39;A(f+)&#39;:&#39;&#39;, &#39;A0&#39;: &#39;&#39;, &#39;P+&#39;: &#39;&#39;, &#39;P-&#39;: &#39;&#39;, &#39;Sf&#39;: &#39;&#39;, &#39;Sa&#39;: &#39;&#39;},
                          &#39;PassList&#39;:{},
                          &#39;PeakPasses&#39;:False})
    return _peak

#Check reliability of HVSR of curve
def __check_curve_reliability(hvsr_data, _peak):
    &#34;&#34;&#34;Tests to check for reliable H/V curve

    Tests include:
        1) Peak frequency is greater than 10 / window length (f0 &gt; 10 / Lw)
            f0 = peak frequency [Hz]
            Lw = window length [seconds]
        2) Number of significant cycles (Nc) is greater than 200 (Nc(f0) &gt; 200)
            Nc = Lw * Nw * f0
                Lw = window length [sec]
                Nw = Number of windows used in analysis
                f0 = peak frequency [Hz]
        3) StDev of amplitude of H/V curve is less than 2 at all frequencies between 0.5f0 and 2f0
            (less than 3 if f0 is less than 0.5 Hz)
            f0 = peak frequency [Hz]
            StDev is a measure of the variation of all the H/V curves generated for each time window
                Our main H/V curve is the median of these

    Parameters
    ----------
    hvsr_data   : dict
        Dictionary containing all important information generated about HVSR curve
    _peak       : list
        A list of dictionaries, with each dictionary containing information about each peak

    Returns
    -------
    _peak   : list
        List of dictionaries, same as above, except with information about curve reliability tests added
    &#34;&#34;&#34;
    anyKey = list(hvsr_data[&#39;ppsds&#39;].keys())[0]#Doesn&#39;t matter which channel we use as key

    delta = hvsr_data[&#39;ppsds&#39;][anyKey][&#39;delta&#39;]
    window_len = (hvsr_data[&#39;ppsds&#39;][anyKey][&#39;len&#39;] * delta) #Window length in seconds
    window_num = np.array(hvsr_data[&#39;psd_raw&#39;][anyKey]).shape[0]

    for _i in range(len(_peak)):
        peakFreq= _peak[_i][&#39;f0&#39;]
        test1 = peakFreq &gt; 10/window_len

        nc = window_len * window_num * peakFreq
        test2 = nc &gt; 200

        halfF0 = peakFreq/2
        doublef0 = peakFreq*2
        
        test3 = True
        failCount = 0
        for i, freq in enumerate(hvsr_data[&#39;x_freqs&#39;][anyKey][:-1]):
            ###IS THIS RIGHT???
            if freq &gt;= halfF0 and freq &lt;doublef0:
                if peakFreq &gt;= 0.5:
                    if hvsr_data[&#39;hvsr_log_std&#39;][i] &gt;= 2:
                        test3=False
                        failCount +=1
                else: #if peak freq is less than 0.5
                    if hvsr_data[&#39;hvsr_log_std&#39;][i] &gt;= 3:
                        test3=False
                        failCount +=1

        if test1:
            _peak[_i][&#39;Report&#39;][&#39;Lw&#39;] = &#39;{} &gt; 10 / {}  {}&#39;.format(round(peakFreq,3), int(window_len), sprit_utils.check_mark())
        else:
            _peak[_i][&#39;Report&#39;][&#39;Lw&#39;] = &#39;{} &gt; 10 / {}  {}&#39;.format(round(peakFreq,3), int(window_len), &#39;âœ˜&#39;)

        if test2:
            _peak[_i][&#39;Report&#39;][&#39;Nc&#39;] = &#39;{} &gt; 200  {}&#39;.format(round(nc,0), sprit_utils.check_mark())
        else:
            _peak[_i][&#39;Report&#39;][&#39;Nc&#39;] = &#39;{} &gt; 200  {}&#39;.format(round(nc,0), &#39;âœ˜&#39;)

        if test3:
            if peakFreq &gt;= 0.5:
                compVal = 2
            else:
                compVal = 3
            _peak[_i][&#39;Report&#39;][&#39;Ïƒ_A(f)&#39;] = &#39;Ïƒ_A for all freqs {}-{} &lt; {}  {}&#39;.format(round(peakFreq*0.5, 3), round(peakFreq*2, 3), compVal, sprit_utils.check_mark())
        else:
            _peak[_i][&#39;Report&#39;][&#39;Ïƒ_A(f)&#39;] = &#39;Ïƒ_A for all freqs {}-{} &lt; {}  {}&#39;.format(round(peakFreq*0.5, 3), round(peakFreq*2, 3), compVal, &#39;âœ˜&#39;)

        _peak[_i][&#39;PassList&#39;][&#39;WindowLengthFreq.&#39;] = test1
        _peak[_i][&#39;PassList&#39;][&#39;SignificantCycles&#39;] = test2
        _peak[_i][&#39;PassList&#39;][&#39;LowCurveStDevOverTime&#39;] = test3
    return _peak

#Check clarity of peaks
def __check_clarity(_x, _y, _peak, do_rank=True):
    &#34;&#34;&#34;Check clarity of peak amplitude(s)

       Test peaks for satisfying amplitude clarity conditions as outlined by SESAME 2004:
           - there exist one frequency f-, lying between f0/4 and f0, such that A0 / A(f-) &gt; 2
           - there exist one frequency f+, lying between f0 and 4*f0, such that A0 / A(f+) &gt; 2
           - A0 &gt; 2

        Parameters
        ----------
        x : list-like obj 
            List with x-values (frequency or period values)
        y : list-like obj 
            List with hvsr curve values
        _peak : list
            List with dictionaries for each peak, containing info about that peak
        do_rank : bool, default=False
            Include Rank in output

        Returns
        -------
        _peak : list
            List of dictionaries, each containing the clarity test information for the different peaks that were read in
    &#34;&#34;&#34;
    global max_rank

    # Test each _peak for clarity.
    if do_rank:
        max_rank += 1

    if np.array(_x).shape[0] == 1000:
        jstart = len(_y)-2
    else:
        jstart = len(_y)-1

    
    for _i in range(len(_peak)):
        #Initialize as False
        _peak[_i][&#39;f-&#39;] = &#39;âœ˜&#39;
        _peak[_i][&#39;Report&#39;][&#39;A(f-)&#39;] = &#39;No A_h/v in freqs {}-{} &lt; {}  {}&#39;.format(round(_peak[_i][&#39;A0&#39;]/4, 3), round(_peak[_i][&#39;A0&#39;], 3), round(_peak[_i][&#39;A0&#39;]/2, 3), &#39;âœ˜&#39;)
        _peak[_i][&#39;PassList&#39;][&#39;PeakFreqClarityBelow&#39;] = False #Start with assumption that it is False until we find an instance where it is True
        for _j in range(jstart, -1, -1):
            # There exist one frequency f-, lying between f0/4 and f0, such that A0 / A(f-) &gt; 2.
            if (float(_peak[_i][&#39;f0&#39;]) / 4.0 &lt;= _x[_j] &lt; float(_peak[_i][&#39;f0&#39;])) and float(_peak[_i][&#39;A0&#39;]) / _y[_j] &gt; 2.0:
                _peak[_i][&#39;Score&#39;] += 1
                _peak[_i][&#39;f-&#39;] = &#39;%10.3f %1s&#39; % (_x[_j], sprit_utils.check_mark())
                _peak[_i][&#39;Report&#39;][&#39;A(f-)&#39;] = &#39;A({}): {} &lt; {}  {}&#39;.format(round(_x[_j], 3), round(_y[_j], 3), round(_peak[_i][&#39;A0&#39;]/2,3), sprit_utils.check_mark())
                _peak[_i][&#39;PassList&#39;][&#39;PeakFreqClarityBelow&#39;] = True
                break
            else:
                pass
    
    if do_rank:
        max_rank += 1
    for _i in range(len(_peak)):
        #Initialize as False
        _peak[_i][&#39;f+&#39;] = &#39;âœ˜&#39;
        _peak[_i][&#39;Report&#39;][&#39;A(f+)&#39;] = &#39;No A_h/v in freqs {}-{} &lt; {}  {}&#39;.format(round(_peak[_i][&#39;A0&#39;], 3), round(_peak[_i][&#39;A0&#39;]*4, 3), round(_peak[_i][&#39;A0&#39;]/2, 3), &#39;âœ˜&#39;)
        _peak[_i][&#39;PassList&#39;][&#39;PeakFreqClarityAbove&#39;] = False
        for _j in range(len(_x) - 1):

            # There exist one frequency f+, lying between f0 and 4*f0, such that A0 / A(f+) &gt; 2.
            if float(_peak[_i][&#39;f0&#39;]) * 4.0 &gt;= _x[_j] &gt; float(_peak[_i][&#39;f0&#39;]) and \
                    float(_peak[_i][&#39;A0&#39;]) / _y[_j] &gt; 2.0:
                _peak[_i][&#39;Score&#39;] += 1
                _peak[_i][&#39;f+&#39;] = &#39;%10.3f %1s&#39; % (_x[_j], sprit_utils.check_mark())
                _peak[_i][&#39;Report&#39;][&#39;A(f+)&#39;] = &#39;A({}): {} &lt; {}  {}&#39;.format(round(_x[_j], 3), round(_y[_j], 3), round(_peak[_i][&#39;A0&#39;]/2,3), sprit_utils.check_mark())
                _peak[_i][&#39;PassList&#39;][&#39;PeakFreqClarityAbove&#39;] = True
                break
            else:
                pass
#        if False in clarityPass:
#            _peak[_i][&#39;PassList&#39;][&#39;PeakFreqClarityBelow&#39;] = False
#        else:
#            _peak[_i][&#39;PassList&#39;][&#39;PeakFreqClarityAbove&#39;] = True

    #Amplitude Clarity test
    # Only peaks with A0 &gt; 2 pass
    if do_rank:
        max_rank += 1
    _a0 = 2.0
    for _i in range(len(_peak)):

        if float(_peak[_i][&#39;A0&#39;]) &gt; _a0:
            _peak[_i][&#39;Report&#39;][&#39;A0&#39;] = &#39;%10.2f &gt; %0.1f %1s&#39; % (_peak[_i][&#39;A0&#39;], _a0, sprit_utils.check_mark())
            _peak[_i][&#39;Score&#39;] += 1
            _peak[_i][&#39;PassList&#39;][&#39;PeakAmpClarity&#39;] = True
        else:
            _peak[_i][&#39;Report&#39;][&#39;A0&#39;] = &#39;%10.2f &gt; %0.1f %1s&#39; % (_peak[_i][&#39;A0&#39;], _a0, &#39;âœ˜&#39;)
            _peak[_i][&#39;PassList&#39;][&#39;PeakAmpClarity&#39;] = False

    return _peak

#Check the stability of the frequency peak
def __check_freq_stability(_peak, _peakm, _peakp):
    &#34;&#34;&#34;Test peaks for satisfying stability conditions 

    Test as outlined by SESAME 2004:
        - the _peak should appear at the same frequency (within a percentage Â± 5%) on the H/V
            curves corresponding to mean + and - one standard deviation.

    Parameters
    ----------
    _peak : list
        List of dictionaries containing input information about peak, without freq stability test
    _peakm : list
        List of dictionaries containing input information about peakm (peak minus one StDev in freq)
    _peakp : list
        List of dictionaries containing input information about peak (peak plus one StDev in freq)  

    Returns
    -------
    _peak : list
        List of dictionaries containing output information about peak test  
    &#34;&#34;&#34;
    global max_rank

    #
    # check Ïƒf and ÏƒA
    #
    max_rank += 1

    #First check below
    _found_m = list()
    for _i in range(len(_peak)):
        _dx = 1000000.
        _found_m.append(False)
        _peak[_i][&#39;Report&#39;][&#39;P-&#39;] = &#39;âœ˜&#39;
        for _j in range(len(_peakm)):
            if abs(_peakm[_j][&#39;f0&#39;] - _peak[_i][&#39;f0&#39;]) &lt; _dx:
                _index = _j
                _dx = abs(_peakm[_j][&#39;f0&#39;] - _peak[_i][&#39;f0&#39;])
            if _peak[_i][&#39;f0&#39;] * 0.95 &lt;= _peakm[_j][&#39;f0&#39;] &lt;= _peak[_i][&#39;f0&#39;] * 1.05:
                _peak[_i][&#39;Report&#39;][&#39;P-&#39;] = &#39;%0.3f within Â±5%s of %0.3f %1s&#39; % (_peakm[_j][&#39;f0&#39;], &#39;%&#39;,
                                                                                 _peak[_i][&#39;f0&#39;], sprit_utils.check_mark())
                _found_m[_i] = True
                break
        if _peak[_i][&#39;Report&#39;][&#39;P-&#39;] == &#39;âœ˜&#39;:
            _peak[_i][&#39;Report&#39;][&#39;P-&#39;] = &#39;%0.3f within Â±5%s of %0.3f %1s&#39; % (_peakm[_j][&#39;f0&#39;], &#39;%&#39;, ##changed i to j
                                                                             _peak[_i][&#39;f0&#39;], &#39;âœ˜&#39;)

    #Then Check above
    _found_p = list()
    for _i in range(len(_peak)):
        _dx = 1000000.
        _found_p.append(False)
        _peak[_i][&#39;Report&#39;][&#39;P+&#39;] = &#39;âœ˜&#39;
        for _j in range(len(_peakp)):
            if abs(_peakp[_j][&#39;f0&#39;] - _peak[_i][&#39;f0&#39;]) &lt; _dx:
                _index = _j
                _dx = abs(_peakp[_j][&#39;f0&#39;] - _peak[_i][&#39;f0&#39;])
            if _peak[_i][&#39;f0&#39;] * 0.95 &lt;= _peakp[_j][&#39;f0&#39;] &lt;= _peak[_i][&#39;f0&#39;] * 1.05:
                if _found_m[_i]:
                    _peak[_i][&#39;Report&#39;][&#39;P+&#39;] = &#39;%0.3f within Â±5%s of %0.3f %1s&#39; % (
                        _peakp[_j][&#39;f0&#39;], &#39;%&#39;, _peak[_i][&#39;f0&#39;], sprit_utils.check_mark())
                    _peak[_i][&#39;Score&#39;] += 1
                    _peak[_i][&#39;PassList&#39;][&#39;FreqStability&#39;] = True
                else:
                    _peak[_i][&#39;Report&#39;][&#39;P+&#39;] = &#39;%0.3f within Â±5%s of %0.3f %1s&#39; % (
                        _peakp[_j][&#39;f0&#39;], &#39;%&#39;, _peak[_i][&#39;f0&#39;], &#39;âœ˜&#39;)
                    _peak[_i][&#39;PassList&#39;][&#39;FreqStability&#39;] = False
                break
            else:
                _peak[_i][&#39;Report&#39;][&#39;P+&#39;] = &#39;%0.3f within Â±5%s of %0.3f %1s&#39; % (_peakp[_j][&#39;f0&#39;], &#39;%&#39;, _peak[_i][&#39;f0&#39;], &#39;âœ˜&#39;)
                _peak[_i][&#39;PassList&#39;][&#39;FreqStability&#39;] = False                
        if _peak[_i][&#39;Report&#39;][&#39;P+&#39;] == &#39;âœ˜&#39; and len(_peakp) &gt; 0:
            _peak[_i][&#39;Report&#39;][&#39;P+&#39;] = &#39;%0.3f within Â±5%s of %0.3f %1s&#39; % (
                _peakp[_j][&#39;f0&#39;], &#39;%&#39;, _peak[_i][&#39;f0&#39;], &#39;âœ˜&#39;)###changed i to j

    return _peak

#Check stability
def __check_stability(_stdf, _peak, _hvsr_log_std, rank):
    &#34;&#34;&#34;Test peaks for satisfying stability conditions as outlined by SESAME 2004
    This includes:
       - Ïƒf lower than a frequency dependent threshold Îµ(f)
       - ÏƒA (f0) lower than a frequency dependent threshold Î¸(f),


    Parameters
    ----------
    _stdf : list
        List with dictionaries containint frequency standard deviation for each peak
    _peak : list
        List of dictionaries containing input information about peak, without freq stability test
    _hvsr_log_std : list
        List of dictionaries containing log standard deviation along curve
    rank : int
        Integer value, higher value is &#34;higher-ranked&#34; peak, helps determine which peak is actual hvsr peak  

    Returns
    -------
    _peak : list
        List of dictionaries containing output information about peak test  
    &#34;&#34;&#34;

    global max_rank

    peakPassList = []
    #
    # check Ïƒf and ÏƒA
    #
    if rank:
        max_rank += 2
    for _i in range(len(_peak)):
        _peak[_i][&#39;Sf&#39;] = _stdf[_i]
        _peak[_i][&#39;Sa&#39;] = _hvsr_log_std[_i]
        _this_peak = _peak[_i]
        if _this_peak[&#39;f0&#39;] &lt; 0.2:
            _e = 0.25
            if _stdf[_i] &lt; _e * _this_peak[&#39;f0&#39;]:
                _peak[_i][&#39;Report&#39;][&#39;Sf&#39;] = &#39;%10.4f &lt; %0.2f * %0.3f %1s&#39; % (_stdf[_i], _e, _this_peak[&#39;f0&#39;],
                                                                            sprit_utils.check_mark())
                _this_peak[&#39;Score&#39;] += 1
                _this_peak[&#39;PassList&#39;][&#39;PeakStability_FreqStD&#39;] = True

            else:
                _peak[_i][&#39;Report&#39;][&#39;Sf&#39;] = &#39;%10.4f &lt; %0.2f * %0.3f  %1s&#39; % (_stdf[_i], _e, _this_peak[&#39;f0&#39;], &#39;âœ˜&#39;)
                _this_peak[&#39;PassList&#39;][&#39;PeakStability_FreqStD&#39;] = False

            _t = 0.48
            if _hvsr_log_std[_i] &lt; _t:
                _peak[_i][&#39;Report&#39;][&#39;Sa&#39;] = &#39;%10.4f &lt; %0.2f %1s&#39; % (_hvsr_log_std[_i], _t,
                                                                    sprit_utils.check_mark())
                _this_peak[&#39;Score&#39;] += 1
                _this_peak[&#39;PassList&#39;][&#39;PeakStability_AmpStD&#39;] = True
            else:
                _peak[_i][&#39;Report&#39;][&#39;Sa&#39;] = &#39;%10.4f &lt; %0.2f  %1s&#39; % (_hvsr_log_std[_i], _t, &#39;âœ˜&#39;)
                _this_peak[&#39;PassList&#39;][&#39;PeakStability_AmpStD&#39;] = False

        elif 0.2 &lt;= _this_peak[&#39;f0&#39;] &lt; 0.5:
            _e = 0.2
            if _stdf[_i] &lt; _e * _this_peak[&#39;f0&#39;]:
                _peak[_i][&#39;Report&#39;][&#39;Sf&#39;] = &#39;%10.4f &lt; %0.2f * %0.3f %1s&#39; % (_stdf[_i], _e, _this_peak[&#39;f0&#39;],
                                                                            sprit_utils.check_mark())
                _this_peak[&#39;Score&#39;] += 1
                _this_peak[&#39;PassList&#39;][&#39;PeakStability_FreqStD&#39;] = True
            else:
                _peak[_i][&#39;Report&#39;][&#39;Sf&#39;] = &#39;%10.4f &lt; %0.2f * %0.3f  %1s&#39; % (_stdf[_i], _e, _this_peak[&#39;f0&#39;], &#39;âœ˜&#39;)
                _this_peak[&#39;PassList&#39;][&#39;PeakStability_FreqStD&#39;] = False

            _t = 0.40
            if _hvsr_log_std[_i] &lt; _t:
                _peak[_i][&#39;Report&#39;][&#39;Sa&#39;] = &#39;%10.4f &lt; %0.2f %1s&#39; % (_hvsr_log_std[_i], _t,
                                                                    sprit_utils.check_mark())
                _this_peak[&#39;Score&#39;] += 1
                _this_peak[&#39;PassList&#39;][&#39;PeakStability_AmpStD&#39;] = True
            else:
                _peak[_i][&#39;Report&#39;][&#39;Sa&#39;] = &#39;%10.4f &lt; %0.2f  %1s&#39; % (_hvsr_log_std[_i], _t, &#39;âœ˜&#39;)
                _this_peak[&#39;PassList&#39;][&#39;PeakStability_AmpStD&#39;] = False

        elif 0.5 &lt;= _this_peak[&#39;f0&#39;] &lt; 1.0:
            _e = 0.15
            if _stdf[_i] &lt; _e * _this_peak[&#39;f0&#39;]:
                _peak[_i][&#39;Report&#39;][&#39;Sf&#39;] = &#39;%10.4f &lt; %0.2f * %0.3f %1s&#39; % (_stdf[_i], _e, _this_peak[&#39;f0&#39;],
                                                                            sprit_utils.check_mark())
                _this_peak[&#39;Score&#39;] += 1
                _this_peak[&#39;PassList&#39;][&#39;PeakStability_FreqStD&#39;] = True
            else:
                _peak[_i][&#39;Report&#39;][&#39;Sf&#39;] = &#39;%10.4f &lt; %0.2f * %0.3f  %1s&#39; % (_stdf[_i], _e, _this_peak[&#39;f0&#39;], &#39;âœ˜&#39;)
                _this_peak[&#39;PassList&#39;][&#39;PeakStability_FreqStD&#39;] = False

            _t = 0.3
            if _hvsr_log_std[_i] &lt; _t:
                _peak[_i][&#39;Report&#39;][&#39;Sa&#39;] = &#39;%10.4f &lt; %0.2f %1s&#39; % (_hvsr_log_std[_i], _t, sprit_utils.check_mark())
                _this_peak[&#39;Score&#39;] += 1
                _this_peak[&#39;PassList&#39;][&#39;PeakStability_AmpStD&#39;] = True
            else:
                _peak[_i][&#39;Report&#39;][&#39;Sa&#39;] = &#39;%10.4f &lt; %0.2f  %1s&#39; % (_hvsr_log_std[_i], _t, &#39;âœ˜&#39;)
                _this_peak[&#39;PassList&#39;][&#39;PeakStability_AmpStD&#39;] = False

        elif 1.0 &lt;= _this_peak[&#39;f0&#39;] &lt;= 2.0:
            _e = 0.1
            if _stdf[_i] &lt; _e * _this_peak[&#39;f0&#39;]:
                _peak[_i][&#39;Report&#39;][&#39;Sf&#39;] = &#39;%10.4f &lt; %0.2f * %0.3f %1s&#39; % (_stdf[_i], _e, _this_peak[&#39;f0&#39;],
                                                                            sprit_utils.check_mark())
                _this_peak[&#39;Score&#39;] += 1
                _this_peak[&#39;PassList&#39;][&#39;PeakStability_FreqStD&#39;] = True
            else:
                _peak[_i][&#39;Report&#39;][&#39;Sf&#39;] = &#39;%10.4f &lt; %0.2f * %0.3f %1s &#39; % (_stdf[_i], _e, _this_peak[&#39;f0&#39;], &#39;âœ˜&#39;)
                _this_peak[&#39;PassList&#39;][&#39;PeakStability_FreqStD&#39;] = False

            _t = 0.25
            if _hvsr_log_std[_i] &lt; _t:
                _peak[_i][&#39;Report&#39;][&#39;Sa&#39;] = &#39;%10.4f &lt; %0.2f %1s&#39; % (_hvsr_log_std[_i], _t, sprit_utils.check_mark())
                _this_peak[&#39;Score&#39;] += 1
                _this_peak[&#39;PassList&#39;][&#39;PeakStability_AmpStD&#39;] = True
            else:
                _peak[_i][&#39;Report&#39;][&#39;Sa&#39;] = &#39;%10.4f &lt; %0.2f  %1s&#39; % (_hvsr_log_std[_i], _t, &#39;âœ˜&#39;)
                _this_peak[&#39;PassList&#39;][&#39;PeakStability_AmpStD&#39;] = False

        elif _this_peak[&#39;f0&#39;] &gt; 0.2:
            _e = 0.05
            if _stdf[_i] &lt; _e * _this_peak[&#39;f0&#39;]:
                _peak[_i][&#39;Report&#39;][&#39;Sf&#39;] = &#39;%10.4f &lt; %0.2f * %0.3f %1s&#39; % (_stdf[_i], _e, _this_peak[&#39;f0&#39;],
                                                                            sprit_utils.check_mark())
                _this_peak[&#39;Score&#39;] += 1
                _this_peak[&#39;PassList&#39;][&#39;PeakStability_FreqStD&#39;] = True
            else:
                _peak[_i][&#39;Report&#39;][&#39;Sf&#39;] = &#39;%10.4f &lt; %0.2f * %0.3f  %1s&#39; % (_stdf[_i], _e, _this_peak[&#39;f0&#39;], &#39;âœ˜&#39;)
                _this_peak[&#39;PassList&#39;][&#39;PeakStability_FreqStD&#39;] = False

            _t = 0.2
            if _hvsr_log_std[_i] &lt; _t:
                _peak[_i][&#39;Report&#39;][&#39;Sa&#39;] = &#39;%10.4f &lt; %0.2f %1s&#39; % (_hvsr_log_std[_i], _t, sprit_utils.check_mark())
                _this_peak[&#39;Score&#39;] += 1
                _this_peak[&#39;PassList&#39;][&#39;PeakStability_AmpStD&#39;] = True
            else:
                _peak[_i][&#39;Report&#39;][&#39;Sa&#39;] = &#39;%10.4f &lt; %0.2f  %1s&#39; % (_hvsr_log_std[_i], _t, &#39;âœ˜&#39;)
                _this_peak[&#39;PassList&#39;][&#39;PeakStability_FreqStD&#39;] = False

    return _peak

#Get frequency standard deviation
def __get_stdf(x_values, indexList, hvsrPeaks):
    &#34;&#34;&#34;Private function to get frequency standard deviation, from multiple time-step HVSR curves&#34;&#34;&#34;
    stdf = list()
    for index in indexList:
        point = list()
        for j in range(len(hvsrPeaks)):
            p = None
            for k in range(len(hvsrPeaks.iloc[j])):
                if p is None:
                    p = hvsrPeaks.iloc[j][k]
                else:
                    #Find closest peak in current time to (current) main hvsr peak
                    if abs(index - hvsrPeaks.iloc[j][k]) &lt; abs(index - p):
                        p = hvsrPeaks.iloc[j][k]
                        #p = hvsrPeaks[j][k]
                        #print(p=p1, p, p1)
            if p is not None:
                point.append(p)
        point.append(index)
        v = list()
        for l in range(len(point)):
            v.append(x_values[point[l]])
        stdf.append(np.std(v))
    return stdf</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="sprit.sprit_hvsr.batch_data_read"><code class="name flex">
<span>def <span class="ident">batch_data_read</span></span>(<span>input_data, batch_type='table', param_col=None, batch_params=None, verbose=False, **readcsv_getMeta_fetch_kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to read data in data as a batch of multiple data files. This is best used through sprit.fetch_data(<em>args, source='batch', </em>*other_kwargs).</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>input_data</code></strong> :&ensp;<code>filepath</code> or <code>list</code></dt>
<dd>Input data information for how to read in data as batch</dd>
<dt><strong><code>batch_type</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Type of batch read, only 'table' and 'filelist' accepted. If 'table', will read data from a file read in using pandas.read_csv(), by default 'table'</dd>
<dt><strong><code>param_col</code></strong> :&ensp;<code>None</code> or <code>str</code>, optional</dt>
<dd>Name of parameter column from batch information file. Only used if a batch_type='table' and single parameter column is used, rather than one column per parameter (for single parameter column, parameters are formatted with = between keys/values and , between item pairs), by default None</dd>
<dt><strong><code>batch_params</code></strong> :&ensp;<code>list, dict,</code> or <code>None</code>, default <code>= None</code></dt>
<dd>Parameters to be used if batch_type='filelist'. If it is a list, needs to be the same length as input_data. If it is a dict, will be applied to all files in input_data and will combined with extra keyword arguments caught by **readcsv_getMeta_fetch_kwargs.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to print information to terminal during batch read, by default False</dd>
<dt><strong><code>**readcsv_getMeta_fetch_kwargs</code></strong></dt>
<dd>Keyword arguments that will be read into pandas.read_csv(), sprit.input_params, sprit.get_metadata(), and/or sprit.fetch_data()</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>Dictionary with each item representing a different file read in, and which consists of its own parameter dictionary to be used by the rest of the processing steps</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>IndexError</code></dt>
<dd><em>description</em></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def batch_data_read(input_data, batch_type=&#39;table&#39;, param_col=None, batch_params=None, verbose=False, **readcsv_getMeta_fetch_kwargs):
    &#34;&#34;&#34;Function to read data in data as a batch of multiple data files. This is best used through sprit.fetch_data(*args, source=&#39;batch&#39;, **other_kwargs).

    Parameters
    ----------
    input_data : filepath or list
        Input data information for how to read in data as batch
    batch_type : str, optional
        Type of batch read, only &#39;table&#39; and &#39;filelist&#39; accepted. If &#39;table&#39;, will read data from a file read in using pandas.read_csv(), by default &#39;table&#39;
    param_col : None or str, optional
        Name of parameter column from batch information file. Only used if a batch_type=&#39;table&#39; and single parameter column is used, rather than one column per parameter (for single parameter column, parameters are formatted with = between keys/values and , between item pairs), by default None
    batch_params : list, dict, or None, default = None
        Parameters to be used if batch_type=&#39;filelist&#39;. If it is a list, needs to be the same length as input_data. If it is a dict, will be applied to all files in input_data and will combined with extra keyword arguments caught by **readcsv_getMeta_fetch_kwargs.
    verbose : bool, optional
        Whether to print information to terminal during batch read, by default False
    **readcsv_getMeta_fetch_kwargs
        Keyword arguments that will be read into pandas.read_csv(), sprit.input_params, sprit.get_metadata(), and/or sprit.fetch_data()

    Returns
    -------
    dict
        Dictionary with each item representing a different file read in, and which consists of its own parameter dictionary to be used by the rest of the processing steps

    Raises
    ------
    IndexError
        _description_
    &#34;&#34;&#34;
    #First figure out columns
    input_params_params = input_params.__code__.co_varnames
    get_metadata_params = get_metadata.__code__.co_varnames
    fetch_data_params = fetch_data.__code__.co_varnames

    if batch_type==&#39;sample&#39;:
        sample_data=True
        batch_type=&#39;table&#39;
    else:
        sample_data = False
    # Dictionary to store the stream objects
    stream_dict = {}
    data_dict = {}
    if batch_type == &#39;table&#39;:
        if isinstance(input_data, pd.DataFrame):
            dataReadInfoDF = input_data
        elif isinstance(input_data, dict):
            #For params input
            pass
        else:#Read csv
            read_csv_kwargs = {k: v for k, v in locals()[&#39;readcsv_getMeta_fetch_kwargs&#39;].items() if k in pd.read_csv.__code__.co_varnames}
            dataReadInfoDF = pd.read_csv(input_data, **read_csv_kwargs)
            if &#39;datapath&#39; in dataReadInfoDF.columns:
                filelist = list(dataReadInfoDF[&#39;datapath&#39;])
            #dataReadInfoDF = dataReadInfoDF.replace(np.nan, None)

        #If this is sample data, we need to create absolute paths to the filepaths
        if sample_data:
            sample_data_dir = pathlib.Path(pkg_resources.resource_filename(__name__, &#39;resources/sample_data/&#39;))
            for index, row in dataReadInfoDF.iterrows():
                dataReadInfoDF.loc[index, &#39;datapath&#39;] = sample_data_dir.joinpath(row.loc[&#39;datapath&#39;])

        default_dict = {&#39;site&#39;:&#39;HVSR Site&#39;,
                    &#39;network&#39;:&#39;AM&#39;, 
                    &#39;station&#39;:&#39;RAC84&#39;, 
                    &#39;loc&#39;:&#39;00&#39;, 
                    &#39;channels&#39;:[&#39;EHZ&#39;, &#39;EHN&#39;, &#39;EHE&#39;],
                    &#39;acq_date&#39;:str(datetime.datetime.now().date()),
                    &#39;starttime&#39; : &#39;00:00:00.00&#39;,
                    &#39;endtime&#39; : &#39;23:59:59.999&#39;,
                    &#39;tzone&#39; : &#39;UTC&#39;,
                    &#39;xcoord&#39; : -88.2290526,
                    &#39;ycoord&#39; :  40.1012122,
                    &#39;elevation&#39; : 755,
                    &#39;input_crs&#39;:&#39;EPSG:4326&#39;,#4269 is NAD83, defautling to WGS
                    &#39;output_crs&#39;:&#39;EPSG:4326&#39;,
                    &#39;elev_unit&#39; : &#39;feet&#39;,
                    &#39;depth&#39; : 0,
                    &#39;instrument&#39; : &#39;Raspberry Shake&#39;,
                    &#39;metapath&#39; : &#39;&#39;,
                    &#39;hvsr_band&#39; : [0.4, 40],
                    &#39;write_path&#39;:&#39;&#39;,
                    &#39;source&#39;:&#39;file&#39;, 
                    &#39;export_format&#39;:&#39;mseed&#39;, 
                    &#39;detrend&#39;:&#39;spline&#39;, 
                    &#39;detrend_order&#39;:2, 
                    &#39;verbose&#39;:False}

        print(f&#34;\t{dataReadInfoDF.shape[0]} sites found: {list(dataReadInfoDF[&#39;site&#39;])}&#34;)
        if verbose:
            maxLength = 25
            maxColWidth = 12
            if dataReadInfoDF.shape[0] &gt; maxLength:
                print(f&#39;\t Showing information for first {maxLength} files only:&#39;)
            print()
            #Print nicely formated df
            #Print column names
            print(&#39;\t&#39;, end=&#39;&#39;)
            for col in dataReadInfoDF.columns:
                print(str(col)[:maxColWidth].ljust(maxColWidth), end=&#39;  &#39;)
            print(&#39;\n\t&#39;, end=&#39;&#39;)

            #Print separator
            tableLen = (maxColWidth+2)*len(dataReadInfoDF.columns)
            for r in range(tableLen):
                print(&#39;-&#39;, end=&#39;&#39;)
            print()

            #Print columns/rows
            for index, row in dataReadInfoDF.iterrows():
                print(&#39;\t&#39;, end=&#39;&#39;)
                for col in row:
                    if len(str(col)) &gt; maxColWidth:
                        print((str(col)[:maxColWidth-3]+&#39;...&#39;).ljust(maxColWidth), end=&#39;  &#39;)
                    else:
                        print(str(col)[:maxColWidth].ljust(maxColWidth), end=&#39;  &#39;)
                print()
            if dataReadInfoDF.shape[0] &gt; maxLength:
                endline = f&#39;\t...{dataReadInfoDF.shape[0]-maxLength} more rows in file.\n&#39;
            else:
                endline = &#39;\n&#39;
            print(endline)

            print(&#39;Fetching the following files:&#39;)
        param_dict_list = []
        verboseStatement = []
        if param_col is None: #Not a single parameter column, each col=parameter
            for row_ind in range(dataReadInfoDF.shape[0]):
                param_dict = {}
                verboseStatement.append([])
                for col in dataReadInfoDF.columns:
                    if col in input_params_params or col in get_metadata_params or col in fetch_data_params:
                        currParam = dataReadInfoDF.loc[row_ind, col]
                        if pd.isna(currParam) or currParam == &#39;nan&#39;:
                            if col in default_dict.keys():
                                param_dict[col] = default_dict[col] #Get default value
                                if verbose:
                                    if type(default_dict[col]) is str:
                                        verboseStatement[row_ind].append(&#34;\t\t&#39;{}&#39; parameter not specified in batch file. Using {}=&#39;{}&#39;&#34;.format(col, col, default_dict[col]))
                                    else:
                                        verboseStatement[row_ind].append(&#34;\t\t&#39;{}&#39; parameter not specified in batch file. Using {}={}&#34;.format(col, col, default_dict[col]))
                            else:
                                param_dict[col] = None
                        else:
                            param_dict[col] = dataReadInfoDF.loc[row_ind, col]
                param_dict_list.append(param_dict)
        else:
            if param_col not in dataReadInfoDF.columns:
                raise IndexError(&#39;{} is not a column in {} (columns are: {})&#39;.format(param_col, input_data, dataReadInfoDF.columns))
            for row in dataReadInfoDF[param_col]:
                param_dict = {}
                splitRow = str(row).split(&#39;,&#39;)
                for item in splitRow:
                    param_dict[item.split(&#39;=&#39;)[0]] = item.split(&#39;=&#39;)[1]
                param_dict_list.append(param_dict)
        #input_params(datapath,site,network,station,loc,channels, acq_date,starttime, endtime, tzone, xcoord, ycoord, elevation, depth, instrument, metapath, hvsr_band)
        #fetch_data(params, inv, source, trim_dir, export_format, detrend, detrend_order, verbose)
        #get_metadata(params, write_path)
    elif batch_type == &#39;filelist&#39;:
        if isinstance(batch_params, list):
            if len(batch_params) != len(input_data):
                raise RuntimeError(&#39;If batch_params is list, it must be the same length as input_data. len(batch_params)={} != len(input_data)={}&#39;.format(len(batch_params), len(input_data)))
            param_dict_list = batch_params
        elif isinstance(batch_params, dict):
            batch_params.update(readcsv_getMeta_fetch_kwargs)
            param_dict_list = []
            for i in range(len(input_data)):
                param_dict_list.append(batch_params)
        
        # Read and process each MiniSEED file
        for i, file in enumerate(input_data):
            if isinstance(file, obspy.core.stream.Stream):
                warnings.warn(&#39;Reading in a list of Obspy streams is not currently supported, but may be implemented in the future&#39;, FutureWarning)
                pass 
            else:
                param_dict_list[i][&#39;datapath&#39;] = file

    hvsr_metaDict = {}
    zfillDigs = len(str(len(param_dict_list))) #Get number of digits of length of param_dict_list
    i=0
    for i, param_dict in enumerate(param_dict_list):
        # Read the data file into a Stream object
        input_params_kwargs = {k: v for k, v in locals()[&#39;readcsv_getMeta_fetch_kwargs&#39;].items() if k in input_params.__code__.co_varnames}
        input_params_kwargs2 = {k: v for k, v in param_dict.items() if k in input_params.__code__.co_varnames}
        input_params_kwargs.update(input_params_kwargs2)

        params = input_params(**input_params_kwargs)

        fetch_data_kwargs = {k: v for k, v in locals()[&#39;readcsv_getMeta_fetch_kwargs&#39;].items() if k in fetch_data.__code__.co_varnames}
        fetch_data_kwargs2 = {k: v for k, v in param_dict.items() if k in fetch_data.__code__.co_varnames[0:7]}
        fetch_data_kwargs.update(fetch_data_kwargs2)
        
        try:
            params = fetch_data(params=params, **fetch_data_kwargs)
        except:
            params[&#39;ProcessingStatus&#39;][&#39;FetchDataStatus&#39;]=False
            params[&#39;ProcessingStatus&#39;][&#39;OverallStatus&#39;] = False            
        
        if verbose and params[&#39;ProcessingStatus&#39;][&#39;FetchDataStatus&#39;]:
            print(&#34;\t  {}&#34;.format(params[&#39;site&#39;]))
            if verboseStatement !=[]:
                for item in verboseStatement[i]:
                    print(item)
        elif verbose and not params[&#39;ProcessingStatus&#39;][&#39;FetchDataStatus&#39;]:
            print(&#34;\t  {} not read correctly. Processing will not be carried out.&#34;.format(params[&#39;site&#39;]))
                
        params[&#39;batch&#39;] = True

        if params[&#39;site&#39;] == default_dict[&#39;site&#39;]: #If site was not designated
            params[&#39;site&#39;] = &#34;{}_{}&#34;.format(params[&#39;site&#39;], str(i).zfill(zfillDigs))
            i+=1
        hvsr_metaDict[params[&#39;site&#39;]] = params

    hvsr_metaDict = HVSRBatch(hvsr_metaDict)

    return hvsr_metaDict</code></pre>
</details>
</dd>
<dt id="sprit.sprit_hvsr.check_instance"><code class="name flex">
<span>def <span class="ident">check_instance</span></span>(<span>init)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_instance(init):
    def wrapper(self, *args, **kwargs):
        # Check if the first argument is an instance of self.__class__
        if args and isinstance(args[0], self.__class__):
            # Copy its attributes to self
            self.__dict__.update(args[0].__dict__)
        else:
            # Call the original __init__ method
            init(self, *args, **kwargs)
    return wrapper</code></pre>
</details>
</dd>
<dt id="sprit.sprit_hvsr.check_peaks"><code class="name flex">
<span>def <span class="ident">check_peaks</span></span>(<span>hvsr_data, hvsr_band=[0.4, 40], peak_freq_range=[1, 20], verbose=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to run tests on HVSR peaks to find best one and see if it passes quality checks</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>hvsr_data</code></strong> :&ensp;<code>dict</code></dt>
<dd>Dictionary containing all the calculated information about the HVSR data (i.e., hvsr_out returned from process_hvsr)</dd>
<dt><strong><code>hvsr_band</code></strong> :&ensp;<code>tuple</code> or <code>list</code>, default=<code>[0.4, 40]</code></dt>
<dd>2-item tuple or list with lower and upper limit of frequencies to analyze</dd>
<dt><strong><code>peak_freq_range</code></strong> :&ensp;<code>tuple</code> or <code>list</code>, default=<code>[1, 20];</code></dt>
<dd>The frequency range within which to check for peaks. If there is an HVSR curve with multiple peaks, this allows the full range of data to be processed while limiting peak picks to likely range.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, default=<code>False</code></dt>
<dd>Whether to print results and inputs to terminal.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>hvsr_data
: HVSRData</code> or <code><a title="sprit.sprit_hvsr.HVSRBatch" href="#sprit.sprit_hvsr.HVSRBatch">HVSRBatch</a> object</code></dt>
<dd>Object containing previous input data, plus information about peak tests</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_peaks(hvsr_data, hvsr_band=[0.4, 40], peak_freq_range=[1, 20], verbose=False):
    &#34;&#34;&#34;Function to run tests on HVSR peaks to find best one and see if it passes quality checks

        Parameters
        ----------
        hvsr_data : dict
            Dictionary containing all the calculated information about the HVSR data (i.e., hvsr_out returned from process_hvsr)
        hvsr_band : tuple or list, default=[0.4, 40]
            2-item tuple or list with lower and upper limit of frequencies to analyze
        peak_freq_range : tuple or list, default=[1, 20];
            The frequency range within which to check for peaks. If there is an HVSR curve with multiple peaks, this allows the full range of data to be processed while limiting peak picks to likely range.
        verbose : bool, default=False
            Whether to print results and inputs to terminal.
        
        Returns
        -------
        hvsr_data   : HVSRData or HVSRBatch object
            Object containing previous input data, plus information about peak tests
    &#34;&#34;&#34;
    orig_args = locals().copy() #Get the initial arguments

    if (verbose and &#39;input_params&#39; not in hvsr_data.keys()) or (verbose and not hvsr_data[&#39;batch&#39;]):
        if isinstance(hvsr_data, HVSRData) and hvsr_data[&#39;batch&#39;]:
            pass
        else:
            print(&#39;\nChecking peaks in the H/V Curve (check_peaks())&#39;)
            print(&#39;\tUsing the following parameters:&#39;)
            for key, value in orig_args.items():
                if key==&#39;hvsr_data&#39;:
                    pass
                else:
                    print(&#39;\t  {}={}&#39;.format(key, value))
            print()
  
    #First, divide up for batch or not
    if isinstance(hvsr_data, HVSRBatch):
        if verbose:
            print(&#39;\t  Running in batch mode&#39;)
        #If running batch, we&#39;ll loop through each site
        for site_name in hvsr_data.keys():
            args = orig_args.copy() #Make a copy so we don&#39;t accidentally overwrite
            args[&#39;hvsr_data&#39;] =  hvsr_data[site_name] #Get what would normally be the &#34;params&#34; variable for each site
            if hvsr_data[site_name][&#39;ProcessingStatus&#39;][&#39;OverallStatus&#39;]:
                try:
                    hvsr_data[site_name] = _check_peaks_batch(**args) #Call another function, that lets us run this function again
                except:
                    if verbose:
                        print(f&#34;\t{site_name}: check_peaks() unsuccessful. Peaks not checked.&#34;)
                    else:
                        warnings.warn(f&#34;\t{site_name}: check_peaks() unsuccessful. Peaks not checked.&#34;, RuntimeWarning)
                
        hvsr_data = HVSRBatch(hvsr_data)
    else:
        if hvsr_data[&#39;ProcessingStatus&#39;][&#39;OverallStatus&#39;]:
            if not hvsr_band:
                hvsr_band = [0.4,40]
            hvsr_data[&#39;hvsr_band&#39;] = hvsr_band

            anyK = list(hvsr_data[&#39;x_freqs&#39;].keys())[0]

            x = hvsr_data[&#39;x_freqs&#39;][anyK] #Consistent for all curves
            y = hvsr_data[&#39;hvsr_curve&#39;] #Calculated based on &#34;Use&#34; column
            index_list = hvsr_data[&#39;hvsr_peak_indices&#39;] #Calculated based on hvsr_curve
            hvsrp = hvsr_data[&#39;hvsrp&#39;] #Calculated based on &#34;Use&#34; column
            hvsrm = hvsr_data[&#39;hvsrm&#39;] #Calculated based on &#34;Use&#34; column

            hvsrPeaks = hvsr_data[&#39;hvsr_df&#39;][hvsr_data[&#39;hvsr_df&#39;][&#39;Use&#39;]][&#39;CurvesPeakIndices&#39;]
            #hvsrPeaks = hvsr_data[&#39;ind_hvsr_peak_indices&#39;] #Original calculation

            hvsr_log_std = hvsr_data[&#39;hvsr_log_std&#39;]
            peak_freq_range = hvsr_data[&#39;peak_freq_range&#39;]

            #Do for hvsr
            peak = __init_peaks(x, y, index_list, hvsr_band, peak_freq_range)

            peak = __check_curve_reliability(hvsr_data, peak)
            peak = __check_clarity(x, y, peak, do_rank=True)

            #Do for hvsrp
            # Find  the relative extrema of hvsrp (hvsr + 1 standard deviation)
            if not np.isnan(np.sum(hvsrp)):
                index_p = __find_peaks(hvsrp)
            else:
                index_p = list()

            peakp = __init_peaks(x, hvsrp, index_p, hvsr_band, peak_freq_range)
            peakp = __check_clarity(x, hvsrp, peakp, do_rank=True)

            #Do for hvsrm
            # Find  the relative extrema of hvsrm (hvsr - 1 standard deviation)
            if not np.isnan(np.sum(hvsrm)):
                index_m = __find_peaks(hvsrm)
            else:
                index_m = list()

            peakm = __init_peaks(x, hvsrm, index_m, hvsr_band, peak_freq_range)
            peakm = __check_clarity(x, hvsrm, peakm, do_rank=True)

            stdf = __get_stdf(x, index_list, hvsrPeaks)

            peak = __check_freq_stability(peak, peakm, peakp)
            peak = __check_stability(stdf, peak, hvsr_log_std, rank=True)

            hvsr_data[&#39;Peak Report&#39;] = peak

            #Iterate through peaks and 
            #   Get the BestPeak based on the peak score
            #   Calculate whether each peak passes enough tests
            curveTests = [&#39;WindowLengthFreq.&#39;,&#39;SignificantCycles&#39;, &#39;LowCurveStDevOverTime&#39;]
            peakTests = [&#39;PeakFreqClarityBelow&#39;, &#39;PeakFreqClarityAbove&#39;, &#39;PeakAmpClarity&#39;, &#39;FreqStability&#39;, &#39;PeakStability_FreqStD&#39;, &#39;PeakStability_AmpStD&#39;]
            bestPeakScore = 0

            for p in hvsr_data[&#39;Peak Report&#39;]:
                #Get BestPeak
                if p[&#39;Score&#39;] &gt; bestPeakScore:
                    bestPeakScore = p[&#39;Score&#39;]
                    bestPeak = p

                #Calculate if peak passes criteria
                cTestsPass = 0
                pTestsPass = 0
                for testName in p[&#39;PassList&#39;].keys():
                    if testName in curveTests:
                        if p[&#39;PassList&#39;][testName]:
                            cTestsPass += 1
                    elif testName in peakTests:
                        if p[&#39;PassList&#39;][testName]:
                            pTestsPass += 1

                if cTestsPass == 3 and pTestsPass &gt;= 5:
                    p[&#39;PeakPasses&#39;] = True
                else:
                    p[&#39;PeakPasses&#39;] = False
                
            #Designate BestPeak in output dict
            if len(hvsr_data[&#39;Peak Report&#39;]) == 0:
                bestPeak={}
                print(f&#34;No Best Peak identified for {hvsr_data[&#39;site&#39;]}&#34;)

            hvsr_data[&#39;BestPeak&#39;] = bestPeak
        else:
            hvsr_data[&#39;BestPeak&#39;] = {}
            print(f&#34;Processing Errors: No Best Peak identified for {hvsr_data[&#39;site&#39;]}&#34;)
            
    return hvsr_data</code></pre>
</details>
</dd>
<dt id="sprit.sprit_hvsr.export_data"><code class="name flex">
<span>def <span class="ident">export_data</span></span>(<span>hvsr_data, export_path=None, ext='hvsr', verbose=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Export data into pickle format that can be read back in using import_data() so data does not need to be processed each time.
Default extension is .hvsr but it is still a pickled file that can be read in using pickle.load().</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>hvsr_data</code></strong> :&ensp;<code><a title="sprit.sprit_hvsr.HVSRData" href="#sprit.sprit_hvsr.HVSRData">HVSRData</a></code> or <code><a title="sprit.sprit_hvsr.HVSRBatch" href="#sprit.sprit_hvsr.HVSRBatch">HVSRBatch</a></code></dt>
<dd>Data to be exported</dd>
<dt><strong><code>export_path</code></strong> :&ensp;<code>str</code> or <code>filepath object</code>, default <code>= None</code></dt>
<dd>String or filepath object to be read by pathlib.Path() and/or a with open(export_path, 'wb') statement. If None, defaults to input datapath directory, by default None</dd>
<dt><strong><code>ext</code></strong> :&ensp;<code>str</code>, default <code>= 'hvsr'</code></dt>
<dd>Filepath extension to use for data file, by default 'hvsr'</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def export_data(hvsr_data, export_path=None, ext=&#39;hvsr&#39;, verbose=False):
    &#34;&#34;&#34;Export data into pickle format that can be read back in using import_data() so data does not need to be processed each time. 
    Default extension is .hvsr but it is still a pickled file that can be read in using pickle.load().

    Parameters
    ----------
    hvsr_data : HVSRData or HVSRBatch
        Data to be exported
    export_path : str or filepath object, default = None
        String or filepath object to be read by pathlib.Path() and/or a with open(export_path, &#39;wb&#39;) statement. If None, defaults to input datapath directory, by default None
    ext : str, default = &#39;hvsr&#39;
        Filepath extension to use for data file, by default &#39;hvsr&#39;
    &#34;&#34;&#34;
    def _do_export(_hvsr_data=hvsr_data, _export_path=export_path, _ext=ext):
        
        fname = f&#34;{_hvsr_data.site}_{_hvsr_data.acq_date}_pickled.{ext}&#34;
        if _export_path is None or _export_path is True:
            _export_path = _hvsr_data[&#39;datapath&#39;]
            _export_path = pathlib.Path(_export_path).with_name(fname)
        else:
            _export_path = pathlib.Path(_export_path)
            if _export_path.is_dir():
                _export_path = _export_path.joinpath(fname)    

        _export_path = str(_export_path)
        with open(_export_path, &#39;wb&#39;) as f:
            pickle.dump(_hvsr_data, f) 
            
        if verbose:
            print(f&#34;Processed data exported as pickled data to: {_export_path} [~{round(float(pathlib.Path(_export_path).stat().st_size)/2**20,1)} Mb]&#34;)    
            
    if isinstance(hvsr_data, HVSRBatch):
        for sitename in hvsr_data.keys():
            _do_export(hvsr_data[sitename], export_path, ext)
    elif isinstance(hvsr_data, HVSRData):
        _do_export(hvsr_data, export_path, ext)
    else:
        print(&#34;Error in data export. Data must be either of type sprit.HVSRData or sprit.HVSRBatch&#34;)         
    return</code></pre>
</details>
</dd>
<dt id="sprit.sprit_hvsr.fetch_data"><code class="name flex">
<span>def <span class="ident">fetch_data</span></span>(<span>params, inv=None, source='file', trim_dir=None, export_format='mseed', detrend='spline', detrend_order=2, update_metadata=True, plot_input_stream=False, verbose=False, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fetch_data(params, inv=None, source=&#39;file&#39;, trim_dir=None, export_format=&#39;mseed&#39;, detrend=&#39;spline&#39;, detrend_order=2, update_metadata=True, plot_input_stream=False, verbose=False, **kwargs):
    import warnings

    &#34;&#34;&#34;Fetch ambient seismic data from a source to read into obspy stream
        
        Parameters
        ----------
        params  : dict
            Dictionary containing all the necessary params to get data.
                Parameters defined using input_params() function.
        inv     : obspy inventory object, default=None
            Obspy inventory object containing metadata for instrument that collected data to be fetched. By default, the inventory object is read from params[&#39;inv&#39;], but this can be manually specified here too.
        source  : str, {&#39;raw&#39;, &#39;dir&#39;, &#39;file&#39;, &#39;batch&#39;}
            String indicating where/how data file was created. For example, if raw data, will need to find correct channels.
                &#39;raw&#39; finds raspberry shake data, from raw output copied using scp directly from Raspberry Shake, either in folder or subfolders; 
                &#39;dir&#39; is used if the day&#39;s 3 component files (currently Raspberry Shake supported only) are all 3 contained in a directory by themselves.
                &#39;file&#39; is used if the params[&#39;datapath&#39;] specified in input_params() is the direct filepath to a single file to be read directly into an obspy stream.
                &#39;batch&#39; is used to read a list or specified set of seismic files. 
                    Most commonly, a csv file can be read in with all the parameters. Each row in the csv is a separate file. Columns can be arranged by parameter.
        trim_dir : None or str or pathlib obj, default=None
            If None (or False), data is not trimmed in this function.
            Otherwise, this is the directory to save trimmed and exported data.
        export_format: str=&#39;mseed&#39;
            If trim_dir is not False, this is the format in which to save the data
        detrend : str or bool, default=&#39;spline&#39;
            If False, data is not detrended.
            Otherwise, this should be a string accepted by the type parameter of the obspy.core.trace.Trace.detrend method: https://docs.obspy.org/packages/autogen/obspy.core.trace.Trace.detrend.html
        detrend_order : int, default=2
            If detrend parameter is &#39;spline&#39; or &#39;polynomial&#39;, this is passed directly to the order parameter of obspy.core.trace.Trace.detrend method.
        update_metadata : bool, default=True
            Whether to update the metadata file, used primarily with Raspberry Shake data which uses a generic inventory file.
        plot_input_stream : bool, default=False
            Whether to plot the raw input stream. This plot includes a spectrogram (Z component) and the raw (with decimation for speed) plots of each component signal.
        verbose : bool, default=False
            Whether to print outputs and inputs to the terminal
        **kwargs
            Keywords arguments, primarily for &#39;batch&#39; and &#39;dir&#39; sources
            
        Returns
        -------
        params : HVSRData or HVSRBatch object
            Same as params parameter, but with an additional &#34;stream&#34; attribute with an obspy data stream with 3 traces: Z (vertical), N (North-south), and E (East-west)
        &#34;&#34;&#34;
    if source != &#39;batch&#39; and verbose:
        print(&#39;\nFetching data (fetch_data())&#39;)
        print()

    params = get_metadata(params, update_metadata=update_metadata, source=source)
    inv = params[&#39;inv&#39;]
    date=params[&#39;acq_date&#39;]

    #Cleanup for gui input
    if &#39;}&#39; in str(params[&#39;datapath&#39;]):
        params[&#39;datapath&#39;] = params[&#39;datapath&#39;].as_posix().replace(&#39;{&#39;,&#39;&#39;)
        params[&#39;datapath&#39;] = params[&#39;datapath&#39;].split(&#39;}&#39;)
    
    sampleListNos = [&#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;4&#39;, &#39;5&#39;, &#39;6&#39;]
    sampleList = [&#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;4&#39;, &#39;5&#39;, &#39;6&#39;, &#39;batch&#39;, &#39;sample&#39;, &#39;sample_batch&#39;]
    for s in sampleListNos:
        sampleList.append(f&#39;sample{s}&#39;)
        sampleList.append(f&#39;sample_{s}&#39;)

    #Make sure datapath is pointing to an actual file
    if isinstance(params[&#39;datapath&#39;],list):
        for i, d in enumerate(params[&#39;datapath&#39;]):
            params[&#39;datapath&#39;][i] = sprit_utils.checkifpath(str(d).strip())
    else:
        dPath = sprit_utils.checkifpath(params[&#39;datapath&#39;], sample_list=sampleList)

    inst = params[&#39;instrument&#39;]

    #Need to put dates and times in right formats first
    if type(date) is datetime.datetime:
        doy = date.timetuple().tm_yday
        year = date.year
    elif type(date) is datetime.date:
        date = datetime.datetime.combine(date, datetime.time(hour=0, minute=0, second=0))
        doy = date.timetuple().tm_yday
        year = date.year
    elif type(date) is tuple:
        if date[0]&gt;366:
            raise ValueError(&#39;First item in date tuple must be day of year (0-366)&#39;, 0)
        elif date[1] &gt; datetime.datetime.now().year:
            raise ValueError(&#39;Second item in date tuple should be year, but given item is in the future&#39;, 0)
        else:
            doy = date[0]
            year = date[1]
    elif type(date) is str:
        if &#39;/&#39; in date:
            dateSplit = date.split(&#39;/&#39;)            
        elif &#39;-&#39; in date:
            dateSplit = date.split(&#39;-&#39;)
        else:
            dateSplit = date

        if int(dateSplit[0]) &gt; 31:
            date = datetime.datetime(int(dateSplit[0]), int(dateSplit[1]), int(dateSplit[2]))
            doy = date.timetuple().tm_yday
            year = date.year
        elif int(dateSplit[0])&lt;=12 and int(dateSplit[2]) &gt; 31:
            warnings.warn(&#34;Preferred date format is &#39;yyyy-mm-dd&#39; or &#39;yyyy/mm/dd&#39;. Will attempt to parse date.&#34;)
            date = datetime.datetime(int(dateSplit[2]), int(dateSplit[0]), int(dateSplit[1]))
            doy = date.timetuple().tm_yday
            year = date.year
        else:
            warnings.warn(&#34;Preferred date format is &#39;yyyy-mm-dd&#39; or &#39;yyyy/mm/dd&#39;. Cannot parse date.&#34;)
    elif type(date) is int:
        doy = date
        year = datetime.datetime.today().year
    else: #FOR NOW, need to update
        date = datetime.datetime.now()
        doy = date.timetuple().tm_yday
        year = date.year
        warnings.warn(&#34;Did not recognize date, using year {} and day {}&#34;.format(year, doy))

    #Select which instrument we are reading from (requires different processes for each instrument)
    raspShakeInstNameList = [&#39;raspberry shake&#39;, &#39;shake&#39;, &#39;raspberry&#39;, &#39;rs&#39;, &#39;rs3d&#39;, &#39;rasp. shake&#39;, &#39;raspshake&#39;]

    #Select how reading will be done
    if source==&#39;raw&#39;:
        if inst.lower() in raspShakeInstNameList:
            try:
                rawDataIN = __read_RS_file_struct(dPath, source, year, doy, inv, params, verbose=verbose)
            except:
                raise RuntimeError(f&#34;Data not fetched for {params[&#39;site&#39;]}. Check input parameters or the data file.&#34;)
                return params
    elif source==&#39;dir&#39;:
        if inst.lower() in raspShakeInstNameList:
            rawDataIN = __read_RS_file_struct(dPath, source, year, doy, inv, params, verbose=verbose)
        else:
            obspyFiles = {}
            for obForm in obspyFormats:
                temp_file_glob = pathlib.Path(dPath.as_posix().lower()).glob(&#39;.&#39;+obForm.lower())
                for f in temp_file_glob:
                    currParams = params
                    currParams[&#39;datapath&#39;] = f
                    curr_data = fetch_data(params, source=&#39;file&#39;, #all the same as input, except just reading the one file using the source=&#39;file&#39;
                                trim_dir=trim_dir, export_format=export_format, detrend=detrend, detrend_order=detrend_order, update_metadata=update_metadata, verbose=verbose, **kwargs), 
                    obspyFiles[f.stem] = curr_data  #Add path object to dict, with filepath&#39;s stem as the site name
            return HVSRBatch(obspyFiles)
        
    elif source==&#39;file&#39; and str(params[&#39;datapath&#39;]).lower() not in sampleList:
        if isinstance(dPath, list) or isinstance(dPath, tuple):
            rawStreams = []
            for datafile in dPath:
                rawStream = obspy.read(datafile)
                rawStreams.append(rawStream) #These are actually streams, not traces
            
            for i, stream in enumerate(rawStreams):
                if i == 0:
                    rawDataIN = obspy.Stream(stream) #Just in case
                else:
                    rawDataIN = rawDataIN + stream #This adds a stream/trace to the current stream object
            
        elif str(dPath)[:6].lower()==&#39;sample&#39;:
            pass
        else:
            rawDataIN = obspy.read(dPath)#, starttime=obspy.core.UTCDateTime(params[&#39;starttime&#39;]), endttime=obspy.core.UTCDateTime(params[&#39;endtime&#39;]), nearest_sample =True)
        import warnings
        with warnings.catch_warnings():
            warnings.simplefilter(action=&#39;ignore&#39;, category=UserWarning)
            rawDataIN.attach_response(inv)
    elif source==&#39;batch&#39; and str(params[&#39;datapath&#39;]).lower() not in sampleList:
        if verbose:
            print(&#39;\nFetching data (fetch_data())&#39;)
        batch_data_read_kwargs = {k: v for k, v in locals()[&#39;kwargs&#39;].items() if k in batch_data_read.__code__.co_varnames}
        params = batch_data_read(input_data=params[&#39;datapath&#39;], verbose=verbose, **batch_data_read_kwargs)
        params = HVSRBatch(params)
        return params
    elif str(params[&#39;datapath&#39;]).lower() in sampleList or f&#34;sample{params[&#39;datapath&#39;].lower()}&#34; in sampleList:
        sample_data_dir = pathlib.Path(pkg_resources.resource_filename(__name__, &#39;resources/sample_data/&#39;))
        if source==&#39;batch&#39;:
            params[&#39;datapath&#39;] = sample_data_dir.joinpath(&#39;Batch_SampleData.csv&#39;)
            params = batch_data_read(input_data=params[&#39;datapath&#39;], batch_type=&#39;sample&#39;, verbose=verbose)
            params = HVSRBatch(params)
            return params

        elif source==&#39;dir&#39;:
            params[&#39;datapath&#39;] = sample_data_dir.joinpath(&#39;Batch_SampleData.csv&#39;)
            params = batch_data_read(input_data=params[&#39;datapath&#39;], batch_type=&#39;sample&#39;, verbose=verbose)
            params = HVSRBatch(params)
            return params

        elif source==&#39;file&#39;:
            params[&#39;datapath&#39;] = str(params[&#39;datapath&#39;]).lower()
            
            if params[&#39;datapath&#39;].lower() in sampleFileKeyMap.keys():
                params[&#39;datapath&#39;] = sampleFileKeyMap[params[&#39;datapath&#39;].lower()]
            else:
                params[&#39;datapath&#39;] = sample_data_dir.joinpath(&#39;SampleHVSRSite1_AM.RAC84.00.2023.046_2023-02-15_1704-1734.MSEED&#39;)

            dPath = params[&#39;datapath&#39;]
            rawDataIN = obspy.read(dPath)#, starttime=obspy.core.UTCDateTime(params[&#39;starttime&#39;]), endttime=obspy.core.UTCDateTime(params[&#39;endtime&#39;]), nearest_sample =True)
            import warnings
            with warnings.catch_warnings():
                warnings.simplefilter(action=&#39;ignore&#39;, category=UserWarning)
                rawDataIN.attach_response(inv)
    else:
        try:
            rawDataIN = obspy.read(dPath)
            rawDataIN.attach_response(inv)
        except:
            RuntimeError(f&#39;source={source} not recognized, and datapath cannot be read using obspy.read()&#39;)

    try:
        dataIN = rawDataIN.copy()
        if source!=&#39;raw&#39;:
            #Use metadata from file for;
            # site
            if params[&#39;site&#39;] == &#34;HVSR Site&#34;:
                params[&#39;site&#39;] = dPath.stem
                params[&#39;params&#39;][&#39;site&#39;] = dPath.stem
            
            # network
            if str(params[&#39;net&#39;]) == &#39;AM&#39;:
                params[&#39;net&#39;] = dataIN[0].stats.network
                params[&#39;params&#39;][&#39;net&#39;] = dataIN[0].stats.network

            # station
            if str(params[&#39;sta&#39;]) == &#39;RAC84&#39;:
                params[&#39;sta&#39;] = dataIN[0].stats.station
                params[&#39;params&#39;][&#39;sta&#39;] = dataIN[0].stats.station

            # loc
            if str(params[&#39;loc&#39;]) == &#39;00&#39;:
                params[&#39;loc&#39;] = dataIN[0].stats.location
                params[&#39;params&#39;][&#39;loc&#39;] = dataIN[0].stats.location
            
            # channels
            channelList = []
            if str(params[&#39;cha&#39;]) == [&#39;EHZ&#39;, &#39;EHN&#39;, &#39;EHE&#39;]:
                for tr in dataIN:
                    if tr.stats.channel not in channelList:
                        channelList.append(tr.stats.channel)
                        channelList.sort(reverse=True) #Just so z is first, just in case
                params[&#39;cha&#39;] = channelList
                params[&#39;params&#39;][&#39;cha&#39;] = channelList
           
            # Acquisition date
            if str(params[&#39;acq_date&#39;]) == str(datetime.datetime.now().date()):
                params[&#39;acq_date&#39;] = dataIN[0].stats.starttime.date

            # starttime
            today_Starttime = obspy.UTCDateTime(datetime.datetime(year=datetime.date.today().year, month=datetime.date.today().month,
                                                                 day = datetime.date.today().day,
                                                                hour=0, minute=0, second=0, microsecond=0))
            maxStarttime = datetime.time(hour=0, minute=0, second=0, microsecond=0)
            if str(params[&#39;starttime&#39;]) == str(today_Starttime):
                for tr in dataIN.merge():
                    currTime = datetime.time(hour=tr.stats.starttime.hour, minute=tr.stats.starttime.minute, 
                                       second=tr.stats.starttime.second, microsecond=tr.stats.starttime.microsecond)
                    if currTime &gt; maxStarttime:
                        maxStarttime = currTime

                newStarttime = obspy.UTCDateTime(datetime.datetime(year=params[&#39;acq_date&#39;].year, month=params[&#39;acq_date&#39;].month,
                                                                 day = params[&#39;acq_date&#39;].day,
                                                                hour=maxStarttime.hour, minute=maxStarttime.minute, 
                                                                second=maxStarttime.second, microsecond=maxStarttime.microsecond))
                params[&#39;starttime&#39;] = newStarttime
                params[&#39;params&#39;][&#39;starttime&#39;] = newStarttime

            # endttime
            today_Endtime = obspy.UTCDateTime(datetime.datetime(year=datetime.date.today().year, month=datetime.date.today().month,
                                                                 day = datetime.date.today().day,
                                                                hour=23, minute=59, second=59, microsecond=999999))
            minEndtime = datetime.time(hour=23, minute=59, second=59, microsecond=999999)
            if str(params[&#39;endtime&#39;]) == str(today_Endtime):
                for tr in dataIN.merge():
                    currTime = datetime.time(hour=tr.stats.endtime.hour, minute=tr.stats.endtime.minute, 
                                       second=tr.stats.endtime.second, microsecond=tr.stats.endtime.microsecond)
                    if currTime &lt; minEndtime:
                        minEndtime = currTime
                newEndtime = obspy.UTCDateTime(datetime.datetime(year=params[&#39;acq_date&#39;].year, month=params[&#39;acq_date&#39;].month,
                                                                 day = params[&#39;acq_date&#39;].day,
                                                                hour=minEndtime.hour, minute=minEndtime.minute, 
                                                                second=minEndtime.second, microsecond=minEndtime.microsecond))
                params[&#39;endtime&#39;] = newEndtime
                params[&#39;params&#39;][&#39;endtime&#39;] = newEndtime


            #print(dataIN)
            #print(params[&#39;starttime&#39;])
            #print(params[&#39;endtime&#39;])
            dataIN = dataIN.split()
            dataIN = dataIN.trim(starttime=params[&#39;starttime&#39;], endtime=params[&#39;endtime&#39;])
            dataIN.merge()
            #print(dataIN)
    except:
        raise RuntimeError(&#39;Data not fetched. Check your input parameters or the data file.&#39;)
        
    #Trim and save data as specified
    if not trim_dir:
        pass
    else:
        if isinstance(params, HVSRBatch):
            pass
        else:
            dataIN = _trim_data(input=params, stream=dataIN, export_dir=trim_dir, source=source, export_format=export_format)

    #Split data if masked array (if there are gaps)...detrending cannot be done without
    for tr in dataIN:
        if isinstance(tr.data, np.ma.masked_array):
            dataIN = dataIN.split()
            #Splits entire stream if any trace is masked_array
            break

    #Detrend data
    if isinstance(params, HVSRBatch):
        pass
    else:
        dataIN =  __detrend_data(input=dataIN, detrend=detrend, detrend_order=detrend_order, verbose=verbose, source=source)

    #Remerge data
    dataIN = dataIN.merge(method=1)

    if plot_input_stream:
        #dataIN.plot(method=&#39;full&#39;, linewidth=0.25)
        params[&#39;InputPlot&#39;] = _plot_specgram_stream(stream=dataIN, params=params, component=&#39;Z&#39;, stack_type=&#39;linear&#39;, detrend=&#39;mean&#39;, dbscale=True, fill_gaps=None, ylimstd=3, return_fig=True, fig=None, ax=None, show_plot=False)
        _get_removed_windows(input=dataIN, fig=params[&#39;InputPlot&#39;][0], ax=params[&#39;InputPlot&#39;][1], lineArtist =[], winArtist = [], existing_lineArtists=[], existing_xWindows=[], exist_win_format=&#39;matplotlib&#39;, keep_line_artists=True, time_type=&#39;matplotlib&#39;, show_plot=True)

        #Sort channels (make sure Z is first, makes things easier later)
    if isinstance(params, HVSRBatch):
        pass
    else:
        dataIN = _sort_channels(input=dataIN, source=source, verbose=verbose)

    if &#39;clean_ends&#39; not in kwargs.keys():
        clean_ends=True 
    else:
        clean_ends = kwargs[&#39;clean_ends&#39;]

    if clean_ends:
        maxStarttime = datetime.datetime.utcnow().replace(tzinfo=datetime.timezone.utc) - datetime.timedelta(days=36500) #100 years ago
        minEndtime = datetime.datetime.utcnow().replace(tzinfo=datetime.timezone.utc) 

        for tr in dataIN:
            currStarttime = datetime.datetime(year=tr.stats.starttime.year, month=tr.stats.starttime.month, day=tr.stats.starttime.day, 
                                         hour=tr.stats.starttime.hour, minute=tr.stats.starttime.minute, 
                                         second=tr.stats.starttime.second, microsecond=tr.stats.starttime.microsecond, tzinfo=datetime.timezone.utc)
            if currStarttime &gt; maxStarttime:
                maxStarttime = currStarttime

            currEndtime = datetime.datetime(year=tr.stats.endtime.year, month=tr.stats.endtime.month, day=tr.stats.endtime.day, 
                                         hour=tr.stats.endtime.hour, minute=tr.stats.endtime.minute, 
                                         second=tr.stats.endtime.second, microsecond=tr.stats.endtime.microsecond, tzinfo=datetime.timezone.utc)

            if currEndtime &lt; minEndtime:
                minEndtime = currEndtime


        maxStarttime = obspy.UTCDateTime(maxStarttime)
        minEndtime = obspy.UTCDateTime(minEndtime)
        dataIN = dataIN.split()
        for tr in dataIN:
            tr.trim(starttime=maxStarttime, endtime=minEndtime)
            pass
        dataIN.merge()
    
    params[&#39;batch&#39;] = False #Set False by default, will get corrected later in batch mode        
    params[&#39;input_stream&#39;] = dataIN
    params[&#39;stream&#39;] = dataIN.copy()
    params[&#39;ProcessingStatus&#39;][&#39;FetchDataStatus&#39;] = True
    if verbose and not isinstance(params, HVSRBatch):
        dataINStr = dataIN.__str__().split(&#39;\n&#39;)
        for line in dataINStr:
            print(&#39;\t&#39;,line)
    
    params = _check_processing_status(params)

    return params</code></pre>
</details>
</dd>
<dt id="sprit.sprit_hvsr.generate_ppsds"><code class="name flex">
<span>def <span class="ident">generate_ppsds</span></span>(<span>params, remove_outliers=True, outlier_std=3, verbose=False, **ppsd_kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Generates PPSDs for each channel</p>
<p>Channels need to be in Z, N, E order
Info on PPSD creation here: <a href="https://docs.obspy.org/packages/autogen/obspy.signal.spectral_estimation.PPSD.html">https://docs.obspy.org/packages/autogen/obspy.signal.spectral_estimation.PPSD.html</a></p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>params</code></strong> :&ensp;<code>dict, <a title="sprit.sprit_hvsr.HVSRData" href="#sprit.sprit_hvsr.HVSRData">HVSRData</a> object,</code> or <code><a title="sprit.sprit_hvsr.HVSRBatch" href="#sprit.sprit_hvsr.HVSRBatch">HVSRBatch</a> object</code></dt>
<dd>Data object containing all the parameters and other data of interest (stream and paz, for example)</dd>
<dt><strong><code>remove_outliers</code></strong> :&ensp;<code>bool</code>, default=<code>True</code></dt>
<dd>Whether to remove outlier h/v curves. This is recommended, particularly if remove_noise() has been used.</dd>
<dt><strong><code>outlier_std</code></strong> :&ensp;<code> float</code>, default=<code>3</code></dt>
<dd>The standard deviation value to use as a threshold for determining whether a curve is an outlier.
This averages over the entire curve so that curves with very abberant data (often occurs when using the remove_noise() method), can be identified.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, default=<code>True</code></dt>
<dd>Whether to print inputs and results to terminal</dd>
<dt><strong><code>**ppsd_kwargs</code></strong> :&ensp;<code>dict</code></dt>
<dd>Dictionary with keyword arguments that are passed directly to obspy.signal.PPSD.
If the following keywords are not specified, their defaults are amended in this function from the obspy defaults for its PPSD function. Specifically:
- ppsd_length defaults to 60 (seconds) here instead of 3600
- skip_on_gaps defaults to True instead of False
- period_step_octaves defaults to 0.03125 instead of 0.125</dd>
</dl>
<h2 id="returns">Returns</h2>
<pre><code>ppsds : HVSRData object
    Dictionary containing entries with ppsds for each channel
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_ppsds(params, remove_outliers=True, outlier_std=3, verbose=False, **ppsd_kwargs):
    &#34;&#34;&#34;Generates PPSDs for each channel

        Channels need to be in Z, N, E order
        Info on PPSD creation here: https://docs.obspy.org/packages/autogen/obspy.signal.spectral_estimation.PPSD.html
        
        Parameters
        ----------
        params : dict, HVSRData object, or HVSRBatch object
            Data object containing all the parameters and other data of interest (stream and paz, for example)
        remove_outliers : bool, default=True
            Whether to remove outlier h/v curves. This is recommended, particularly if remove_noise() has been used.
        outlier_std :  float, default=3
            The standard deviation value to use as a threshold for determining whether a curve is an outlier. 
            This averages over the entire curve so that curves with very abberant data (often occurs when using the remove_noise() method), can be identified.
        verbose : bool, default=True
            Whether to print inputs and results to terminal
        **ppsd_kwargs : dict
            Dictionary with keyword arguments that are passed directly to obspy.signal.PPSD.
            If the following keywords are not specified, their defaults are amended in this function from the obspy defaults for its PPSD function. Specifically:
                - ppsd_length defaults to 60 (seconds) here instead of 3600
                - skip_on_gaps defaults to True instead of False
                - period_step_octaves defaults to 0.03125 instead of 0.125

        Returns
        -------
            ppsds : HVSRData object
                Dictionary containing entries with ppsds for each channel
    &#34;&#34;&#34;
    #First, divide up for batch or not
    orig_args = locals().copy() #Get the initial arguments

    ppsd_kwargs_sprit_defaults = ppsd_kwargs.copy()
    #Set defaults here that are different than obspy defaults
    if &#39;ppsd_length&#39; not in ppsd_kwargs:
        ppsd_kwargs_sprit_defaults[&#39;ppsd_length&#39;] = 30.0
    if &#39;skip_on_gaps&#39; not in ppsd_kwargs:
        ppsd_kwargs_sprit_defaults[&#39;skip_on_gaps&#39;] = True
    if &#39;period_step_octaves&#39; not in ppsd_kwargs:
        ppsd_kwargs_sprit_defaults[&#39;period_step_octaves&#39;] = 0.03125

    #Get Probablistic power spectral densities (PPSDs)
    #Get default args for function
    def get_default_args(func):
        signature = inspect.signature(func)
        return {
            k: v.default
            for k, v in signature.parameters.items()
            if v.default is not inspect.Parameter.empty
            }
    
    ppsd_kwargs = get_default_args(PPSD)
    ppsd_kwargs.update(ppsd_kwargs_sprit_defaults)#Update with sprit defaults, or user input

    orig_args[&#39;ppsd_kwargs&#39;] = [ppsd_kwargs]

    if (verbose and isinstance(params, HVSRBatch)) or (verbose and not params[&#39;batch&#39;]):
        if isinstance(params, HVSRData) and params[&#39;batch&#39;]:
            pass
        else:
            print(&#39;\nGenerating Probabilistic Power Spectral Densities (generate_ppsds())&#39;)
            print(&#39;\tUsing the following parameters:&#39;)
            for key, value in orig_args.items():
                if key==&#39;params&#39;:
                    pass
                else:
                    print(&#39;\t  {}={}&#39;.format(key, value))
            print()
    
    #Site is in the keys anytime it&#39;s not batch
    if isinstance(params, HVSRBatch):
        #If running batch, we&#39;ll loop through each one
        for site_name in params.keys():
            args = orig_args.copy() #Make a copy so we don&#39;t accidentally overwrite
            individual_params = params[site_name] #Get what would normally be the &#34;params&#34; variable for each site
            args[&#39;params&#39;] = individual_params #reset the params parameter we originally read in to an individual site params
            #args[&#39;params&#39;][&#39;batch&#39;] = False #Set to false, since only running this time
            if params[site_name][&#39;ProcessingStatus&#39;][&#39;OverallStatus&#39;]:
                try:
                    params[site_name] = _generate_ppsds_batch(**args) #Call another function, that lets us run this function again
                except:
                    params[site_name][&#39;ProcessingStatus&#39;][&#39;PPSDStatus&#39;]=False
                    params[site_name][&#39;ProcessingStatus&#39;][&#39;OverallStatus&#39;] = False                     
            else:
                params[site_name][&#39;ProcessingStatus&#39;][&#39;PPSDStatus&#39;]=False
                params[site_name][&#39;ProcessingStatus&#39;][&#39;OverallStatus&#39;] = False                
        return params
    else:
        paz=params[&#39;paz&#39;]
        stream = params[&#39;stream&#39;]

        #Get ppsds of e component
        eStream = stream.select(component=&#39;E&#39;)
        estats = eStream.traces[0].stats
        ppsdE = PPSD(estats, paz[&#39;E&#39;],  **ppsd_kwargs)
        ppsdE.add(eStream)

        #Get ppsds of n component
        nStream = stream.select(component=&#39;N&#39;)
        nstats = nStream.traces[0].stats
        ppsdN = PPSD(nstats, paz[&#39;N&#39;], **ppsd_kwargs)
        ppsdN.add(nStream)

        #Get ppsds of z component
        zStream = stream.select(component=&#39;Z&#39;)
        zstats = zStream.traces[0].stats
        ppsdZ = PPSD(zstats, paz[&#39;Z&#39;], **ppsd_kwargs)
        ppsdZ.add(zStream)

        ppsds = {&#39;Z&#39;:ppsdZ, &#39;N&#39;:ppsdN, &#39;E&#39;:ppsdE}

        #Add to the input dictionary, so that some items can be manipulated later on, and original can be saved
        params[&#39;ppsds_obspy&#39;] = ppsds
        params[&#39;ppsds&#39;] = {}
        anyKey = list(params[&#39;ppsds_obspy&#39;].keys())[0]
        
        #Get ppsd class members
        members = [mems for mems in dir(params[&#39;ppsds_obspy&#39;][anyKey]) if not callable(mems) and not mems.startswith(&#34;_&#34;)]
        params[&#39;ppsds&#39;][&#39;Z&#39;] = {}
        params[&#39;ppsds&#39;][&#39;E&#39;] = {}
        params[&#39;ppsds&#39;][&#39;N&#39;] = {}
        
        #Get lists/arrays so we can manipulate data later and copy everything over to main &#39;ppsds&#39; subdictionary (convert lists to np.arrays for consistency)
        listList = [&#39;times_data&#39;, &#39;times_gaps&#39;, &#39;times_processed&#39;,&#39;current_times_used&#39;, &#39;psd_values&#39;] #Things that need to be converted to np.array first, for consistency
        timeKeys= [&#39;times_processed&#39;,&#39;current_times_used&#39;,&#39;psd_values&#39;]
        timeDiffWarn = True
        dfList = []
        time_data = {}
        time_dict = {}
        for m in members:
            params[&#39;ppsds&#39;][&#39;Z&#39;][m] = getattr(params[&#39;ppsds_obspy&#39;][&#39;Z&#39;], m)
            params[&#39;ppsds&#39;][&#39;E&#39;][m] = getattr(params[&#39;ppsds_obspy&#39;][&#39;E&#39;], m)
            params[&#39;ppsds&#39;][&#39;N&#39;][m] = getattr(params[&#39;ppsds_obspy&#39;][&#39;N&#39;], m)
            if m in listList:
               
                params[&#39;ppsds&#39;][&#39;Z&#39;][m] = np.array(params[&#39;ppsds&#39;][&#39;Z&#39;][m])
                params[&#39;ppsds&#39;][&#39;E&#39;][m] = np.array(params[&#39;ppsds&#39;][&#39;E&#39;][m])
                params[&#39;ppsds&#39;][&#39;N&#39;][m] = np.array(params[&#39;ppsds&#39;][&#39;N&#39;][m])
            
            if str(m)==&#39;times_processed&#39;:
                unique_times = np.unique(np.array([params[&#39;ppsds&#39;][&#39;Z&#39;][m],
                                          params[&#39;ppsds&#39;][&#39;E&#39;][m],
                                          params[&#39;ppsds&#39;][&#39;N&#39;][m]]))
                
                common_times = []
                for currTime in unique_times:
                    if currTime in params[&#39;ppsds&#39;][&#39;Z&#39;][m]:
                        if currTime in params[&#39;ppsds&#39;][&#39;E&#39;][m]:
                            if currTime in params[&#39;ppsds&#39;][&#39;N&#39;][m]:
                                common_times.append(currTime)

                cTimeIndList = []
                for cTime in common_times:
                    ZArr = params[&#39;ppsds&#39;][&#39;Z&#39;][m]
                    EArr = params[&#39;ppsds&#39;][&#39;E&#39;][m]
                    NArr = params[&#39;ppsds&#39;][&#39;N&#39;][m]

                    cTimeIndList.append([int(np.where(ZArr == cTime)[0][0]),
                                        int(np.where(EArr == cTime)[0][0]),
                                        int(np.where(NArr == cTime)[0][0])])
                    
            #Make sure number of time windows is the same between PPSDs (this can happen with just a few slightly different number of samples)
            if m in timeKeys:
                if str(m) != &#39;times_processed&#39;:
                    time_data[str(m)] = (params[&#39;ppsds&#39;][&#39;Z&#39;][m], params[&#39;ppsds&#39;][&#39;E&#39;][m], params[&#39;ppsds&#39;][&#39;N&#39;][m])

                #print(m, params[&#39;ppsds&#39;][&#39;Z&#39;][m])

                tSteps_same = params[&#39;ppsds&#39;][&#39;Z&#39;][m].shape[0] == params[&#39;ppsds&#39;][&#39;E&#39;][m].shape[0] == params[&#39;ppsds&#39;][&#39;N&#39;][m].shape[0]

                if not tSteps_same:
                    shortestTimeLength = min(params[&#39;ppsds&#39;][&#39;Z&#39;][m].shape[0], params[&#39;ppsds&#39;][&#39;E&#39;][m].shape[0], params[&#39;ppsds&#39;][&#39;N&#39;][m].shape[0])

                    maxPctDiff = 0
                    for comp in params[&#39;ppsds&#39;].keys():
                        currCompTimeLength = params[&#39;ppsds&#39;][comp][m].shape[0]
                        timeLengthDiff = currCompTimeLength - shortestTimeLength
                        percentageDiff = timeLengthDiff / currCompTimeLength
                        if percentageDiff &gt; maxPctDiff:
                            maxPctDiff = percentageDiff

                    for comp in params[&#39;ppsds&#39;].keys():
                        while params[&#39;ppsds&#39;][comp][m].shape[0] &gt; shortestTimeLength:
                            params[&#39;ppsds&#39;][comp][m] = params[&#39;ppsds&#39;][comp][m][:-1]
                    
                    
                    if maxPctDiff &gt; 0.05 and timeDiffWarn:
                        warnings.warn(f&#34;\t  Number of ppsd time windows between different components is significantly different: {round(maxPctDiff*100,2)}% &gt; 5%. Last windows will be trimmed.&#34;)
                    elif verbose  and timeDiffWarn:
                        print(f&#34;\t  Number of ppsd time windows between different components is different by {round(maxPctDiff*100,2)}%. Last window(s) of components with larger number of ppsd windows will be trimmed.&#34;)
                    timeDiffWarn = False #So we only do this warning once, even though there are multiple arrays that need to be trimmed

        for i, currTStep in enumerate(cTimeIndList):
            colList = []
            currTStepList = []
            colList.append(&#39;TimesProcessed_Obspy&#39;)
            currTStepList.append(common_times[i])
            for tk in time_data.keys():
                colList.append(str(tk)+&#39;_Z&#39;)
                colList.append(str(tk)+&#39;_E&#39;)
                colList.append(str(tk)+&#39;_N&#39;)
                currTStepList.append(time_data[tk][0][currTStep[0]])#z
                currTStepList.append(time_data[tk][1][currTStep[1]])#e
                currTStepList.append(time_data[tk][2][currTStep[2]])#n

            dfList.append(currTStepList)
            
        hvsrDF = pd.DataFrame(dfList, columns=colList)
        hvsrDF[&#39;TimesProcessed_ObspyEnd&#39;] = hvsrDF[&#39;TimesProcessed_Obspy&#39;] + ppsd_kwargs[&#39;ppsd_length&#39;]
        
        #Add other times (for start times)
        def convert_to_datetime(obspyUTCDateTime):
            return obspyUTCDateTime.datetime.replace(tzinfo=datetime.timezone.utc)

        def convert_to_mpl_dates(obspyUTCDateTime):
            return obspyUTCDateTime.matplotlib_date

        hvsrDF[&#39;TimesProcessed&#39;] = hvsrDF[&#39;TimesProcessed_Obspy&#39;].apply(convert_to_datetime)     
        hvsrDF[&#39;TimesProcessed_End&#39;] = hvsrDF[&#39;TimesProcessed&#39;] + datetime.timedelta(days=0,seconds=ppsd_kwargs[&#39;ppsd_length&#39;]) 
        hvsrDF[&#39;TimesProcessed_MPL&#39;] = hvsrDF[&#39;TimesProcessed_Obspy&#39;].apply(convert_to_mpl_dates)
        hvsrDF[&#39;TimesProcessed_MPLEnd&#39;] = hvsrDF[&#39;TimesProcessed_MPL&#39;] + (ppsd_kwargs[&#39;ppsd_length&#39;]/86400)
        
        hvsrDF[&#39;Use&#39;] = True
        for gap in params[&#39;ppsds&#39;][&#39;Z&#39;][&#39;times_gaps&#39;]:
            hvsrDF[&#39;Use&#39;] = (hvsrDF[&#39;TimesProcessed_Obspy&#39;].gt(gap[0]) &amp; hvsrDF[&#39;TimesProcessed_Obspy&#39;].gt(gap[1]) )| \
                                (hvsrDF[&#39;TimesProcessed_ObspyEnd&#39;].lt(gap[0]) &amp; hvsrDF[&#39;TimesProcessed_ObspyEnd&#39;].lt(gap[1]))# | \

        hvsrDF.set_index(&#39;TimesProcessed&#39;, inplace=True)
        params[&#39;hvsr_df&#39;] = hvsrDF
        #Create dict entry to keep track of how many outlier hvsr curves are removed (2-item list with [0]=current number, [1]=original number of curves)
        params[&#39;tsteps_used&#39;] = [hvsrDF[&#39;Use&#39;].sum(), hvsrDF[&#39;Use&#39;].shape[0]]
        #params[&#39;tsteps_used&#39;] = [params[&#39;ppsds&#39;][&#39;Z&#39;][&#39;times_processed&#39;].shape[0], params[&#39;ppsds&#39;][&#39;Z&#39;][&#39;times_processed&#39;].shape[0]]
        
        #Remove outlier ppsds (those derived from data within the windows to be removed)

        if remove_outliers and &#39;xwindows_out&#39; in params.keys():
            params = remove_outlier_curves(params, outlier_std=outlier_std, ppsd_length=ppsd_kwargs[&#39;ppsd_length&#39;])
        params[&#39;tsteps_used&#39;][0] = params[&#39;ppsds&#39;][&#39;Z&#39;][&#39;current_times_used&#39;].shape[0]
        
        params = sprit_utils.make_it_classy(params)
    
    params[&#39;ProcessingStatus&#39;][&#39;PPSDStatus&#39;] = True
    params = _check_processing_status(params)
    return params</code></pre>
</details>
</dd>
<dt id="sprit.sprit_hvsr.get_metadata"><code class="name flex">
<span>def <span class="ident">get_metadata</span></span>(<span>params, write_path='', update_metadata=True, source=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Get metadata and calculate or get paz parameter needed for PPSD</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>params</code></strong> :&ensp;<code>dict</code></dt>
<dd>Dictionary containing all the input and other parameters needed for processing
Ouput from input_params() function</dd>
<dt><strong><code>write_path</code></strong> :&ensp;<code>str</code></dt>
<dd>String with output filepath of where to write updated inventory or metadata file
If not specified, does not write file</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>params</code></strong> :&ensp;<code>dict</code></dt>
<dd>Modified input dictionary with additional key:value pair containing paz dictionary (key = "paz")</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_metadata(params, write_path=&#39;&#39;, update_metadata=True, source=None):
    &#34;&#34;&#34;Get metadata and calculate or get paz parameter needed for PPSD

    Parameters
    ----------
    params : dict
        Dictionary containing all the input and other parameters needed for processing
            Ouput from input_params() function
    write_path : str
        String with output filepath of where to write updated inventory or metadata file
            If not specified, does not write file 

    Returns
    -------
    params : dict
        Modified input dictionary with additional key:value pair containing paz dictionary (key = &#34;paz&#34;)
    &#34;&#34;&#34;
    
    invPath = params[&#39;metapath&#39;]
    raspShakeInstNameList = [&#39;raspberry shake&#39;, &#39;shake&#39;, &#39;raspberry&#39;, &#39;rs&#39;, &#39;rs3d&#39;, &#39;rasp. shake&#39;, &#39;raspshake&#39;]
    if params[&#39;instrument&#39;].lower() in raspShakeInstNameList:
        if update_metadata:
            params = _update_shake_metadata(filepath=invPath, params=params, write_path=write_path)
        params = _read_RS_Metadata(params, source=source)
    else:
        warnings.warn(&#39;{} not currently supported\n Returning input params dictionary.&#39;.format(params[&#39;instrument&#39;]))
        return params
    return params</code></pre>
</details>
</dd>
<dt id="sprit.sprit_hvsr.get_report"><code class="name flex">
<span>def <span class="ident">get_report</span></span>(<span>hvsr_results, report_format='print', plot_type='HVSR p ann C+ p ann Spec', export_path=None, return_results=False, csv_overwrite_opt='append', no_output=False, verbose=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Print a report of the HVSR analysis (not currently implemented)</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>hvsr_results</code></strong> :&ensp;<code>dict</code></dt>
<dd>Dictionary containing all the information about the processed hvsr data</dd>
<dt><strong><code>report_format</code></strong> :&ensp;<code>{'csv', 'print', plot}</code></dt>
<dd>Format in which to print or export the report.</dd>
<dt><strong><code>plot_type</code></strong> :&ensp;<code>str</code>, default <code>= 'HVSR p ann C+ p ann Spec</code></dt>
<dd>What type of plot to plot, if 'plot' part of report_format input</dd>
<dt><strong><code>export_path</code></strong> :&ensp;<code>None, bool,</code> or <code>filepath</code>, default <code>= None</code></dt>
<dd>If None or False, does not export; if True, will export to same directory as the datapath parameter in the input_params() function.
Otherwise, it should be a string or path object indicating where to export results. May be a file or directory.
If a directory is specified, the filename will be
"<site_name><em><acq_date></em><UTC start time>-<UTC end time>". The suffix defaults to png for report_format="plot", csv for 'csv', and does not export if 'print.'</dd>
<dt><strong><code>return_results</code></strong> :&ensp;<code>bool</code>, default=<code>False</code></dt>
<dd>Whether to return results. The following report_formats return the following items:
'plot'- str
'print' - matplotlib.Figure object
'csv' - pandas.DataFrame object
list/tuple - a list or tuple of the above objects, in the same order they are in the report_format list</dd>
<dt><strong><code>csv_overwrite_opts</code></strong> :&ensp;<code>str, {'append', 'overwrite', 'keep/rename'}</code></dt>
<dd>How to handle csv report outputs if the designated csv output file already exists. By default, appends the new information to the end of the existing file.</dd>
<dt><strong><code>no_output</code></strong> :&ensp;<code>bool</code>, default=<code>False</code></dt>
<dd>If True, only reads output to appropriate attribute of data class (ie, print does not print, only reads text into variable). If False, performs as normal.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, default=<code>True</code></dt>
<dd>Whether to print the results to terminal. This is the same output as report_format='print', and will not repeat if that is already selected</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>If return_results=True, the following report_formats return the following items:</code></dt>
<dd>'plot'-
matplotlib.Figure object
'print' - str
'csv' - pandas.DataFrame object
list/tuple - a list or tuple of the above objects, in the same order they are in the report_format list</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_report(hvsr_results, report_format=&#39;print&#39;, plot_type=&#39;HVSR p ann C+ p ann Spec&#39;, export_path=None, return_results=False, csv_overwrite_opt=&#39;append&#39;, no_output=False, verbose=False):    
    &#34;&#34;&#34;Print a report of the HVSR analysis (not currently implemented)
        
    Parameters
    ----------
    hvsr_results : dict
        Dictionary containing all the information about the processed hvsr data
    report_format : {&#39;csv&#39;, &#39;print&#39;, plot}
        Format in which to print or export the report.
    plot_type : str, default = &#39;HVSR p ann C+ p ann Spec
        What type of plot to plot, if &#39;plot&#39; part of report_format input
    export_path : None, bool, or filepath, default = None
        If None or False, does not export; if True, will export to same directory as the datapath parameter in the input_params() function.
        Otherwise, it should be a string or path object indicating where to export results. May be a file or directory.
        If a directory is specified, the filename will be  &#34;&lt;site_name&gt;_&lt;acq_date&gt;_&lt;UTC start time&gt;-&lt;UTC end time&gt;&#34;. The suffix defaults to png for report_format=&#34;plot&#34;, csv for &#39;csv&#39;, and does not export if &#39;print.&#39;
    return_results : bool, default=False
        Whether to return results. The following report_formats return the following items:
            &#39;plot&#39;- str
            &#39;print&#39; - matplotlib.Figure object
            &#39;csv&#39; - pandas.DataFrame object
            list/tuple - a list or tuple of the above objects, in the same order they are in the report_format list
    csv_overwrite_opts : str, {&#39;append&#39;, &#39;overwrite&#39;, &#39;keep/rename&#39;}
        How to handle csv report outputs if the designated csv output file already exists. By default, appends the new information to the end of the existing file.
    no_output : bool, default=False
        If True, only reads output to appropriate attribute of data class (ie, print does not print, only reads text into variable). If False, performs as normal.
    verbose : bool, default=True
        Whether to print the results to terminal. This is the same output as report_format=&#39;print&#39;, and will not repeat if that is already selected

    Returns
    -------
    If return_results=True, the following report_formats return the following items:
        &#39;plot&#39;-  matplotlib.Figure object
        &#39;print&#39; - str
        &#39;csv&#39; - pandas.DataFrame object
        list/tuple - a list or tuple of the above objects, in the same order they are in the report_format list

    &#34;&#34;&#34;
    #print statement
    #Check if results are good
    #Curve pass?
    orig_args = locals().copy() #Get the initial arguments

    if (verbose and isinstance(hvsr_results, HVSRBatch)) or (verbose and not hvsr_results[&#39;batch&#39;]):
        if isinstance(hvsr_results, HVSRData) and hvsr_results[&#39;batch&#39;]:
            pass
        else:
            print(&#39;\nGetting HVSR Report: get_report()&#39;)
            print(&#39;\tUsing the following parameters:&#39;)
            for key, value in orig_args.items():
                if key==&#39;params&#39;:
                    pass
                else:
                    print(&#39;\t  {}={}&#39;.format(key, value))
            print()

    if isinstance(hvsr_results, HVSRBatch):
        if verbose:
            print(&#39;\nGetting Reports: Running in batch mode&#39;)

            print(&#39;\tUsing parameters:&#39;)
            for key, value in orig_args.items():
                print(f&#39;\t  {key}={value}&#39;)    
            print()
        #If running batch, we&#39;ll loop through each site
        for site_name in hvsr_results.keys():
            args = orig_args.copy() #Make a copy so we don&#39;t accidentally overwrite
            individual_params = hvsr_results[site_name] #Get what would normally be the &#34;params&#34; variable for each site
            args[&#39;hvsr_results&#39;] = individual_params #reset the params parameter we originally read in to an individual site params
            if hvsr_results[site_name][&#39;ProcessingStatus&#39;][&#39;OverallStatus&#39;]:
                try:
                    hvsr_results[site_name] = _get_report_batch(**args) #Call another function, that lets us run this function again
                except:
                    hvsr_results[site_name] = hvsr_results[site_name]
            else:
                hvsr_results[site_name] = hvsr_results[site_name]
        
        combined_csvReport = pd.DataFrame()
        for site_name in hvsr_results.keys():
            if &#39;CSV_Report&#39; in hvsr_results[site_name].keys():
                combined_csvReport = pd.concat([combined_csvReport, hvsr_results[site_name][&#39;CSV_Report&#39;]], ignore_index=True, join=&#39;inner&#39;)
        
        if export_path is not None:
            if export_path is True:
                if pathlib.Path(hvsr_results[&#39;input_params&#39;][&#39;datapath&#39;]) in sampleFileKeyMap.values():
                    csvExportPath = pathlib.Path(os.getcwd())
                else:
                    csvExportPath = pathlib.Path(hvsr_results[&#39;input_params&#39;][&#39;datapath&#39;])
            elif pathlib.Path(export_path).is_dir():
                csvExportPath = export_path
            elif pathlib.Path(export_path).is_file():
                csvExportPath = export_path.parent
            else:
                csvExportPath = pathlib.Path(hvsr_results[site_name].datapath)
                if csvExportPath.is_dir():
                    pass
                else:
                    csvExportPath = csvExportPath.parent
                
            combined_csvReport.to_csv(csvExportPath, index=False)
        
        if return_results:
            return hvsr_results
    else:
        #if &#39;BestPeak&#39; in hvsr_results.keys() and &#39;PassList&#39; in hvsr_results[&#39;BestPeak&#39;].keys():
        try:
            curvTestsPassed = (hvsr_results[&#39;BestPeak&#39;][&#39;PassList&#39;][&#39;WindowLengthFreq.&#39;] +
                                hvsr_results[&#39;BestPeak&#39;][&#39;PassList&#39;][&#39;SignificantCycles&#39;]+
                                hvsr_results[&#39;BestPeak&#39;][&#39;PassList&#39;][&#39;LowCurveStDevOverTime&#39;])
            curvePass = curvTestsPassed &gt; 2
            
            #Peak Pass?
            peakTestsPassed = ( hvsr_results[&#39;BestPeak&#39;][&#39;PassList&#39;][&#39;PeakFreqClarityBelow&#39;] +
                        hvsr_results[&#39;BestPeak&#39;][&#39;PassList&#39;][&#39;PeakFreqClarityAbove&#39;]+
                        hvsr_results[&#39;BestPeak&#39;][&#39;PassList&#39;][&#39;PeakAmpClarity&#39;]+
                        hvsr_results[&#39;BestPeak&#39;][&#39;PassList&#39;][&#39;FreqStability&#39;]+
                        hvsr_results[&#39;BestPeak&#39;][&#39;PassList&#39;][&#39;PeakStability_FreqStD&#39;]+
                        hvsr_results[&#39;BestPeak&#39;][&#39;PassList&#39;][&#39;PeakStability_AmpStD&#39;])
            peakPass = peakTestsPassed &gt;= 5
        except Exception as e:
            errMsg= &#39;No BestPeak identified. Check peak_freq_range or hvsr_band or try to remove bad noise windows using remove_noise() or change processing parameters in process_hvsr() or generate_ppsds(). Otherwise, data may not be usable for HVSR.&#39;
            print(errMsg)
            print(e)
            return hvsr_results
            #raise RuntimeError(&#39;No BestPeak identified. Check peak_freq_range or hvsr_band or try to remove bad noise windows using remove_noise() or change processing parameters in process_hvsr() or generate_ppsds(). Otherwise, data may not be usable for HVSR.&#39;)
    
        if isinstance(report_format, (list, tuple)):
            pass
        else:
            #We will use a loop later even if it&#39;s just one report type, so reformat to prepare for for loop
            allList = [&#39;:&#39;, &#39;all&#39;]
            if report_format.lower() in allList:
                report_format = [&#39;print&#39;, &#39;csv&#39;, &#39;plot&#39;]
            else:
                report_format = [report_format]   

        def export_report(export_obj, _export_path, _rep_form):
            if _export_path is None:
                return
            else:
                if _rep_form == &#39;csv&#39;:
                    ext = &#39;.csv&#39;
                elif _rep_form ==&#39;plot&#39;:
                    ext=&#39;.png&#39;
                else:
                    ext=&#39;&#39;
                    
                sitename=hvsr_results[&#39;input_params&#39;][&#39;site&#39;]#.replace(&#39;.&#39;, &#39;-&#39;)
                fname = f&#34;{sitename}_{hvsr_results[&#39;input_params&#39;][&#39;acq_date&#39;]}_{str(hvsr_results[&#39;input_params&#39;][&#39;starttime&#39;].time)[:5]}-{str(hvsr_results[&#39;input_params&#39;][&#39;endtime&#39;].time)[:5]}{ext}&#34;
                fname = fname.replace(&#39;:&#39;, &#39;&#39;)

                if _export_path==True:
                    #Check so we don&#39;t write in sample directory
                    if pathlib.Path(hvsr_results[&#39;input_params&#39;][&#39;datapath&#39;]) in sampleFileKeyMap.values():
                        if pathlib.Path(os.getcwd()) in sampleFileKeyMap.values(): #Just in case current working directory is also sample directory
                            inFile = pathlib.Path.home() #Use the path to user&#39;s home if all else fails
                        else:
                            inFile = pathlib.Path(os.getcwd())
                    else:
                        inFile = pathlib.Path(hvsr_results[&#39;input_params&#39;][&#39;datapath&#39;])
                                 
                    if inFile.is_dir():
                        outFile = inFile.joinpath(fname)
                    else:
                        outFile = inFile.with_name(fname)
                else:
                    if pathlib.Path(_export_path).is_dir():
                        outFile = pathlib.Path(_export_path).joinpath(fname)
                    else:
                        outFile=pathlib.Path(_export_path)

            if _rep_form == &#39;csv&#39;:
                if outFile.exists():
                    existFile = pd.read_csv(outFile)
                    if csv_overwrite_opt.lower() == &#39;append&#39;:
                        export_obj = pd.concat([existFile, export_obj], ignore_index=True, join=&#39;inner&#39;)
                    elif csv_overwrite_opt.lower() == &#39;overwrite&#39;:
                        pass
                    else:# csv_overwrite_opt.lower() in [&#39;keep&#39;, &#39;rename&#39;]:
                        fileNameExists = True
                        i=1
                        while fileNameExists:
                            outFile = outFile.with_stem(f&#34;{outFile.stem}_{i}&#34;)
                            i+=1
                            if not outFile.exists():
                                fileNameExists = False
                try:
                    print(f&#39;\nSaving csv data to: {outFile}&#39;)
                    export_obj.to_csv(outFile, index_label=&#39;ID&#39;)
                except:
                    warnings.warn(&#34;Report not exported. \n\tDataframe to be exported as csv has been saved in hvsr_results[&#39;BestPeak&#39;][&#39;Report&#39;][&#39;CSV_Report]&#34;, category=RuntimeWarning)
            elif _rep_form ==&#39;plot&#39;:
                if verbose:
                    print(f&#39;\nSaving plot to: {outFile}&#39;)
                plt.scf = export_obj
                plt.savefig(outFile)
            return 

        def report_output(_report_format, _plot_type=&#39;HVSR p ann C+ p ann Spec&#39;, _return_results=False, _export_path=None, _no_output=False, verbose=False):
            if _report_format==&#39;print&#39;:
                #Print results

                #Make separators for nicely formatted print output
                sepLen = 99
                siteSepSymbol = &#39;=&#39;
                intSepSymbol = u&#34;\u2013&#34;
                extSepSymbol = u&#34;\u2014&#34;
                
                if sepLen % 2 == 0:
                    remainVal = 1
                else:
                    remainVal = 0

                siteWhitespace = 2
                #Format the separator lines internal to each site
                internalSeparator = intSepSymbol.center(sepLen-4, intSepSymbol).center(sepLen, &#39; &#39;)

                extSiteSeparator = &#34;&#34;.center(sepLen, extSepSymbol)
                siteSeparator = f&#34;{hvsr_results[&#39;input_params&#39;][&#39;site&#39;]}&#34;.center(sepLen - siteWhitespace, &#39; &#39;).center(sepLen, siteSepSymbol)
                endSiteSeparator = &#34;&#34;.center(sepLen, siteSepSymbol)

                #Start building list to print
                report_string_list = []
                report_string_list.append(&#34;&#34;) #Blank line to start
                report_string_list.append(extSiteSeparator)
                report_string_list.append(siteSeparator)
                report_string_list.append(extSiteSeparator)
                #report_string_list.append(internalSeparator)
                report_string_list.append(&#39;&#39;)
                report_string_list.append(f&#34;\tSite Name: {hvsr_results[&#39;input_params&#39;][&#39;site&#39;]}&#34;)
                report_string_list.append(f&#34;\tAcq. Date: {hvsr_results[&#39;input_params&#39;][&#39;acq_date&#39;]}&#34;)
                report_string_list.append(f&#34;\tLocation : {hvsr_results[&#39;input_params&#39;][&#39;longitude&#39;]}, {hvsr_results[&#39;input_params&#39;][&#39;latitude&#39;]}&#34;)
                report_string_list.append(f&#34;\tElevation: {hvsr_results[&#39;input_params&#39;][&#39;elevation&#39;]}&#34;)
                report_string_list.append(&#39;&#39;)
                report_string_list.append(internalSeparator)
                report_string_list.append(&#39;&#39;)
                if &#39;BestPeak&#39; not in hvsr_results.keys():
                    report_string_list.append(&#39;\tNo identifiable BestPeak was present between {} for {}&#39;.format(hvsr_results[&#39;input_params&#39;][&#39;hvsr_band&#39;], hvsr_results[&#39;input_params&#39;][&#39;site&#39;]))
                else:
                    report_string_list.append(&#39;\t{0:.3f} Hz Peak Frequency&#39;.format(hvsr_results[&#39;BestPeak&#39;][&#39;f0&#39;]))        
                    if curvePass and peakPass:
                        report_string_list.append(&#39;\t  {} Curve at {} Hz passed quality checks! â˜º :D&#39;.format(sprit_utils.check_mark(), round(hvsr_results[&#39;BestPeak&#39;][&#39;f0&#39;],3)))
                    else:
                        report_string_list.append(&#39;\t  {} Peak at {} Hz did NOT pass quality checks â˜¹:(&#39;.format(sprit_utils.x_mark(), round(hvsr_results[&#39;BestPeak&#39;][&#39;f0&#39;],3)))            
                    report_string_list.append(&#39;&#39;)
                    report_string_list.append(internalSeparator)
                    report_string_list.append(&#39;&#39;)

                    #Print individual results
                    report_string_list.append(&#39;\tCurve Tests: {}/3 passed (3/3 needed)&#39;.format(curvTestsPassed))
                    report_string_list.append(f&#34;\t\t {hvsr_results[&#39;BestPeak&#39;][&#39;Report&#39;][&#39;Lw&#39;][-1]} Length of processing windows: {hvsr_results[&#39;BestPeak&#39;][&#39;Report&#39;][&#39;Lw&#39;]}&#34;)
                    report_string_list.append(f&#34;\t\t {hvsr_results[&#39;BestPeak&#39;][&#39;Report&#39;][&#39;Nc&#39;][-1]} Number of significant cycles: {hvsr_results[&#39;BestPeak&#39;][&#39;Report&#39;][&#39;Nc&#39;]}&#34;)
                    report_string_list.append(f&#34;\t\t {hvsr_results[&#39;BestPeak&#39;][&#39;Report&#39;][&#39;Ïƒ_A(f)&#39;][-1]} Low StDev. of H/V Curve over time: {hvsr_results[&#39;BestPeak&#39;][&#39;Report&#39;][&#39;Ïƒ_A(f)&#39;]}&#34;)

                    report_string_list.append(&#39;&#39;)
                    report_string_list.append(&#34;\tPeak Tests: {}/6 passed (5/6 needed)&#34;.format(peakTestsPassed))
                    report_string_list.append(f&#34;\t\t {hvsr_results[&#39;BestPeak&#39;][&#39;Report&#39;][&#39;A(f-)&#39;][-1]} Clarity Below Peak Frequency: {hvsr_results[&#39;BestPeak&#39;][&#39;Report&#39;][&#39;A(f-)&#39;]}&#34;)
                    report_string_list.append(f&#34;\t\t {hvsr_results[&#39;BestPeak&#39;][&#39;Report&#39;][&#39;A(f+)&#39;][-1]} Clarity Above Peak Frequency: {hvsr_results[&#39;BestPeak&#39;][&#39;Report&#39;][&#39;A(f+)&#39;]}&#34;)
                    report_string_list.append(f&#34;\t\t {hvsr_results[&#39;BestPeak&#39;][&#39;Report&#39;][&#39;A0&#39;][-1]} Clarity of Peak Amplitude: {hvsr_results[&#39;BestPeak&#39;][&#39;Report&#39;][&#39;A0&#39;]}&#34;)
                    if hvsr_results[&#39;BestPeak&#39;][&#39;PassList&#39;][&#39;FreqStability&#39;]:
                        res = sprit_utils.check_mark()
                    else:
                        res = sprit_utils.x_mark()
                    report_string_list.append(f&#34;\t\t {res} Stability of Peak Freq. Over time: {hvsr_results[&#39;BestPeak&#39;][&#39;Report&#39;][&#39;P-&#39;][:5]} and {hvsr_results[&#39;BestPeak&#39;][&#39;Report&#39;][&#39;P+&#39;][:-1]} {res}&#34;)
                    report_string_list.append(f&#34;\t\t {hvsr_results[&#39;BestPeak&#39;][&#39;Report&#39;][&#39;Sf&#39;][-1]} Stability of Peak (Freq. StDev): {hvsr_results[&#39;BestPeak&#39;][&#39;Report&#39;][&#39;Sf&#39;]}&#34;)
                    report_string_list.append(f&#34;\t\t {hvsr_results[&#39;BestPeak&#39;][&#39;Report&#39;][&#39;Sa&#39;][-1]} Stability of Peak (Amp. StDev): {hvsr_results[&#39;BestPeak&#39;][&#39;Report&#39;][&#39;Sa&#39;]}&#34;)
                report_string_list.append(&#39;&#39;)
                report_string_list.append(f&#34;Calculated using {hvsr_results[&#39;hvsr_df&#39;][&#39;Use&#39;].sum()}/{hvsr_results[&#39;hvsr_df&#39;][&#39;Use&#39;].count()} time windows&#34;.rjust(sepLen-1))
                report_string_list.append(extSiteSeparator)
                #report_string_list.append(endSiteSeparator)
                #report_string_list.append(extSiteSeparator)
                report_string_list.append(&#39;&#39;)
                
                reportStr=&#39;&#39;
                #Now print it
                for line in report_string_list:
                    reportStr = reportStr+&#39;\n&#39;+line

                if not _no_output:
                    print(reportStr)

                export_report(export_obj=reportStr, _export_path=_export_path, _rep_form=_report_format)
                hvsr_results[&#39;BestPeak&#39;][&#39;Report&#39;][&#39;Print_Report&#39;] = reportStr
                hvsr_results[&#39;Print_Report&#39;] = reportStr

            elif _report_format==&#39;csv&#39;:
                import pandas as pd
                pdCols = [&#39;Site Name&#39;, &#39;Acq_Date&#39;, &#39;Longitude&#39;, &#39;Latitide&#39;, &#39;Elevation&#39;, &#39;PeakFrequency&#39;, 
                        &#39;WindowLengthFreq.&#39;,&#39;SignificantCycles&#39;,&#39;LowCurveStDevOverTime&#39;,
                        &#39;PeakFreqClarityBelow&#39;,&#39;PeakFreqClarityAbove&#39;,&#39;PeakAmpClarity&#39;,&#39;FreqStability&#39;, &#39;PeakStability_FreqStD&#39;,&#39;PeakStability_AmpStD&#39;, &#39;PeakPasses&#39;]
                d = hvsr_results
                criteriaList = []
                for p in hvsr_results[&#39;BestPeak&#39;][&#34;PassList&#34;]:
                    criteriaList.append(hvsr_results[&#39;BestPeak&#39;][&#34;PassList&#34;][p])
                criteriaList.append(hvsr_results[&#39;BestPeak&#39;][&#34;PeakPasses&#34;])
                dfList = [[d[&#39;input_params&#39;][&#39;site&#39;], d[&#39;input_params&#39;][&#39;acq_date&#39;], d[&#39;input_params&#39;][&#39;longitude&#39;], d[&#39;input_params&#39;][&#39;latitude&#39;], d[&#39;input_params&#39;][&#39;elevation&#39;], round(d[&#39;BestPeak&#39;][&#39;f0&#39;], 3)]]
                dfList[0].extend(criteriaList)
                outDF = pd.DataFrame(dfList, columns=pdCols)

                if verbose:
                    print(&#39;\nCSV Report:\n&#39;)
                    maxColWidth = 13
                    print(&#39;  &#39;, end=&#39;&#39;)
                    for col in outDF.columns:
                        if len(str(col)) &gt; maxColWidth:
                            colStr = str(col)[:maxColWidth-3]+&#39;...&#39;
                        else:
                            colStr = str(col)
                        print(colStr.ljust(maxColWidth), end=&#39;  &#39;)
                    print() #new line
                    for c in range(len(outDF.columns) * (maxColWidth+2)):
                        if c % (maxColWidth+2) == 0:
                            print(&#39;|&#39;, end=&#39;&#39;)
                        else:
                            print(&#39;-&#39;, end=&#39;&#39;)
                    print(&#39;|&#39;) #new line
                    print(&#39;  &#39;, end=&#39;&#39;) #Small indent at start                    
                    for row in outDF.iterrows():
                        for col in row[1]:
                            if len(str(col)) &gt; maxColWidth:
                                colStr = str(col)[:maxColWidth-3]+&#39;...&#39;
                            else:
                                colStr = str(col)
                            print(colStr.ljust(maxColWidth), end=&#39;  &#39;)
                        print()

                try:
                    export_report(export_obj=outDF, _export_path=_export_path, _rep_form=_report_format)
                except:
                    print(&#34;Error in exporting csv report. CSV not exported&#34;)
                hvsr_results[&#39;BestPeak&#39;][&#39;Report&#39;][&#39;CSV_Report&#39;] = outDF
                hvsr_results[&#39;CSV_Report&#39;] = outDF
                        
            elif _report_format==&#39;plot&#39;:
                fig_ax = plot_hvsr(hvsr_results, plot_type=_plot_type, show=False, return_fig=True)

                export_report(export_obj=fig_ax[0], _export_path=_export_path, _rep_form=_report_format)
                hvsr_results[&#39;BestPeak&#39;][&#39;Report&#39;][&#39;HV_Plot&#39;]=hvsr_results[&#39;HV_Plot&#39;]=fig_ax

                print(&#39;\nPlot of data report:&#39;)
                plt.show()
                
            return hvsr_results

        for i, rep_form in enumerate(report_format):
            if isinstance(export_path, (list, tuple)):
                if not isinstance(report_format, (list, tuple)):
                    warnings.warn(&#39;export_path is a list/tuple and report_format is not. This may result in unexpected behavior.&#39;)
                if isinstance(report_format, (list, tuple)) and isinstance(export_path, (list, tuple)) and len(report_format) != len(export_path):
                    warnings.warn(&#39;export_path and report_format are both lists or tuples, but they are not the same length. This may result in unexpected behavior.&#39;)
            
                exp_path = export_path[i]
            else:
                exp_path = export_path
            hvsr_results = report_output(_report_format=rep_form, _plot_type=plot_type, _return_results=return_results, _export_path=exp_path, _no_output=no_output, verbose=verbose)
    return hvsr_results</code></pre>
</details>
</dd>
<dt id="sprit.sprit_hvsr.gui"><code class="name flex">
<span>def <span class="ident">gui</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to open a window with a graphical user interface (gui)</p>
<p>No parameters, no returns; just opens the gui window.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def gui():
    &#34;&#34;&#34;Function to open a window with a graphical user interface (gui)
    
    No parameters, no returns; just opens the gui window.
    &#34;&#34;&#34;
    import pkg_resources
    #guiPath = pathlib.Path(os.path.realpath(__file__))
    try:
        from sprit.sprit_gui import SPRIT_App
    except:
        from sprit_gui import SPRIT_App
    
    try:
        import tkinter as tk
    except:
        if sys.platform == &#39;linux&#39;:
            raise ImportError(&#39;The SpRIT graphical interface uses tkinter, which ships with python but is not pre-installed on linux machines. Use &#34;apt-get install python-tk&#34; or &#34;apt-get install python3-tk&#34; to install tkinter. You may need to use the sudo command at the start of those commands.&#39;)

    def on_gui_closing():
        plt.close(&#39;all&#39;)
        gui_root.quit()
        gui_root.destroy()

    if sys.platform == &#39;linux&#39;:
        if not pathlib.Path(&#34;/usr/share/doc/python3-tk&#34;).exists():
            warnings.warn(&#39;The SpRIT graphical interface uses tkinter, which ships with python but is not pre-installed on linux machines. Use &#34;apt-get install python-tk&#34; or &#34;apt-get install python3-tk&#34; to install tkinter. You may need to use the sudo command at the start of those commands.&#39;)

    gui_root = tk.Tk()
    try:
        try:
            icon_path =pathlib.Path(pkg_resources.resource_filename(__name__, &#39;resources/icon/sprit_icon_alpha.ico&#39;)) 
            gui_root.iconbitmap(icon_path)
        except:
            icon_path = pathlib.Path(pkg_resources.resource_filename(__name__, &#39;resources/icon/sprit_icon.png&#39;))
            gui_root.iconphoto(False, tk.PhotoImage(file=icon_path.as_posix()))
    except Exception as e:
        print(&#34;ICON NOT LOADED, still opening GUI&#34;)

    gui_root.resizable(True, True)
    SPRIT_App(master=gui_root) #Open the app with a tk.Tk root

    gui_root.protocol(&#34;WM_DELETE_WINDOW&#34;, on_gui_closing)    
    gui_root.mainloop() #Run the main loop</code></pre>
</details>
</dd>
<dt id="sprit.sprit_hvsr.import_data"><code class="name flex">
<span>def <span class="ident">import_data</span></span>(<span>import_filepath, data_format='pickle')</span>
</code></dt>
<dd>
<div class="desc"><p>Function to import .hvsr (or other extension) data exported using export_data() function</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>import_filepath</code></strong> :&ensp;<code>str</code> or <code>path object</code></dt>
<dd>Filepath of file created using export_data() function. This is usually a pickle file with a .hvsr extension</dd>
<dt><strong><code>data_format</code></strong> :&ensp;<code>str</code>, default=<code>'pickle'</code></dt>
<dd>Type of format data is in. Currently, only 'pickle' supported. Eventually, json or other type may be supported, by default 'pickle'.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="sprit.sprit_hvsr.HVSRData" href="#sprit.sprit_hvsr.HVSRData">HVSRData</a></code> or <code><a title="sprit.sprit_hvsr.HVSRBatch" href="#sprit.sprit_hvsr.HVSRBatch">HVSRBatch</a> object</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def import_data(import_filepath, data_format=&#39;pickle&#39;):
    &#34;&#34;&#34;Function to import .hvsr (or other extension) data exported using export_data() function

    Parameters
    ----------
    import_filepath : str or path object
        Filepath of file created using export_data() function. This is usually a pickle file with a .hvsr extension
    data_format : str, default=&#39;pickle&#39;
        Type of format data is in. Currently, only &#39;pickle&#39; supported. Eventually, json or other type may be supported, by default &#39;pickle&#39;.

    Returns
    -------
    HVSRData or HVSRBatch object
    &#34;&#34;&#34;
    if data_format==&#39;pickle&#39;:
        with open(import_filepath, &#39;rb&#39;) as f:
            dataIN = pickle.load(f)
    else:
        dataIN = import_filepath
    return dataIN</code></pre>
</details>
</dd>
<dt id="sprit.sprit_hvsr.input_params"><code class="name flex">
<span>def <span class="ident">input_params</span></span>(<span>datapath, site='HVSR Site', network='AM', station='RAC84', loc='00', channels=['EHZ', 'EHN', 'EHE'], acq_date='2023-10-11', starttime='00:00:00.00', endtime='23:59:59.999999', tzone='UTC', xcoord=-88.2290526, ycoord=40.1012122, elevation=755, input_crs='EPSG:4326', output_crs='EPSG:4326', elev_unit='feet', depth=0, instrument='Raspberry Shake', metapath='', hvsr_band=[0.4, 40], peak_freq_range=[0.4, 40], verbose=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Function for designating input parameters for reading in and processing data</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>datapath</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path object</code></dt>
<dd>Filepath of data. This can be a directory or file, but will need to match with what is chosen later as the source parameter in fetch_data()</dd>
<dt><strong><code>site</code></strong> :&ensp;<code>str</code>, default=<code>"HVSR Site"</code></dt>
<dd>Site name as designated by user for ease of reference. Used for plotting titles, filenames, etc.</dd>
<dt><strong><code>network</code></strong> :&ensp;<code>str</code>, default=<code>'AM'</code></dt>
<dd>The network designation of the seismometer. This is necessary for data from Raspberry Shakes. 'AM' is for Amateur network, which fits Raspberry Shakes.</dd>
<dt><strong><code>station</code></strong> :&ensp;<code>str</code>, default=<code>'RAC84'</code></dt>
<dd>The station name of the seismometer. This is necessary for data from Raspberry Shakes.</dd>
<dt><strong><code>loc</code></strong> :&ensp;<code>str</code>, default=<code>'00'</code></dt>
<dd>Location information of the seismometer.</dd>
<dt><strong><code>channels</code></strong> :&ensp;<code>list</code>, default=<code>['EHZ', 'EHN', 'EHE']</code></dt>
<dd>The three channels used in this analysis, as a list of strings. Preferred that Z component is first, but not necessary</dd>
<dt><strong><code>acq_date</code></strong> :&ensp;<code>str, int, date object,</code> or <code>datetime object</code></dt>
<dd>If string, preferred format is 'YYYY-MM-DD'.
If int, this will be interpreted as the time_int of year of current year (e.g., 33 would be Feb 2 of current year)
If date or datetime object, this will be the date. Make sure to account for time change when converting to UTC (if UTC is the following time_int, use the UTC time_int).</dd>
<dt><strong><code>starttime</code></strong> :&ensp;<code>str, time object,</code> or <code>datetime object</code>, default=<code>'00:00:00.00'</code></dt>
<dd>Start time of data stream. This is necessary for Raspberry Shake data in 'raw' form, or for trimming data. Format can be either 'HH:MM:SS.micros' or 'HH:MM' at minimum.</dd>
<dt><strong><code>endtime</code></strong> :&ensp;<code>str, time obejct,</code> or <code>datetime object</code>, default=<code>'23:59:99.99'</code></dt>
<dd>End time of data stream. This is necessary for Raspberry Shake data in 'raw' form, or for trimming data. Same format as starttime.</dd>
<dt><strong><code>tzone</code></strong> :&ensp;<code>str</code> or <code>int</code>, default <code>= 'UTC'</code></dt>
<dd>Timezone of input data. If string, 'UTC' will use the time as input directly. Any other string value needs to be a TZ identifier in the IANA database, a wikipedia page of these is available here: <a href="https://en.wikipedia.org/wiki/List_of_tz_database_time_zones.">https://en.wikipedia.org/wiki/List_of_tz_database_time_zones.</a>
If int, should be the int value of the UTC offset (e.g., for American Eastern Standard Time: -5).
This is necessary for Raspberry Shake data in 'raw' format.</dd>
<dt><strong><code>xcoord</code></strong> :&ensp;<code>float</code>, default=<code>-88.2290526</code></dt>
<dd>Longitude (or easting, or, generally, X coordinate) of data point, in Coordinate Reference System (CRS) designated by input_crs. Currently only used in csv output, but will likely be used in future for mapping/profile purposes.</dd>
<dt><strong><code>ycoord</code></strong> :&ensp;<code>float</code>, default=<code>40.1012122</code></dt>
<dd>Latitute (or northing, or, generally, X coordinate) of data point, in Coordinate Reference System (CRS) designated by input_crs. Currently only used in csv output, but will likely be used in future for mapping/profile purposes.</dd>
<dt><strong><code>input_crs</code></strong> :&ensp;<code>str</code> or <code>other format read by pyproj</code>, default=<code>'EPSG:4326'</code></dt>
<dd>Coordinate reference system of input data, as used by pyproj.CRS.from_user_input()</dd>
<dt><strong><code>output_crs</code></strong> :&ensp;<code>str</code> or <code>other format read by pyproj</code>, default=<code>'EPSG:4326'</code></dt>
<dd>Coordinate reference system to which input data will be transformed, as used by pyproj.CRS.from_user_input()</dd>
<dt><strong><code>elevation</code></strong> :&ensp;<code>float</code>, default=<code>755</code></dt>
<dd>Surface elevation of data point. Not currently used (except in csv output), but will likely be used in the future.</dd>
<dt><strong><code>depth</code></strong> :&ensp;<code>float</code>, default=<code>0</code></dt>
<dd>Depth of seismometer. Not currently used, but will likely be used in the future.</dd>
<dt><strong><code>instrument</code></strong> :&ensp;<code>str</code> or <code>list {'Raspberry Shake')</code></dt>
<dd>Instrument from which the data was acquired.</dd>
<dt><strong><code>metapath</code></strong> :&ensp;<code>str</code> or <code>pathlib.Path object</code>, default=<code>''</code></dt>
<dd>Filepath of metadata, in format supported by obspy.read_inventory. If default value of '', will read from resources folder of repository (only supported for Raspberry Shake).</dd>
<dt><strong><code>hvsr_band</code></strong> :&ensp;<code>list</code>, default=<code>[0.4, 40]</code></dt>
<dd>Two-element list containing low and high "corner" frequencies (in Hz) for processing. This can specified again later.</dd>
<dt><strong><code>peak_freq_range</code></strong> :&ensp;<code>list</code> or <code>tuple</code>, default=<code>[0.4, 40]</code></dt>
<dd>Two-element list or tuple containing low and high frequencies (in Hz) that are used to check for HVSR Peaks. This can be a tigher range than hvsr_band, but if larger, it will still only use the hvsr_band range.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, default=<code>False</code></dt>
<dd>Whether to print output and results to terminal</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>params</code></strong> :&ensp;<code><a title="sprit.HVSRData" href="index.html#sprit.HVSRData">HVSRData</a></code></dt>
<dd>sprit.HVSRData class containing input parameters, including data file path and metadata path. This will be used as an input to other functions. If batch processing, params will be converted to batch type in fetch_data() step.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def input_params(datapath,
                site=&#39;HVSR Site&#39;,
                network=&#39;AM&#39;, 
                station=&#39;RAC84&#39;, 
                loc=&#39;00&#39;, 
                channels=[&#39;EHZ&#39;, &#39;EHN&#39;, &#39;EHE&#39;],
                acq_date=str(datetime.datetime.now().date()),
                starttime = &#39;00:00:00.00&#39;,
                endtime = &#39;23:59:59.999999&#39;,
                tzone = &#39;UTC&#39;,
                xcoord = -88.2290526,
                ycoord =  40.1012122,
                elevation = 755,
                input_crs=&#39;EPSG:4326&#39;,#4269 is NAD83, defautling to WGS
                output_crs=&#39;EPSG:4326&#39;,
                elev_unit = &#39;feet&#39;,
                depth = 0,
                instrument = &#39;Raspberry Shake&#39;,
                metapath = &#39;&#39;,
                hvsr_band = [0.4, 40],
                peak_freq_range=[0.4, 40],
                verbose=False
                ):
    &#34;&#34;&#34;Function for designating input parameters for reading in and processing data
    
    Parameters
    ----------
    datapath : str or pathlib.Path object
        Filepath of data. This can be a directory or file, but will need to match with what is chosen later as the source parameter in fetch_data()
    site : str, default=&#34;HVSR Site&#34;
        Site name as designated by user for ease of reference. Used for plotting titles, filenames, etc.
    network : str, default=&#39;AM&#39;
        The network designation of the seismometer. This is necessary for data from Raspberry Shakes. &#39;AM&#39; is for Amateur network, which fits Raspberry Shakes.
    station : str, default=&#39;RAC84&#39;
        The station name of the seismometer. This is necessary for data from Raspberry Shakes.
    loc : str, default=&#39;00&#39;
        Location information of the seismometer.
    channels : list, default=[&#39;EHZ&#39;, &#39;EHN&#39;, &#39;EHE&#39;]
        The three channels used in this analysis, as a list of strings. Preferred that Z component is first, but not necessary
    acq_date : str, int, date object, or datetime object
        If string, preferred format is &#39;YYYY-MM-DD&#39;. 
        If int, this will be interpreted as the time_int of year of current year (e.g., 33 would be Feb 2 of current year)
        If date or datetime object, this will be the date. Make sure to account for time change when converting to UTC (if UTC is the following time_int, use the UTC time_int).
    starttime : str, time object, or datetime object, default=&#39;00:00:00.00&#39;
        Start time of data stream. This is necessary for Raspberry Shake data in &#39;raw&#39; form, or for trimming data. Format can be either &#39;HH:MM:SS.micros&#39; or &#39;HH:MM&#39; at minimum.
    endtime : str, time obejct, or datetime object, default=&#39;23:59:99.99&#39;
        End time of data stream. This is necessary for Raspberry Shake data in &#39;raw&#39; form, or for trimming data. Same format as starttime.
    tzone : str or int, default = &#39;UTC&#39;
        Timezone of input data. If string, &#39;UTC&#39; will use the time as input directly. Any other string value needs to be a TZ identifier in the IANA database, a wikipedia page of these is available here: https://en.wikipedia.org/wiki/List_of_tz_database_time_zones.
        If int, should be the int value of the UTC offset (e.g., for American Eastern Standard Time: -5). 
        This is necessary for Raspberry Shake data in &#39;raw&#39; format.
    xcoord : float, default=-88.2290526
        Longitude (or easting, or, generally, X coordinate) of data point, in Coordinate Reference System (CRS) designated by input_crs. Currently only used in csv output, but will likely be used in future for mapping/profile purposes.
    ycoord : float, default=40.1012122
        Latitute (or northing, or, generally, X coordinate) of data point, in Coordinate Reference System (CRS) designated by input_crs. Currently only used in csv output, but will likely be used in future for mapping/profile purposes.
    input_crs : str or other format read by pyproj, default=&#39;EPSG:4326&#39;
        Coordinate reference system of input data, as used by pyproj.CRS.from_user_input()
    output_crs : str or other format read by pyproj, default=&#39;EPSG:4326&#39;
        Coordinate reference system to which input data will be transformed, as used by pyproj.CRS.from_user_input()
    elevation : float, default=755
        Surface elevation of data point. Not currently used (except in csv output), but will likely be used in the future.
    depth : float, default=0
        Depth of seismometer. Not currently used, but will likely be used in the future.
    instrument : str or list {&#39;Raspberry Shake&#39;)
        Instrument from which the data was acquired. 
    metapath : str or pathlib.Path object, default=&#39;&#39;
        Filepath of metadata, in format supported by obspy.read_inventory. If default value of &#39;&#39;, will read from resources folder of repository (only supported for Raspberry Shake).
    hvsr_band : list, default=[0.4, 40]
        Two-element list containing low and high &#34;corner&#34; frequencies (in Hz) for processing. This can specified again later.
    peak_freq_range : list or tuple, default=[0.4, 40]
        Two-element list or tuple containing low and high frequencies (in Hz) that are used to check for HVSR Peaks. This can be a tigher range than hvsr_band, but if larger, it will still only use the hvsr_band range.
    verbose : bool, default=False
        Whether to print output and results to terminal

    Returns
    -------
    params : sprit.HVSRData
        sprit.HVSRData class containing input parameters, including data file path and metadata path. This will be used as an input to other functions. If batch processing, params will be converted to batch type in fetch_data() step.

    &#34;&#34;&#34;
    orig_args = locals().copy() #Get the initial arguments

    #Declare obspy here instead of at top of file for (for example) colab, where obspy first needs to be installed on environment
    global obspy
    import obspy
    if verbose:
        print(&#39;Gathering input parameters (input_params())&#39;)
        for key, value in orig_args.items():
            print(&#39;\t  {}={}&#39;.format(key, value))
        print()

    #Make Sure metapath is all good
    if not pathlib.Path(metapath).exists() or metapath==&#39;&#39;:
        if metapath == &#39;&#39;:
            pass
        else:
            print(&#39;Specified metadata file cannot be read!&#39;)
        repoDir = pathlib.Path(os.path.dirname(__file__))
        metapath = pathlib.Path(pkg_resources.resource_filename(__name__, &#39;resources/rs3dv5plus_metadata.inv&#39;))
        #print(&#39;Using default metadata file for Raspberry Shake v.7 distributed with package&#39;)
    else:
        if isinstance(metapath, pathlib.PurePath):
            metapath = metapath.as_posix()
        else:
            metapath = pathlib.Path(metapath).as_posix()

    #Reformat times
    if type(acq_date) is datetime.datetime:
        date = str(acq_date.date())
    elif type(acq_date) is datetime.date:
        date=str(acq_date)
    elif type(acq_date) is str:
        monthStrs = {&#39;jan&#39;:1, &#39;january&#39;:1,
                    &#39;feb&#39;:2, &#39;february&#39;:2,
                    &#39;mar&#39;:3, &#39;march&#39;:3,
                    &#39;apr&#39;:4, &#39;april&#39;:4,
                    &#39;may&#39;:5,
                    &#39;jun&#39;:6, &#39;june&#39;:6,
                    &#39;jul&#39;:7, &#39;july&#39;:7,
                    &#39;aug&#39;:8, &#39;august&#39;:8,
                    &#39;sep&#39;:9, &#39;sept&#39;:9, &#39;september&#39;:9,
                    &#39;oct&#39;:10,&#39;october&#39;:10, 
                    &#39;nov&#39;:11,&#39;november&#39;:11,
                    &#39;dec&#39;:12,&#39;december&#39;:12}

        spelledMonth = False
        for m in monthStrs.keys():
            acq_date = acq_date.lower()
            if m in acq_date:
                spelledMonth = True
                break

        if spelledMonth is not False:
            month = monthStrs[m]

        if &#39;/&#39; in acq_date:
            sep = &#39;/&#39;
        elif &#39;.&#39; in acq_date:
            sep=&#39;.&#39;
        elif &#39; &#39; in acq_date:
            sep = &#39; &#39;
            acq_date = acq_date.replace(&#39;,&#39;, &#39;&#39;)
        else:
            sep = &#39;-&#39;

        acq_date = acq_date.split(sep)
        if len(acq_date[2]) &gt; 2: #American format
            date = &#39;{}-{}-{}&#39;.format(acq_date[2], acq_date[0], acq_date[1])
        else: #international format, one we&#39;re going to use
            date = &#39;{}-{}-{}&#39;.format(acq_date[0], acq_date[1], acq_date[2])     

    elif type(acq_date) is int:
        year=datetime.datetime.today().year
        date = str((datetime.datetime(year, 1, 1) + datetime.timedelta(acq_date - 1)).date())
    
    if type(starttime) is str:
        if &#39;T&#39; in starttime:
            #date=starttime.split(&#39;T&#39;)[0]
            starttime = starttime.split(&#39;T&#39;)[1]
        else:
            pass
            #starttime = date+&#39;T&#39;+starttime
    elif type(starttime) is datetime.datetime:
        #date = str(starttime.date())
        starttime = str(starttime.time())
        ###HERE IS NEXT
    elif type(starttime) is datetime.time():
        starttime = str(starttime)
    
    starttime = date+&#34;T&#34;+starttime
    starttime = obspy.UTCDateTime(sprit_utils.format_time(starttime, tzone=tzone))
    
    if type(endtime) is str:
        if &#39;T&#39; in endtime:
            date=endtime.split(&#39;T&#39;)[0]
            endtime = endtime.split(&#39;T&#39;)[1]
    elif type(endtime) is datetime.datetime:
        date = str(endtime.date())
        endtime = str(endtime.time())
    elif type(endtime) is datetime.time():
        endtime = str(endtime)

    endtime = date+&#34;T&#34;+endtime
    endtime = obspy.UTCDateTime(sprit_utils.format_time(endtime, tzone=tzone))

    acq_date = datetime.date(year=int(date.split(&#39;-&#39;)[0]), month=int(date.split(&#39;-&#39;)[1]), day=int(date.split(&#39;-&#39;)[2]))
    raspShakeInstNameList = [&#39;raspberry shake&#39;, &#39;shake&#39;, &#39;raspberry&#39;, &#39;rs&#39;, &#39;rs3d&#39;, &#39;rasp. shake&#39;, &#39;raspshake&#39;]
    
    #Raspberry shake stationxml is in the resources folder, double check we have right path
    if instrument.lower() in raspShakeInstNameList:
        if metapath == r&#39;resources/rs3dv7_metadata.inv&#39;:
            metapath = pathlib.Path(pkg_resources.resource_filename(__name__, &#39;resources/rs3dv7_metadata.inv&#39;))
            #metapath = pathlib.Path(os.path.realpath(__file__)).parent.joinpath(&#39;/resources/rs3dv7_metadata.inv&#39;)

    if output_crs is None:
        output_crs=&#39;EPSG:4326&#39;

    if input_crs is None:
        input_crs = &#39;EPSG:4326&#39;#Default to WGS84
    else:        
        input_crs = CRS.from_user_input(input_crs)
        output_crs = CRS.from_user_input(output_crs)

        coord_transformer = Transformer.from_crs(input_crs, output_crs, always_xy=True)
        xcoord, ycoord = coord_transformer.transform(xcoord, ycoord)

    #Add key/values to input parameter dictionary
    inputParamDict = {&#39;site&#39;:site, &#39;net&#39;:network,&#39;sta&#39;:station, &#39;loc&#39;:loc, &#39;cha&#39;:channels, &#39;instrument&#39;:instrument,
                    &#39;acq_date&#39;:acq_date,&#39;starttime&#39;:starttime,&#39;endtime&#39;:endtime, &#39;timezone&#39;:&#39;UTC&#39;, #Will be in UTC by this point
                    &#39;longitude&#39;:xcoord,&#39;latitude&#39;:ycoord,&#39;elevation&#39;:elevation,&#39;input_crs&#39;:input_crs, &#39;output_crs&#39;:output_crs,
                    &#39;depth&#39;:depth, &#39;datapath&#39;: datapath, &#39;metapath&#39;:metapath, &#39;hvsr_band&#39;:hvsr_band, &#39;peak_freq_range&#39;:peak_freq_range,
                    &#39;ProcessingStatus&#39;:{&#39;InputStatus&#39;:True, &#39;OverallStatus&#39;:True}
                    }
    
    params = sprit_utils.make_it_classy(inputParamDict)
    params[&#39;ProcessingStatus&#39;][&#39;InputParams&#39;] = True
    params = _check_processing_status(params)
    return params</code></pre>
</details>
</dd>
<dt id="sprit.sprit_hvsr.plot_hvsr"><code class="name flex">
<span>def <span class="ident">plot_hvsr</span></span>(<span>hvsr_data, plot_type='HVSR ann p C+ ann p SPEC', use_subplots=True, xtype='freq', fig=None, ax=None, return_fig=False, save_dir=None, save_suffix='', show_legend=False, show=True, close_figs=False, clear_fig=True, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to plot HVSR data</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>hvsr_data</code></strong> :&ensp;<code>dict
</code></dt>
<dd>Dictionary containing output from process_hvsr function</dd>
<dt><strong><code>plot_type</code></strong> :&ensp;<code>str='HVSR'</code> or <code>list
</code></dt>
<dd>The plot_type of plot(s) to plot. If list, will plot all plots listed
'HVSR' : Standard HVSR plot, including standard deviation
- '[HVSR] p' : HVSR plot with BestPeaks shown
- '[HVSR] p' : HVSR plot with best picked peak shown
<br>
- '[HVSR] p<em> all' : HVSR plot with all picked peaks shown
<br>
- '[HVSR] p</em> t' : HVSR plot with peaks from all time steps in background
<br>
- '[HVSR p* ann] : Annotates plot with peaks
- '[HVSR] -s' : HVSR plots don't show standard deviation
- '[HVSR] t' : HVSR plot with individual hv curves for each time step shown
- '[HVSR] c' : HVSR plot with each components' spectra. Recommended to do this last (or just before 'specgram'), since doing c+ can make the component chart its own chart
'Specgram' : Combined spectrogram of all components
- '[spec]' : basic spectrogram plot of H/V curve</dd>
<dt><strong><code>use_subplots</code></strong> :&ensp;<code>bool</code>, default <code>= True</code></dt>
<dd>Whether to output the plots as subplots (True) or as separate plots (False)</dd>
<dt><strong><code>xtype</code></strong> :&ensp;<code>str</code>, default <code>= 'freq'</code></dt>
<dd>String for what to use, between frequency or period
For frequency, the following are accepted (case does not matter): 'f', 'Hz', 'freq', 'frequency'
For period, the following are accepted (case does not matter): 'p', 'T', 's', 'sec', 'second', 'per', 'period'</dd>
<dt><strong><code>fig</code></strong> :&ensp;<code>matplotlib.Figure</code>, default <code>= None</code></dt>
<dd>If not None, matplotlib figure on which plot is plotted</dd>
<dt><strong><code>ax</code></strong> :&ensp;<code>matplotlib.Axis</code>, default <code>= None</code></dt>
<dd>If not None, matplotlib axis on which plot is plotted</dd>
<dt><strong><code>return_fig</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to return figure and axis objects</dd>
<dt><strong><code>save_dir</code></strong> :&ensp;<code>str</code> or <code>None</code></dt>
<dd>Directory in which to save figures</dd>
<dt><strong><code>save_suffix</code></strong> :&ensp;<code>str</code></dt>
<dd>Suffix to add to end of figure filename(s), if save_dir is used</dd>
<dt><strong><code>show_legend</code></strong> :&ensp;<code>bool</code>, default=<code>False</code></dt>
<dd>Whether to show legend in plot</dd>
<dt><strong><code>show</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to show plot</dd>
<dt><strong><code>close_figs</code></strong> :&ensp;<code>bool</code>, default=<code>False</code></dt>
<dd>Whether to close figures before plotting</dd>
<dt><strong><code>clear_fig</code></strong> :&ensp;<code>bool</code>, default=<code>True</code></dt>
<dd>Whether to clear figures before plotting</dd>
<dt><strong><code>**kwargs</code></strong> :&ensp;<code>keyword arguments</code></dt>
<dd>Keyword arguments for matplotlib.pyplot</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>fig</code></strong>, <strong><code>ax</code></strong> :&ensp;<code>matplotlib figure and axis objects</code></dt>
<dd>Returns figure and axis matplotlib.pyplot objects if return_fig=True, otherwise, simply plots the figures</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_hvsr(hvsr_data, plot_type=&#39;HVSR ann p C+ ann p SPEC&#39;, use_subplots=True, xtype=&#39;freq&#39;, fig=None, ax=None, return_fig=False,  save_dir=None, save_suffix=&#39;&#39;, show_legend=False, show=True, close_figs=False, clear_fig=True,**kwargs):
    &#34;&#34;&#34;Function to plot HVSR data

    Parameters
    ----------
    hvsr_data : dict                  
        Dictionary containing output from process_hvsr function
    plot_type : str=&#39;HVSR&#39; or list    
        The plot_type of plot(s) to plot. If list, will plot all plots listed
        &#39;HVSR&#39; : Standard HVSR plot, including standard deviation
        - &#39;[HVSR] p&#39; : HVSR plot with BestPeaks shown
        - &#39;[HVSR] p&#39; : HVSR plot with best picked peak shown                
        - &#39;[HVSR] p* all&#39; : HVSR plot with all picked peaks shown                
        - &#39;[HVSR] p* t&#39; : HVSR plot with peaks from all time steps in background                
        - &#39;[HVSR p* ann] : Annotates plot with peaks
        - &#39;[HVSR] -s&#39; : HVSR plots don&#39;t show standard deviation
        - &#39;[HVSR] t&#39; : HVSR plot with individual hv curves for each time step shown
        - &#39;[HVSR] c&#39; : HVSR plot with each components&#39; spectra. Recommended to do this last (or just before &#39;specgram&#39;), since doing c+ can make the component chart its own chart
        &#39;Specgram&#39; : Combined spectrogram of all components
        - &#39;[spec]&#39; : basic spectrogram plot of H/V curve
    use_subplots : bool, default = True
        Whether to output the plots as subplots (True) or as separate plots (False)
    xtype : str, default = &#39;freq&#39;    
        String for what to use, between frequency or period
            For frequency, the following are accepted (case does not matter): &#39;f&#39;, &#39;Hz&#39;, &#39;freq&#39;, &#39;frequency&#39;
            For period, the following are accepted (case does not matter): &#39;p&#39;, &#39;T&#39;, &#39;s&#39;, &#39;sec&#39;, &#39;second&#39;, &#39;per&#39;, &#39;period&#39;
    fig : matplotlib.Figure, default = None
        If not None, matplotlib figure on which plot is plotted
    ax : matplotlib.Axis, default = None
        If not None, matplotlib axis on which plot is plotted
    return_fig : bool
        Whether to return figure and axis objects
    save_dir : str or None
        Directory in which to save figures
    save_suffix : str
        Suffix to add to end of figure filename(s), if save_dir is used
    show_legend : bool, default=False
        Whether to show legend in plot
    show : bool
        Whether to show plot
    close_figs : bool, default=False
        Whether to close figures before plotting
    clear_fig : bool, default=True
        Whether to clear figures before plotting
    **kwargs : keyword arguments
        Keyword arguments for matplotlib.pyplot

    Returns
    -------
    fig, ax : matplotlib figure and axis objects
        Returns figure and axis matplotlib.pyplot objects if return_fig=True, otherwise, simply plots the figures
    &#34;&#34;&#34;
    orig_args = locals().copy() #Get the initial arguments
    if isinstance(hvsr_data, HVSRBatch):
        #If running batch, we&#39;ll loop through each site
        for site_name in hvsr_data.keys():
            args = orig_args.copy() #Make a copy so we don&#39;t accidentally overwrite
            individual_params = hvsr_data[site_name] #Get what would normally be the &#34;params&#34; variable for each site
            args[&#39;hvsr_results&#39;] = individual_params #reset the params parameter we originally read in to an individual site params
            if hvsr_data[site_name][&#39;ProcessingStatus&#39;][&#39;OverallStatus&#39;]:
                try:
                    _hvsr_plot_batch(**args) #Call another function, that lets us run this function again
                except:
                    print(f&#34;{site_name} not able to be plotted.&#34;)
    else:
        if clear_fig and fig is not None and ax is not None: #Intended use for tkinter
            #Clear everything
            for key in ax:
                ax[key].clear()
            fig.clear()
        if close_figs:
            plt.close(&#39;all&#39;)

        compList = [&#39;c&#39;, &#39;comp&#39;, &#39;component&#39;, &#39;components&#39;]
        specgramList = [&#39;spec&#39;, &#39;specgram&#39;, &#39;spectrogram&#39;]
        hvsrList = [&#39;hvsr&#39;, &#39;hv&#39;, &#39;h&#39;]

        hvsrInd = np.nan
        compInd = np.nan
        specInd = np.nan

        kList = plot_type.split(&#39; &#39;)
        for i, k in enumerate(kList):
            kList[i] = k.lower()

        #Get the plots in the right order, no matter how they were input (and ensure the right options go with the right plot)
        #HVSR index
        if len(set(hvsrList).intersection(kList)):
            for i, hv in enumerate(hvsrList):
                if hv in kList:
                    hvsrInd = kList.index(hv)
                    break
        #Component index
        #if len(set(compList).intersection(kList)):
        for i, c in enumerate(kList):
            if &#39;+&#39; in c and c[:-1] in compList:
                compInd = kList.index(c)
                break
            
        #Specgram index
        if len(set(specgramList).intersection(kList)):
            for i, sp in enumerate(specgramList):
                if sp in kList:
                    specInd = kList.index(sp)
                    break        

        indList = [hvsrInd, compInd, specInd]
        indListCopy = indList.copy()
        plotTypeList = [&#39;hvsr&#39;, &#39;comp&#39;, &#39;spec&#39;]

        plotTypeOrder = []
        plotIndOrder = []

        lastVal = 0
        while lastVal != 99:
            firstInd = np.nanargmin(indListCopy)
            plotTypeOrder.append(plotTypeList[firstInd])
            plotIndOrder.append(indList[firstInd])
            lastVal = indListCopy[firstInd]
            indListCopy[firstInd] = 99 #just a high number

        plotTypeOrder.pop()
        plotIndOrder[-1]=len(kList)

        for i, p in enumerate(plotTypeOrder):
            pStartInd = plotIndOrder[i]
            pEndInd = plotIndOrder[i+1]
            plotComponents = kList[pStartInd:pEndInd]

            if use_subplots and i==0 and fig is None and ax is None:
                mosaicPlots = []
                for pto in plotTypeOrder:
                    mosaicPlots.append([pto])
                fig, ax = plt.subplot_mosaic(mosaicPlots, gridspec_kw={&#39;hspace&#39;:0.3})
                axis = ax[p]
            elif use_subplots:
                with warnings.catch_warnings():
                    warnings.simplefilter(&#34;ignore&#34;) #Often warns about xlim when it is not an issue
                    ax[p].clear()
                axis = ax[p]
            else:
                fig, axis = plt.subplots()
                    
            if p == &#39;hvsr&#39;:
                _plot_hvsr(hvsr_data, fig=fig, ax=axis, plot_type=plotComponents, xtype=&#39;x_freqs&#39;, show_legend=show_legend, axes=ax)
            elif p==&#39;comp&#39;:
                plotComponents[0] = plotComponents[0][:-1]
                _plot_hvsr(hvsr_data, fig=fig, ax=axis, plot_type=plotComponents, xtype=&#39;x_freqs&#39;, show_legend=show_legend, axes=ax)
            elif p==&#39;spec&#39;:
                _plot_specgram_hvsr(hvsr_data, fig=fig, ax=axis, colorbar=False)
            else:
                warnings.warn(&#39;Plot type {p} not recognized&#39;, UserWarning)   

        if show:
            fig.canvas.draw()
            
        if return_fig:
            return fig, ax
    return</code></pre>
</details>
</dd>
<dt id="sprit.sprit_hvsr.plot_stream"><code class="name flex">
<span>def <span class="ident">plot_stream</span></span>(<span>stream, params, fig=None, axes=None, show_plot=False, ylim_std=0.75, return_fig=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to plot a stream of data with Z, E, N components using matplotlib. Similar to obspy.Stream.Plot(), but will be formatted differently and eventually more customizable.
This is also used in various functions throughout the package.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>stream</code></strong> :&ensp;<code>obspy.core.Stream.stream</code></dt>
<dd>Obpsy stream of data with Z, E, N componenents</dd>
<dt><strong><code>params</code></strong> :&ensp;<code><a title="sprit.sprit_hvsr.HVSRData" href="#sprit.sprit_hvsr.HVSRData">HVSRData</a></code> or <code><a title="sprit.sprit_hvsr.HVSRBatch" href="#sprit.sprit_hvsr.HVSRBatch">HVSRBatch</a></code></dt>
<dd>Data object with parameters relevant for creating plot</dd>
<dt><strong><code>fig</code></strong> :&ensp;<code>matplotlib.Figure</code>, default=<code>None</code></dt>
<dd>Optional: if not None, matplotlib.Figure in which to plot the resulting figure (i.e., can be plotted in existing figure)</dd>
<dt><strong><code>axes</code></strong> :&ensp;<code>matplotlib.Axis</code>, default=<code>None</code></dt>
<dd>Optional: if not None, matplotlib.Axis in which to plot the resulting figure (i.e., can be plotted in existing axis)</dd>
<dt><strong><code>show_plot</code></strong> :&ensp;<code>bool</code>, default=<code>False</code></dt>
<dd>Whether to do matplotlib.pylot.show(), by default False</dd>
<dt><strong><code>ylim_std</code></strong> :&ensp;<code>float</code>, default <code>= 0.75</code></dt>
<dd>Optional: the standard deviation of the data at which to clip the chart, by default 0.75</dd>
<dt><strong><code>return_fig</code></strong> :&ensp;<code>bool</code>, default=<code>True</code></dt>
<dd>Optional: whether to return the figure, by default True</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>(matplotlib.Figure, matplotlib.Axes)
Tuple containing the figure and axes of the resulting plot, only returned if return_fig = True</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_stream(stream, params, fig=None, axes=None, show_plot=False, ylim_std=0.75, return_fig=True):
    &#34;&#34;&#34;Function to plot a stream of data with Z, E, N components using matplotlib. Similar to obspy.Stream.Plot(), but will be formatted differently and eventually more customizable.
    This is also used in various functions throughout the package.

    Parameters
    ----------
    stream : obspy.core.Stream.stream
        Obpsy stream of data with Z, E, N componenents
    params : HVSRData or HVSRBatch
        Data object with parameters relevant for creating plot
    fig : matplotlib.Figure, default=None
        Optional: if not None, matplotlib.Figure in which to plot the resulting figure (i.e., can be plotted in existing figure)
    axes : matplotlib.Axis, default=None
        Optional: if not None, matplotlib.Axis in which to plot the resulting figure (i.e., can be plotted in existing axis)
    show_plot : bool, default=False
        Whether to do matplotlib.pylot.show(), by default False
    ylim_std : float, default = 0.75
        Optional: the standard deviation of the data at which to clip the chart, by default 0.75
    return_fig : bool, default=True
        Optional: whether to return the figure, by default True

    Returns
    -------
    (matplotlib.Figure, matplotlib.Axes)
        Tuple containing the figure and axes of the resulting plot, only returned if return_fig = True
    &#34;&#34;&#34;
    if fig is None and axes is None:
        fig, axes = plt.subplot_mosaic([[&#39;Z&#39;],[&#39;N&#39;],[&#39;E&#39;]], sharex=True, sharey=False)

    new_stream = stream.copy()
    #axis.plot(trace.times, trace.data)
    
    sTime = stream[0].stats.starttime
    timeList = {}
    mplTimes = {}

    #In case data is masked, need to split, decimate, then merge back together
    if isinstance(new_stream[0].data, np.ma.masked_array):
        new_stream = new_stream.split()
    new_stream.decimate(10)
    new_stream.merge()

    zStream = new_stream.select(component=&#39;Z&#39;)#[0]
    eStream = new_stream.select(component=&#39;E&#39;)#[0]
    nStream = new_stream.select(component=&#39;N&#39;)#[0]
    streams = [zStream, nStream, eStream]

    for st in streams:
        key = st[0].stats.component
        timeList[key] = []
        mplTimes[key] = []
        for tr in st:
            for t in np.ma.getdata(tr.times()):
                newt = sTime + t
                timeList[key].append(newt)
                mplTimes[key].append(newt.matplotlib_date)

    #Ensure that the min and max times for each component are the same
    for i, k in enumerate(mplTimes.keys()):
        currMin = np.min(list(map(np.min, mplTimes[k])))
        currMax = np.max(list(map(np.max, mplTimes[k])))

        if i == 0:
            xmin = currMin
            xmax = currMax
        else:
            if xmin &gt; currMin:
                xmin = currMin
            if xmax &lt; currMax:
                xmax = currMax

    axes[&#39;Z&#39;].xaxis_date()
    axes[&#39;N&#39;].xaxis_date()
    axes[&#39;E&#39;].xaxis_date()

    #tTicks = mdates.MinuteLocator(interval=5)
    #axis.xaxis.set_major_locator(tTicks)
    axes[&#39;E&#39;].xaxis.set_major_locator(mdates.MinuteLocator(byminute=range(0,60,5)))
    axes[&#39;E&#39;].xaxis.set_major_formatter(mdates.DateFormatter(&#39;%H:%M&#39;))
    axes[&#34;E&#34;].xaxis.set_minor_locator(mdates.MinuteLocator(interval=1))
    axes[&#34;E&#34;].tick_params(axis=&#39;x&#39;, labelsize=8)
    

    streams = [zStream.merge(method=1), 
               nStream.merge(method=1), 
               eStream.merge(method=1)]

    for st in streams:
        for i, tr in enumerate(st):
            key = tr.stats.component
            if key == &#39;Z&#39;:
                C=&#39;k&#39;
            elif key==&#39;N&#39;:
                C=&#39;r&#39;
            else:
                C=&#39;b&#39;
            axes[key].plot(mplTimes[key], tr.data, color=C, linewidth=0.15)


    axes[&#39;Z&#39;].set_ylabel(&#39;Z&#39;)
    axes[&#39;N&#39;].set_ylabel(&#39;N&#39;)
    axes[&#39;E&#39;].set_ylabel(&#39;E&#39;)
    
    #stDz = np.abs(np.nanstd(stream.select(component=&#39;Z&#39;)[0].data))
    #stDn = np.abs(np.nanstd(stream.select(component=&#39;N&#39;)[0].data))
    #stDe = np.abs(np.nanstd(stream.select(component=&#39;E&#39;)[0].data))
    #stD = max([stDz, stDn, stDe])
    
    for i, comp in enumerate(list(mplTimes.keys())):
        stD = np.abs(np.nanstd(np.ma.getdata(stream.select(component=comp)[0].data)))
        dmed = np.nanmedian(np.ma.getdata(stream.select(component=comp)[0].data))

        axes[comp].set_ylim([dmed-ylim_std*stD, dmed+ylim_std*stD])
        if xmin &lt; 0:
            xmin=params[&#39;hvsr_band&#39;][0]
        axes[comp].set_xlim([xmin, xmax])

    fig.suptitle(params[&#39;site&#39;])
    
    day = &#34;{}-{}-{}&#34;.format(stream[0].stats.starttime.year, stream[0].stats.starttime.month, stream[0].stats.starttime.day)
    axes[&#39;E&#39;].set_xlabel(&#39;UTC Time \n&#39;+ day)

    #plt.rcParams[&#39;figure.dpi&#39;] = 100
    #plt.rcParams[&#39;figure.figsize&#39;] = (5,4)
    
    #fig.tight_layout()
    fig.canvas.draw()

    if show_plot:
        plt.show()

    if return_fig:
        return fig, axes
    return                 </code></pre>
</details>
</dd>
<dt id="sprit.sprit_hvsr.process_hvsr"><code class="name flex">
<span>def <span class="ident">process_hvsr</span></span>(<span>params, method=3, smooth=True, freq_smooth='konno ohmachi', f_smooth_width=40, resample=True, outlier_curve_std=1.75, verbose=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Process the input data and get HVSR data</p>
<p>This is the main function that uses other (private) functions to do
the bulk of processing of the HVSR data and the data quality checks.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt>params
: HVSRData or HVSRBatch</dt>
<dt>Data object containing all the parameters input and generated by the user (usually, during sprit.input_params(), sprit.fetch_data(), sprit.generate_ppsds() and/or sprit.remove_noise()).</dt>
<dt>method
: int or str, default=3</dt>
<dt>Method to use for combining the horizontal components</dt>
<dt>0) (not used)</dt>
<dt>1) Diffuse field assumption, or 'DFA' (not currently implemented)</dt>
<dt>2) 'Arithmetic Mean': H â‰¡ (HN + HE)/2</dt>
<dt>3) 'Geometric Mean': H â‰¡ âˆšHN Â· HE, recommended by the SESAME project (2004)</dt>
<dt>4) 'Vector Summation': H â‰¡ âˆšH2 N + H2 E</dt>
<dt>5) 'Quadratic Mean': H â‰¡ âˆš(H2 N + H2 E )/2</dt>
<dt HE HN_="HN,">6) 'Maximum Horizontal Value': H â‰¡ max</dt>
<dt>smooth
: bool, default=True</dt>
<dt>bool or int may be used.</dt>
<dt>If True, default to smooth H/V curve to using savgoy filter with window length of 51 (works well with default resample of 1000 pts)</dt>
<dt>If int, the length of the window in the savgoy filter.</dt>
<dt><strong><code>freq_smooth</code></strong> :&ensp;<code>str {'konno ohmachi', 'constant', 'proportional'}</code></dt>
<dd>Which frequency smoothing method to use. By default, uses the 'konno ohmachi' method.
- The Konno &amp; Ohmachi method uses the obspy.signal.konnoohmachismoothing.konno_ohmachi_smoothing() function: <a href="https://docs.obspy.org/packages/autogen/obspy.signal.konnoohmachismoothing.konno_ohmachi_smoothing.html">https://docs.obspy.org/packages/autogen/obspy.signal.konnoohmachismoothing.konno_ohmachi_smoothing.html</a>
- The constant method uses a window of constant length f_smooth_width
- The proportional method uses a window the percentage length of the frequncy steps/range (f_smooth_width now refers to percentage)
See here for more information: <a href="https://www.geopsy.org/documentation/geopsy/hv-processing.html">https://www.geopsy.org/documentation/geopsy/hv-processing.html</a></dd>
<dt><strong><code>f_smooth_width</code></strong> :&ensp;<code>int</code>, default <code>= 40</code></dt>
<dd>
<ul>
<li>For 'konno ohmachi': passed directly to the bandwidth parameter of the konno_ohmachi_smoothing() function, determines the width of the smoothing peak, with lower values resulting in broader peak. Must be &gt; 0.</li>
<li>For 'constant': the size of a triangular smoothing window in the number of frequency steps</li>
<li>For 'proportional': the size of a triangular smoothing window in percentage of the number of frequency steps (e.g., if 1000 frequency steps/bins and f_smooth_width=40, window would be 400 steps wide)</li>
</ul>
</dd>
<dt>resample
: bool, default = True</dt>
<dt>bool or int.</dt>
<dt>If True, default to resample H/V data to include 1000 frequency values for the rest of the analysis</dt>
<dt>If int, the number of data points to interpolate/resample/smooth the component psd/HV curve data to.</dt>
<dt><strong><code>outlier_curve_std</code></strong> :&ensp;<code>float</code>, default <code>= 1.75</code></dt>
<dd>Standard deviation of mean of each H/V curve to use as cuttoff for whether an H/V curve is considered an 'outlier'</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool, defualt=False</code></dt>
<dd>Whether to print output to terminal</dd>
</dl>
<h2 id="returns">Returns</h2>
<pre><code>hvsr_out    : dict
    Dictionary containing all the information about the data, including input parameters
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def process_hvsr(params, method=3, smooth=True, freq_smooth=&#39;konno ohmachi&#39;, f_smooth_width=40, resample=True, outlier_curve_std=1.75, verbose=False):
    &#34;&#34;&#34;Process the input data and get HVSR data
    
    This is the main function that uses other (private) functions to do 
    the bulk of processing of the HVSR data and the data quality checks.

    Parameters
    ----------
    params  : HVSRData or HVSRBatch
        Data object containing all the parameters input and generated by the user (usually, during sprit.input_params(), sprit.fetch_data(), sprit.generate_ppsds() and/or sprit.remove_noise()).
    method  : int or str, default=3
        Method to use for combining the horizontal components
            0) (not used)
            1) Diffuse field assumption, or &#39;DFA&#39; (not currently implemented)
            2) &#39;Arithmetic Mean&#39;: H â‰¡ (HN + HE)/2
            3) &#39;Geometric Mean&#39;: H â‰¡ âˆšHN Â· HE, recommended by the SESAME project (2004)
            4) &#39;Vector Summation&#39;: H â‰¡ âˆšH2 N + H2 E
            5) &#39;Quadratic Mean&#39;: H â‰¡ âˆš(H2 N + H2 E )/2
            6) &#39;Maximum Horizontal Value&#39;: H â‰¡ max {HN, HE}
    smooth  : bool, default=True
        bool or int may be used. 
            If True, default to smooth H/V curve to using savgoy filter with window length of 51 (works well with default resample of 1000 pts)
            If int, the length of the window in the savgoy filter.
    freq_smooth : str {&#39;konno ohmachi&#39;, &#39;constant&#39;, &#39;proportional&#39;}
        Which frequency smoothing method to use. By default, uses the &#39;konno ohmachi&#39; method.
            - The Konno &amp; Ohmachi method uses the obspy.signal.konnoohmachismoothing.konno_ohmachi_smoothing() function: https://docs.obspy.org/packages/autogen/obspy.signal.konnoohmachismoothing.konno_ohmachi_smoothing.html
            - The constant method uses a window of constant length f_smooth_width
            - The proportional method uses a window the percentage length of the frequncy steps/range (f_smooth_width now refers to percentage)
        See here for more information: https://www.geopsy.org/documentation/geopsy/hv-processing.html
    f_smooth_width : int, default = 40
        - For &#39;konno ohmachi&#39;: passed directly to the bandwidth parameter of the konno_ohmachi_smoothing() function, determines the width of the smoothing peak, with lower values resulting in broader peak. Must be &gt; 0.
        - For &#39;constant&#39;: the size of a triangular smoothing window in the number of frequency steps
        - For &#39;proportional&#39;: the size of a triangular smoothing window in percentage of the number of frequency steps (e.g., if 1000 frequency steps/bins and f_smooth_width=40, window would be 400 steps wide)
    resample  : bool, default = True
        bool or int. 
            If True, default to resample H/V data to include 1000 frequency values for the rest of the analysis
            If int, the number of data points to interpolate/resample/smooth the component psd/HV curve data to.
    outlier_curve_std : float, default = 1.75
        Standard deviation of mean of each H/V curve to use as cuttoff for whether an H/V curve is considered an &#39;outlier&#39;
    verbose : bool, defualt=False
        Whether to print output to terminal

    Returns
    -------
        hvsr_out    : dict
            Dictionary containing all the information about the data, including input parameters

    &#34;&#34;&#34;
    orig_args = locals().copy() #Get the initial arguments
    if (verbose and isinstance(params, HVSRBatch)) or (verbose and not params[&#39;batch&#39;]):
        if isinstance(params, HVSRData) and params[&#39;batch&#39;]:
            pass
        else:
            print(&#39;\nCalculating Horizontal/Vertical Ratios at all frequencies/time steps (process_hvsr())&#39;)
            print(&#39;\tUsing the following parameters:&#39;)
            for key, value in orig_args.items():
                if key==&#39;params&#39;:
                    pass
                else:
                    print(&#39;\t  {}={}&#39;.format(key, value))
            print()

    #First, divide up for batch or not
    #Site is in the keys anytime it&#39;s not batch
    if isinstance(params, HVSRBatch):
        #If running batch, we&#39;ll loop through each site
        hvsr_out = {}
        for site_name in params.keys():
            args = orig_args.copy() #Make a copy so we don&#39;t accidentally overwrite
            args[&#39;params&#39;] = params[site_name] #Get what would normally be the &#34;params&#34; variable for each site
            if params[site_name][&#39;ProcessingStatus&#39;][&#39;OverallStatus&#39;]:
                try:
                    hvsr_out[site_name] = _process_hvsr_batch(**args) #Call another function, that lets us run this function again
                except:
                    hvsr_out = params
                    hvsr_out[site_name][&#39;ProcessingStatus&#39;][&#39;HVStatus&#39;]=False
                    hvsr_out[site_name][&#39;ProcessingStatus&#39;][&#39;OverallStatus&#39;] = False                    
            else:
                hvsr_out = params
                hvsr_out[site_name][&#39;ProcessingStatus&#39;][&#39;HVStatus&#39;]=False
                hvsr_out[site_name][&#39;ProcessingStatus&#39;][&#39;OverallStatus&#39;] = False
        hvsr_out = HVSRBatch(hvsr_out)
    else:
        ppsds = params[&#39;ppsds&#39;].copy()#[k][&#39;psd_values&#39;]
        ppsds = sprit_utils.check_xvalues(ppsds)

        methodList = [&#39;&lt;placeholder_0&gt;&#39;, &#39;Diffuse Field Assumption&#39;, &#39;Arithmetic Mean&#39;, &#39;Geometric Mean&#39;, &#39;Vector Summation&#39;, &#39;Quadratic Mean&#39;, &#39;Maximum Horizontal Value&#39;]
        x_freqs = {}
        x_periods = {}

        psdValsTAvg = {}
        stDev = {}
        stDevValsP = {}
        stDevValsM = {}
        psdRaw={}
        currTimesUsed={}
        hvsrDF = params[&#39;hvsr_df&#39;]

        for k in ppsds.keys():
            #input_ppsds = ppsds[k][&#39;psd_values&#39;] #original, not used anymore
            input_ppsds = np.stack(hvsrDF[&#39;psd_values_&#39;+k].values)

            currPPSDs = hvsrDF[&#39;psd_values_&#39;+k][hvsrDF[&#39;Use&#39;]].values
            used_ppsds = np.stack(currPPSDs)
            

            #if reasmpling has been selected
            if resample is True or type(resample) is int:
                if resample is True:
                    resample = 1000 #Default smooth value

                xValMin = min(ppsds[k][&#39;period_bin_centers&#39;])
                xValMax = max(ppsds[k][&#39;period_bin_centers&#39;])

                #Resample period bin values
                x_periods[k] = np.logspace(np.log10(xValMin), np.log10(xValMax), num=resample)

                if smooth or type(smooth) is int:
                    if smooth:
                        smooth = 51 #Default smoothing window
                    elif smooth % 2==0:
                        smooth = smooth+1

                #Resample raw ppsd values
                for i, ppsd_t in enumerate(input_ppsds):
                    if i==0:
                        psdRaw[k] = np.interp(x_periods[k], ppsds[k][&#39;period_bin_centers&#39;], ppsd_t)
                        if smooth is not False:
                            psdRaw[k] = scipy.signal.savgol_filter(psdRaw[k], smooth, 3)
                    else:
                        psdRaw[k] = np.vstack((psdRaw[k], np.interp(x_periods[k], ppsds[k][&#39;period_bin_centers&#39;], ppsd_t)))
                        if smooth is not False:
                            psdRaw[k][i] = scipy.signal.savgol_filter(psdRaw[k][i], smooth, 3)

            else:
                #If no resampling desired
                x_periods[k] = np.array(ppsds[k][&#39;period_bin_centers&#39;])
                psdRaw[k] = np.array(input_ppsds)

            hvsrDF[&#39;psd_values_&#39;+k] = list(psdRaw[k])

            #Get average psd value across time for each channel (used to calc main H/V curve)
            psdValsTAvg[k] = np.nanmean(np.array(psdRaw[k]), axis=0)
            x_freqs[k] = np.divide(np.ones_like(x_periods[k]), x_periods[k]) 

            stDev[k] = np.std(psdRaw[k], axis=0)
            stDevValsM[k] = np.array(psdValsTAvg[k] - stDev[k])
            stDevValsP[k] = np.array(psdValsTAvg[k] + stDev[k])

            currTimesUsed[k] = np.array(hvsrDF[&#39;TimesProcessed_Obspy&#39;][hvsrDF[&#39;Use&#39;]].values)
            #currTimesUsed[k] = ppsds[k][&#39;current_times_used&#39;] #original one
            
        #Get string of method type
        if type(method) is int:
            methodInt = method
            method = methodList[method]
        params[&#39;method&#39;] = method

        #This gets the main hvsr curve averaged from all time steps
        anyK = list(x_freqs.keys())[0]
        hvsr_curve, _ = __get_hvsr_curve(x=x_freqs[anyK], psd=psdValsTAvg, method=methodInt, hvsr_data=params, verbose=verbose)
        origPPSD = params[&#39;ppsds_obspy&#39;].copy()

        #Add some other variables to our output dictionary
        hvsr_data = {&#39;input_params&#39;:params,
                    &#39;x_freqs&#39;:x_freqs,
                    &#39;hvsr_curve&#39;:hvsr_curve,
                    &#39;x_period&#39;:x_periods,
                    &#39;psd_raw&#39;:psdRaw,
                    &#39;current_times_used&#39;: currTimesUsed,
                    &#39;psd_values_tavg&#39;:psdValsTAvg,
                    &#39;ppsd_std&#39;:stDev,
                    &#39;ppsd_std_vals_m&#39;:stDevValsM,
                    &#39;ppsd_std_vals_p&#39;:stDevValsP,
                    &#39;method&#39;:method,
                    &#39;ppsds&#39;:ppsds,
                    &#39;ppsds_obspy&#39;:origPPSD,
                    &#39;tsteps_used&#39;: params[&#39;tsteps_used&#39;].copy(),
                    &#39;hvsr_df&#39;:params[&#39;hvsr_df&#39;]
                    }
        
        hvsr_out = HVSRData(hvsr_data)

        #This is if manual editing was used (should probably be updated at some point to just use masks)
        if &#39;xwindows_out&#39; in params.keys():
            hvsr_out[&#39;xwindows_out&#39;] = params[&#39;xwindows_out&#39;]
        else:
            hvsr_out[&#39;xwindows_out&#39;] = []

        #These are in other places in the hvsr_out dict, so are redudant
        #del dir(hvsr_out[&#39;input_params&#39;])[&#39;_ppsds_obspy&#39;]
        #del hvsr_out[&#39;input_params&#39;][&#39;_ppsds&#39;]
        #del hvsr_out[&#39;input_params&#39;][&#39;tsteps_used&#39;]

        freq_smooth_ko = [&#39;konno ohmachi&#39;, &#39;konno-ohmachi&#39;, &#39;konnoohmachi&#39;, &#39;konnohmachi&#39;, &#39;ko&#39;, &#39;k&#39;]
        freq_smooth_constant = [&#39;constant&#39;, &#39;const&#39;, &#39;c&#39;]
        freq_smooth_proport = [&#39;proportional&#39;, &#39;proportion&#39;, &#39;prop&#39;, &#39;p&#39;]

        #Frequency Smoothing
        if freq_smooth is False:
            if verbose:
                warnings.warn(&#39;No frequency smoothing is being applied. This is not recommended for noisy datasets.&#39;)
        elif freq_smooth is True or freq_smooth.lower() in freq_smooth_ko:
            from obspy.signal import konnoohmachismoothing
            for k in hvsr_out[&#39;psd_raw&#39;]:
                colName = f&#39;psd_values_{k}&#39;

                ppsd_data = np.stack(hvsr_out[&#39;hvsr_df&#39;][colName])
                ppsd_data = hvsr_out[&#39;psd_raw&#39;][k]

                freqs = hvsr_out[&#39;x_freqs&#39;][k]
                with warnings.catch_warnings():
                    warnings.simplefilter(&#39;ignore&#39;, category=UserWarning) #Filter out UserWarning for just this method, since it throws up a UserWarning that doesn&#39;t really matter about dtypes often
                    smoothed_ppsd_data = konnoohmachismoothing.konno_ohmachi_smoothing(ppsd_data, freqs, bandwidth=f_smooth_width, normalize=True)
                hvsr_out[&#39;psd_raw&#39;][k] = smoothed_ppsd_data
                hvsr_out[&#39;hvsr_df&#39;][colName] = pd.Series(list(smoothed_ppsd_data), index=hvsr_out[&#39;hvsr_df&#39;].index)

        elif freq_smooth.lower() in freq_smooth_constant:
            hvsr_out = __freq_smooth_window(hvsr_out, f_smooth_width, kind_freq_smooth=&#39;constant&#39;)
        elif freq_smooth.lower() in freq_smooth_proport:
            hvsr_out = __freq_smooth_window(hvsr_out, f_smooth_width, kind_freq_smooth=&#39;proportional&#39;)
        else:
            if verbose:
                warnings.warn(f&#39;You indicated no frequency smoothing should be applied (freq_smooth = {freq_smooth}). This is not recommended for noisy datasets.&#39;)

        #Get hvsr curve from three components at each time step
        anyK = list(hvsr_out[&#39;psd_raw&#39;].keys())[0]
        if method==1 or method ==&#39;dfa&#39; or method ==&#39;Diffuse Field Assumption&#39;:
            pass ###UPDATE HERE NEXT???__get_hvsr_curve(x=hvsr_out[&#39;x_freqs&#39;][anyK], psd=tStepDict, method=methodInt, hvsr_data=hvsr_out, verbose=verbose)
        else:
            hvsr_tSteps = []
            for tStep in range(len(hvsr_out[&#39;psd_raw&#39;][anyK])):
                tStepDict = {}
                for k in hvsr_out[&#39;psd_raw&#39;]:
                    tStepDict[k] = hvsr_out[&#39;psd_raw&#39;][k][tStep]
                hvsr_tstep, _ = __get_hvsr_curve(x=hvsr_out[&#39;x_freqs&#39;][anyK], psd=tStepDict, method=methodInt, hvsr_data=hvsr_out, verbose=verbose)
                hvsr_tSteps.append(np.float32(hvsr_tstep)) #Add hvsr curve for each time step to larger list of arrays with hvsr_curves

        hvsr_out[&#39;hvsr_df&#39;][&#39;HV_Curves&#39;] = hvsr_tSteps

        hvsr_out[&#39;ind_hvsr_curves&#39;] = np.stack(hvsr_out[&#39;hvsr_df&#39;][&#39;HV_Curves&#39;][hvsr_out[&#39;hvsr_df&#39;][&#39;Use&#39;]])
        #hvsr_out[&#39;ind_hvsr_curves&#39;] = np.array(hvsr_tSteps)

        #Initialize array based only on the curves we are currently using
        indHVCurvesArr = np.stack(hvsr_out[&#39;hvsr_df&#39;][&#39;HV_Curves&#39;][hvsr_out[&#39;hvsr_df&#39;][&#39;Use&#39;]])
        #indHVCurvesArr = hvsr_out[&#39;ind_hvsr_curves&#39;]

        if outlier_curve_std:
            #use the standard deviation of each individual curve to determine if it overlapped
            stdT = np.nanstd(indHVCurvesArr, axis=1)
            std_stdT= np.nanstd(stdT)
            avg_stdT= np.nanmean(stdT)
            bool_col=&#39;Use&#39;
            eval_col=&#39;HV_Curves&#39;
    
            testCol = hvsr_out[&#39;hvsr_df&#39;].loc[hvsr_out[&#39;hvsr_df&#39;][bool_col], eval_col].apply(np.nanstd).gt((avg_stdT + (std_stdT * outlier_curve_std)))
            low_std_val = avg_stdT - (std_stdT * outlier_curve_std)
            hi_std_val = avg_stdT + (std_stdT * outlier_curve_std)

            #First, do pandas version of it
            updateUseCol = hvsr_out[&#39;hvsr_df&#39;].loc[hvsr_out[&#39;hvsr_df&#39;][bool_col], eval_col].apply(np.nanstd).between(low_std_val, hi_std_val, inclusive=&#39;both&#39;)
            hvsr_out[&#39;hvsr_df&#39;].loc[hvsr_out[&#39;hvsr_df&#39;][bool_col], bool_col] = updateUseCol

            #Find psds to get rid of based on standard deviation of each curve (i.e., how curvy is the curve)
            psds_to_rid = []
            for i,t in enumerate(indHVCurvesArr):
                if stdT[i] &lt; avg_stdT - std_stdT*outlier_curve_std or stdT[i] &gt; avg_stdT + std_stdT*outlier_curve_std:
                    psds_to_rid.append(i)

            for i, r in enumerate(psds_to_rid):
                index = int(r-i)
                indHVCurvesArr = np.delete(indHVCurvesArr, index, axis=0)

                for k in hvsr_out[&#39;ppsds&#39;]:
                    hvsr_out[&#39;psd_raw&#39;][k] = np.delete(hvsr_out[&#39;psd_raw&#39;][k], index, axis=0)         
                    hvsr_out[&#39;current_times_used&#39;][k] = np.delete(hvsr_out[&#39;current_times_used&#39;][k], index)
            hvsr_out[&#39;tsteps_used&#39;][0] = hvsr_out[&#39;ppsds&#39;][k][&#39;current_times_used&#39;].shape[0]

        hvsr_out[&#39;ind_hvsr_stdDev&#39;] = np.nanstd(indHVCurvesArr, axis=0)

        #Get peaks for each time step
        tStepPeaks = []
        for tStepHVSR in hvsr_tSteps:
            tStepPeaks.append(__find_peaks(tStepHVSR))
        hvsr_out[&#39;ind_hvsr_peak_indices&#39;] = tStepPeaks
        hvsr_out[&#39;hvsr_df&#39;][&#39;CurvesPeakIndices&#39;] = tStepPeaks

        tStepPFList = []
        for tPeaks in tStepPeaks:
            tStepPFs = []
            for pInd in tPeaks:
                tStepPFs.append(np.float32(hvsr_out[&#39;x_freqs&#39;][anyK][pInd]))
            tStepPFList.append(tStepPFs)
        hvsr_out[&#39;hvsr_df&#39;][&#39;CurvesPeakFreqs&#39;] = tStepPFList

        #Get peaks of main HV curve
        hvsr_out[&#39;hvsr_peak_indices&#39;] = __find_peaks(hvsr_out[&#39;hvsr_curve&#39;])
        
        #Get frequency values at HV peaks in main curve
        hvsrPF=[]
        for p in hvsr_out[&#39;hvsr_peak_indices&#39;]:
            hvsrPF.append(hvsr_out[&#39;x_freqs&#39;][anyK][p])
        hvsr_out[&#39;hvsr_peak_freqs&#39;] = np.array(hvsrPF)


        #Get other HVSR parameters (i.e., standard deviations, etc.)
        hvsr_out = __gethvsrparams(hvsr_out)

        #Include the original obspy stream in the output
        hvsr_out[&#39;input_stream&#39;] = params[&#39;input_stream&#39;] #input_stream

        hvsr_out = sprit_utils.make_it_classy(hvsr_out)

        hvsr_out[&#39;ProcessingStatus&#39;][&#39;HVStatus&#39;] = True
    hvsr_out = _check_processing_status(hvsr_out)

    return hvsr_out</code></pre>
</details>
</dd>
<dt id="sprit.sprit_hvsr.remove_noise"><code class="name flex">
<span>def <span class="ident">remove_noise</span></span>(<span>hvsr_data, remove_method='auto', sat_percent=0.995, noise_percent=0.8, sta=2, lta=30, stalta_thresh=[0.5, 5], warmup_time=0, cooldown_time=0, min_win_size=1, remove_raw_noise=False, verbose=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to remove noisy windows from data, using various methods.</p>
<p>Methods include
- Manual window selection (by clicking on a chart with spectrogram and stream data),
- Auto window selection, which does the following two in sequence (these can also be done indepently):
- A sta/lta "antitrigger" method (using stalta values to automatically remove triggered windows where there appears to be too much noise)
- A noise threshold method, that cuts off all times where the noise threshold equals more than (by default) 80% of the highest amplitude noise sample for the length specified by lta (in seconds)
- A saturation threshold method, that cuts off all times where the noise threshold equals more than (by default) 99.5% of the highest amplitude noise sample.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>hvsr_data</code></strong> :&ensp;<code>dict, obspy.Stream,</code> or <code>obspy.Trace</code></dt>
<dd>Dictionary containing all the data and parameters for the HVSR analysis</dd>
<dt><strong><code>remove_method</code></strong> :&ensp;<code>str, {'auto', 'manual', 'stalta'/'antitrigger', 'saturation threshold', 'noise threshold', 'warmup'/'cooldown'/'buffer'/'warm_cool'}</code></dt>
<dd>The different methods for removing noise from the dataset. A list of strings will also work, in which case, it should be a list of the above strings. See descriptions above for what how each method works. By default 'auto.'
If remove_method='auto', this is the equivalent of remove_method=['noise threshold', 'antitrigger', 'saturation threshold', 'warm_cool']</dd>
<dt><strong><code>sat_percent</code></strong> :&ensp;<code>float</code>, default=<code>0.995</code></dt>
<dd>Percentage (between 0 and 1), to use as the threshold at which to remove data. This is used in the saturation method. By default 0.995.
If a value is passed that is greater than 1, it will be divided by 100 to obtain the percentage.</dd>
<dt><strong><code>noise_percent</code></strong> :&ensp;<code>float</code>, default <code>= 0.8</code></dt>
<dd>Percentage (between 0 and 1), to use as the threshold at which to remove data, if it persists for longer than time (in seconds (specified by min_win_size)). This is used in the noise threshold method. By default 0.8.
If a value is passed that is greater than 1, it will be divided by 100 to obtain the percentage.</dd>
<dt><strong><code>sta</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Short term average (STA) window (in seconds), by default 2. For use with sta/lta antitrigger method.</dd>
<dt><strong><code>lta</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Long term average (STA) window (in seconds), by default 30. For use with sta/lta antitrigger method.</dd>
<dt><strong><code>stalta_thresh</code></strong> :&ensp;<code>list</code>, default=<code>[0.5,5]</code></dt>
<dd>Two-item list or tuple with the thresholds for the stalta antitrigger. The first value (index [0]) is the lower threshold, the second value (index [1] is the upper threshold), by default [0.5,5]</dd>
<dt><strong><code>warmup_time</code></strong> :&ensp;<code>int</code>, default=<code>0</code></dt>
<dd>Time in seconds to allow for warmup of the instrument (or while operator is still near instrument). This will renove any data before this time, by default 0.</dd>
<dt><strong><code>cooldown_time</code></strong> :&ensp;<code>int</code>, default=<code>0</code></dt>
<dd>Time in seconds to allow for cooldown of the instrument (or for when operator is nearing instrument). This will renove any data before this time, by default 0.</dd>
<dt><strong><code>min_win_size</code></strong> :&ensp;<code>float</code>, default=<code>1</code></dt>
<dd>The minumum size a window must be over specified threshold (in seconds) for it to be removed</dd>
<dt><strong><code>remove_raw_noise</code></strong> :&ensp;<code>bool</code>, default=<code>False</code></dt>
<dd>If remove_raw_noise=True, will perform operation on raw data ('input_stream'), rather than potentially already-modified data ('stream').</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, default=<code>False</code></dt>
<dd>Whether to print status of remove_noise</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>output</code></strong> :&ensp;<code>dict</code></dt>
<dd>Dictionary similar to hvsr_data, but containing modified data with 'noise' removed</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def remove_noise(hvsr_data, remove_method=&#39;auto&#39;, sat_percent=0.995, noise_percent=0.80, sta=2, lta=30, stalta_thresh=[0.5,5], warmup_time=0, cooldown_time=0, min_win_size=1, remove_raw_noise=False, verbose=False):
    &#34;&#34;&#34;Function to remove noisy windows from data, using various methods.
    
    Methods include 
    - Manual window selection (by clicking on a chart with spectrogram and stream data), 
    - Auto window selection, which does the following two in sequence (these can also be done indepently):
        - A sta/lta &#34;antitrigger&#34; method (using stalta values to automatically remove triggered windows where there appears to be too much noise)
        - A noise threshold method, that cuts off all times where the noise threshold equals more than (by default) 80% of the highest amplitude noise sample for the length specified by lta (in seconds)
        - A saturation threshold method, that cuts off all times where the noise threshold equals more than (by default) 99.5% of the highest amplitude noise sample.

    Parameters
    ----------
    hvsr_data : dict, obspy.Stream, or obspy.Trace
        Dictionary containing all the data and parameters for the HVSR analysis
    remove_method : str, {&#39;auto&#39;, &#39;manual&#39;, &#39;stalta&#39;/&#39;antitrigger&#39;, &#39;saturation threshold&#39;, &#39;noise threshold&#39;, &#39;warmup&#39;/&#39;cooldown&#39;/&#39;buffer&#39;/&#39;warm_cool&#39;}
        The different methods for removing noise from the dataset. A list of strings will also work, in which case, it should be a list of the above strings. See descriptions above for what how each method works. By default &#39;auto.&#39;
        If remove_method=&#39;auto&#39;, this is the equivalent of remove_method=[&#39;noise threshold&#39;, &#39;antitrigger&#39;, &#39;saturation threshold&#39;, &#39;warm_cool&#39;]
    sat_percent : float, default=0.995
        Percentage (between 0 and 1), to use as the threshold at which to remove data. This is used in the saturation method. By default 0.995. 
        If a value is passed that is greater than 1, it will be divided by 100 to obtain the percentage.
    noise_percent : float, default = 0.8
        Percentage (between 0 and 1), to use as the threshold at which to remove data, if it persists for longer than time (in seconds (specified by min_win_size)). This is used in the noise threshold method. By default 0.8. 
        If a value is passed that is greater than 1, it will be divided by 100 to obtain the percentage.
    sta : int, optional
        Short term average (STA) window (in seconds), by default 2. For use with sta/lta antitrigger method.
    lta : int, optional
        Long term average (STA) window (in seconds), by default 30. For use with sta/lta antitrigger method.
    stalta_thresh : list, default=[0.5,5]
        Two-item list or tuple with the thresholds for the stalta antitrigger. The first value (index [0]) is the lower threshold, the second value (index [1] is the upper threshold), by default [0.5,5]
    warmup_time : int, default=0
        Time in seconds to allow for warmup of the instrument (or while operator is still near instrument). This will renove any data before this time, by default 0.
    cooldown_time : int, default=0
        Time in seconds to allow for cooldown of the instrument (or for when operator is nearing instrument). This will renove any data before this time, by default 0.
    min_win_size : float, default=1
        The minumum size a window must be over specified threshold (in seconds) for it to be removed
    remove_raw_noise : bool, default=False
        If remove_raw_noise=True, will perform operation on raw data (&#39;input_stream&#39;), rather than potentially already-modified data (&#39;stream&#39;).
    verbose : bool, default=False
        Whether to print status of remove_noise

    Returns
    -------
    output : dict
        Dictionary similar to hvsr_data, but containing modified data with &#39;noise&#39; removed
    &#34;&#34;&#34;
    orig_args = locals().copy() #Get the initial arguments

    if (verbose and isinstance(hvsr_data, HVSRBatch)) or (verbose and not hvsr_data[&#39;batch&#39;]):
        if isinstance(hvsr_data, HVSRData) and hvsr_data[&#39;batch&#39;]:
            pass
        else:
            print(&#39;\nRemoving noisy data windows (remove_noise())&#39;)
            print(&#39;\tUsing the following parameters:&#39;)
            for key, value in orig_args.items():
                if key==&#39;hvsr_data&#39;:
                    pass
                else:
                    print(&#39;\t  {}={}&#39;.format(key, value))

    #Setup lists
    manualList = [&#39;manual&#39;, &#39;man&#39;, &#39;m&#39;, &#39;window&#39;, &#39;windows&#39;, &#39;w&#39;]
    autoList = [&#39;auto&#39;, &#39;automatic&#39;, &#39;all&#39;, &#39;a&#39;]
    antitrigger = [&#39;stalta&#39;, &#39;anti&#39;, &#39;antitrigger&#39;, &#39;trigger&#39;, &#39;at&#39;]
    saturationThresh = [&#39;saturation threshold&#39;, &#39;saturation&#39;, &#39;sat&#39;, &#39;s&#39;]
    noiseThresh = [&#39;noise threshold&#39;, &#39;noise&#39;, &#39;threshold&#39;, &#39;n&#39;]
    warmup_cooldown=[&#39;warmup&#39;, &#39;cooldown&#39;, &#39;warm&#39;, &#39;cool&#39;, &#39;buffer&#39;, &#39;warmup-cooldown&#39;, &#39;warmup_cooldown&#39;, &#39;wc&#39;, &#39;warm_cool&#39;, &#39;warm-cool&#39;]

    #Get Stream from hvsr_data
    if isinstance(hvsr_data, HVSRBatch):
        #If running batch, we&#39;ll loop through each site
        hvsr_out = {}
        for site_name in hvsr_data.keys():
            args = orig_args.copy() #Make a copy so we don&#39;t accidentally overwrite
            args[&#39;hvsr_data&#39;] = hvsr_data[site_name] #Get what would normally be the &#34;hvsr_data&#34; variable for each site
            if hvsr_data[site_name][&#39;ProcessingStatus&#39;][&#39;OverallStatus&#39;]:
                try:
                   hvsr_out[site_name] = __remove_noise_batch(**args) #Call another function, that lets us run this function again
                except:
                    hvsr_out[site_name][&#39;ProcessingStatus&#39;][&#39;RemoveNoiseStatus&#39;]=False
                    hvsr_out[site_name][&#39;ProcessingStatus&#39;][&#39;OverallStatus&#39;]=False
            else:
                hvsr_data[site_name][&#39;ProcessingStatus&#39;][&#39;RemoveNoiseStatus&#39;]=False
                hvsr_data[site_name][&#39;ProcessingStatus&#39;][&#39;OverallStatus&#39;]=False
                hvsr_out = hvsr_data
                
        output = HVSRBatch(hvsr_out)
        return output

    elif isinstance(hvsr_data, (HVSRData, dict)):
        if remove_raw_noise:
            inStream = hvsr_data[&#39;input_stream&#39;].copy()
        else:
            inStream = hvsr_data[&#39;stream&#39;].copy()
        output = hvsr_data#.copy()
    elif isinstance(hvsr_data, (obspy.core.stream.Stream, obspy.core.trace.Trace)):
        inStream = hvsr_data.copy()
        output = inStream.copy()
    else:
        RuntimeError(f&#34;Input of type type(hvsr_data)={type(hvsr_data)} cannot be used.&#34;)
    
    outStream = inStream
    
    if isinstance(remove_method, str):
        if &#39;,&#39; in remove_method:
            remove_method = remove_method.split(&#39;,&#39;)
        else:
            remove_method = [remove_method]
    elif isinstance(remove_method, (list, tuple)):
        pass
    elif not remove_method:
        remove_method=[None]
    else:
        warnings.warn(f&#34;Input value remove_method={remove_method} must be either string, list of strings, None, or False. No noise removal will be carried out. Please choose one of the following: &#39;manual&#39;, &#39;auto&#39;, &#39;antitrigger&#39;, &#39;noise threshold&#39;, &#39;warmup_cooldown&#39;.&#34;)
        return output
        
    #Reorder list so manual is always first
    if len(set(remove_method).intersection(manualList)) &gt; 0:
        manInd = list(set(remove_method).intersection(manualList))[0]
        remove_method.remove(manInd)
        remove_method.insert(0, manInd)
        
    #Go through each type of removal and remove
    for rem_kind in remove_method:
        if not rem_kind:
            break
        elif rem_kind.lower() in manualList:
            if isinstance(output, (HVSRData, dict)):
                if &#39;xwindows_out&#39; in output.keys():
                    pass
                else:
                    output = _select_windows(output)
                window_list = output[&#39;xwindows_out&#39;]
            if isinstance(outStream, obspy.core.stream.Stream):
                if window_list is not None:
                    output[&#39;stream&#39;] = __remove_windows(inStream, window_list, warmup_time)
                else:
                    output = _select_windows(output)
            elif isinstance(output, (HVSRData, dict)):
                pass
            else:
                RuntimeError(&#34;Only obspy.core.stream.Stream data type is currently supported for manual noise removal method.&#34;)     
        elif rem_kind.lower() in autoList:
            outStream = __remove_noise_thresh(outStream, noise_percent=noise_percent, lta=lta, min_win_size=min_win_size)
            outStream = __remove_anti_stalta(outStream, sta=sta, lta=lta, thresh=stalta_thresh)
            outStream = __remove_noise_saturate(outStream, sat_percent=sat_percent, min_win_size=min_win_size)
            outStream = __remove_warmup_cooldown(stream=outStream, warmup_time=warmup_time, cooldown_time=cooldown_time)
        elif rem_kind.lower() in antitrigger:
            outStream = __remove_anti_stalta(outStream, sta=sta, lta=lta, thresh=stalta_thresh)
        elif rem_kind.lower() in saturationThresh:
            outStream = __remove_noise_saturate(outStream, sat_percent=sat_percent, min_win_size=min_win_size)
        elif rem_kind.lower() in noiseThresh:
            outStream = __remove_noise_thresh(outStream, noise_percent=noise_percent, lta=lta, min_win_size=min_win_size)
        elif rem_kind.lower() in warmup_cooldown:
            outStream = __remove_warmup_cooldown(stream=outStream, warmup_time=warmup_time, cooldown_time=cooldown_time)
        else:
            if len(remove_method)==1:
                warnings.warn(f&#34;Input value remove_method={remove_method} is not recognized. No noise removal will be carried out. Please choose one of the following: &#39;manual&#39;, &#39;auto&#39;, &#39;antitrigger&#39;, &#39;noise threshold&#39;, &#39;warmup_cooldown&#39;.&#34;)
                break
            warnings.warn(f&#34;Input value remove_method={remove_method} is not recognized. Continuing with other noise removal methods.&#34;)

    #Add output
    if isinstance(output, (HVSRData, dict)):
        output[&#39;stream&#39;] = outStream
        output[&#39;input_stream&#39;] = hvsr_data[&#39;input_stream&#39;]
        output[&#39;ProcessingStatus&#39;][&#39;RemoveNoiseStatus&#39;] = True
        output = _check_processing_status(output)

        if &#39;hvsr_df&#39; in output.keys():
            hvsrDF = output[&#39;hvsr_df&#39;]
            
            outStream = output[&#39;stream&#39;].split()
            for i, trace in enumerate(outStream):
                if i ==0:
                    trEndTime = trace.stats.endtime
                    comp_end = trace.stats.component
                    continue
                trStartTime = trace.stats.starttime
                comp_start = trace.stats.component
                
                if trEndTime &lt; trStartTime and comp_end==comp_start:
                    gap = [trEndTime,trStartTime]
                    output[&#39;hvsr_df&#39;][&#39;Use&#39;] = (hvsrDF[&#39;TimesProcessed_Obspy&#39;].gt(gap[0]) &amp; hvsrDF[&#39;TimesProcessed_Obspy&#39;].gt(gap[1]) )| \
                                    (hvsrDF[&#39;TimesProcessed_ObspyEnd&#39;].lt(gap[0]) &amp; hvsrDF[&#39;TimesProcessed_ObspyEnd&#39;].lt(gap[1]))# | \
                
                trEndTime = trace.stats.endtime
            
            outStream.merge()
            output[&#39;stream&#39;] = outStream        
    elif isinstance(hvsr_data, obspy.core.stream.Stream) or isinstance(hvsr_data, obspy.core.trace.Trace):
        output = outStream
    else:
        warnings.warn(f&#34;Output of type {type(output)} for this function will likely result in errors in other processing steps. Returning hvsr_data data.&#34;)
        return hvsr_data


    return output</code></pre>
</details>
</dd>
<dt id="sprit.sprit_hvsr.remove_outlier_curves"><code class="name flex">
<span>def <span class="ident">remove_outlier_curves</span></span>(<span>params, outlier_std=3, ppsd_length=30)</span>
</code></dt>
<dd>
<div class="desc"><p>Function used in generate_ppsds() to remove outliers. May also be used independently.</p>
<p>This uses the mean value of the entirety of each ppsd curve. This is not very robust, but it is intended only to remove curves who are well outside of the what would be expected.
These abberant curves often occur due to the remove_noise() function.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>params</code></strong> :&ensp;<code>dict</code></dt>
<dd>Input dictionary containing all the values and parameters of interest</dd>
<dt><strong><code>outlier_std</code></strong> :&ensp;<code> float</code>, default=<code>3</code></dt>
<dd>The standard deviation value to use as a threshold for determining whether a curve is an outlier.
This averages over each individual entire curve so that curves with very abberant data (often occurs when using the remove_noise() method), can be identified.</dd>
<dt><strong><code>ppsd_length</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Length of data segments passed to psd in seconds, by default 60.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>params</code></strong> :&ensp;<code>dict</code></dt>
<dd>Input dictionary with values modified based on work of function.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def remove_outlier_curves(params, outlier_std=3, ppsd_length=30):
    &#34;&#34;&#34;Function used in generate_ppsds() to remove outliers. May also be used independently.
    
    This uses the mean value of the entirety of each ppsd curve. This is not very robust, but it is intended only to remove curves who are well outside of the what would be expected.
    These abberant curves often occur due to the remove_noise() function.

    Parameters
    ----------
    params : dict
        Input dictionary containing all the values and parameters of interest
    outlier_std :  float, default=3
        The standard deviation value to use as a threshold for determining whether a curve is an outlier. 
        This averages over each individual entire curve so that curves with very abberant data (often occurs when using the remove_noise() method), can be identified.
    ppsd_length : float, optional
        Length of data segments passed to psd in seconds, by default 60.

    Returns
    -------
    params : dict
        Input dictionary with values modified based on work of function.
    &#34;&#34;&#34;
    
    ppsds = params[&#39;ppsds&#39;]
    newPPsds = {}
    stds = {}
    psds_to_rid = []

    for k in ppsds:
        #Get the average ppsd curve value
        psdVals = np.array(ppsds[k][&#39;psd_values&#39;])
        meanArr = np.nanmean(psdVals, axis=1)
        medArr = np.nanmedian(psdVals, axis=1)
        
        newPPsds[k] = []
        totMean = np.nanmean(meanArr)
        totMed = np.nanmedian(medArr)

        stds[k] = np.std(meanArr)

        for i, m in enumerate(meanArr):
            if m &gt; totMean + outlier_std*stds[k] or m &lt; totMean - outlier_std*stds[k]:
                psds_to_rid.append(i)

        curr_times_mpl = []
        for i, t in enumerate(ppsds[k][&#39;current_times_used&#39;]):
            curr_times_mpl.append(t.matplotlib_date)

        #Get ppsd length in seconds in matplotlib format
        ppsd_length_mpl = ppsd_length/86400

        ##UPDATE THIS NOT TO USE xWindows_out (calculate from mask)
        #Check if any times fall in excluded zone
        for i, t in enumerate(curr_times_mpl):
            nextT = t + ppsd_length_mpl
            for w, win in enumerate(params[&#39;xwindows_out&#39;]):
                if t &gt; win[0] and t &lt; win[1]:
                    psds_to_rid.append(i)
                elif nextT &gt; win[0] and nextT &lt; win[1]:
                    psds_to_rid.append(i)
    
        #Use dataframe
        hvsrDF = params[&#39;hvsr_df&#39;]
        psdVals = hvsrDF[&#39;psd_values_&#39;+k]
        params[&#39;hvsr_df&#39;][k+&#39;_CurveMedian&#39;] = psdVals.apply(np.nanmedian)
        params[&#39;hvsr_df&#39;][k+&#39;_CurveMean&#39;] = psdVals.apply(np.nanmean)

        totMean = np.nanmean(params[&#39;hvsr_df&#39;][k+&#39;_CurveMean&#39;])
        stds[k] = np.nanstd(params[&#39;hvsr_df&#39;][k+&#39;_CurveMean&#39;])

        meanArr = params[&#39;hvsr_df&#39;][k+&#39;_CurveMean&#39;]
        params[&#39;hvsr_df&#39;][&#39;Use&#39;] = meanArr &lt; (totMean + outlier_std * stds[k])

    psds_to_rid = np.unique(psds_to_rid)

    for k in params[&#39;ppsds&#39;]:
        for i, r in enumerate(psds_to_rid):
            index = int(r-i)
            params[&#39;ppsds&#39;][k][&#39;psd_values&#39;] = np.delete(params[&#39;ppsds&#39;][k][&#39;psd_values&#39;], index, axis=0)
            params[&#39;ppsds&#39;][k][&#39;current_times_used&#39;] = np.delete(params[&#39;ppsds&#39;][k][&#39;current_times_used&#39;], index, axis=0)
    return params</code></pre>
</details>
</dd>
<dt id="sprit.sprit_hvsr.run"><code class="name flex">
<span>def <span class="ident">run</span></span>(<span>datapath, source='file', verbose=False, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>The sprit.run() is the main function that allows you to do all your HVSR processing in one simple step (sprit.run() is how you would call it in your code, but it may also be called using sprit.sprit_hvsr.run())</p>
<p>The datapath parameter of sprit.run() is the only required parameter. This can be either a single file, a list of files (one for each component, for example), a directory (in which case, all obspy-readable files will be added to an HVSRBatch instance), a Rasp. Shake raw data directory, or sample data.</p>
<pre><code>The sprit.run() function calls the following functions. This is the recommended order/set of functions to run to process HVSR using SpRIT. See the API documentation for these functions for more information:
- input_params(): The datapath parameter of input_params() is the only required variable, though others may also need to be called for your data to process correctly.
- fetch_data(): the source parameter of fetch_data() is the only explicit variable in the sprit.run() function aside from datapath and verbose. Everything else gets delivered to the correct function via the kwargs dictionary
- remove_noise(): by default, the kind of noise removal is remove_method='auto'. See the remove_noise() documentation for more information. If remove_method is set to anything other than one of the explicit options in remove_noise, noise removal will not be carried out.
- generate_ppsds(): generates ppsds for each component, which will be combined/used later. Any parameter of obspy.signal.spectral_estimation.PPSD() may also be read into this function.
- process_hvsr(): this is the main function processing the hvsr curve and statistics. See process_hvsr() documentation for more details. The hvsr_band parameter sets the frequency spectrum over which these calculations occur.
- check_peaks(): this is the main function that will find and 'score' peaks to get a best peak. The parameter peak_freq_range can be set to limit the frequencies within which peaks are checked and scored.
- get_report(): this is the main function that will print, plot, and/or save the results of the data. See the get_report() API documentation for more information.
- export_data(): this function exports the final data output as a pickle file (by default, this pickle object has a .hvsr extension). This can be used to read data back into SpRIT without having to reprocess data.
</code></pre>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>datapath</code></strong> :&ensp;<code>str</code> or <code>filepath object that can be read by obspy</code></dt>
<dd>Filepath to data to be processed. This may be a file or directory, depending on what kind of data is being processed (this can be specified with the source parameter).
For sample data, The following can be specified as the datapath parameter:
- Any integer 1-6 (inclusive), or the string (e.g., datapath="1" or datapath=1 will work)
- The word "sample" before any integer (e.g., datapath="sample1")
- The word "sample" will default to "sample1" if source='file'.
- If source='batch', datapath should be datapath='sample' or datapath='batch'. In this case, it will read and process all the sample files using the HVSRBatch class. Set verbose=True to see all the information in the sample batch csv file.</dd>
<dt><strong><code>source</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd><em>description</em>, by default 'file'</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd><em>description</em>, by default False</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Keyword arguments for the functions listed above. The keyword arguments are unique, so they will get parsed out and passed into the appropriate function.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>hvsr_results</code></strong> :&ensp;<code><a title="sprit.HVSRData" href="index.html#sprit.HVSRData">HVSRData</a></code> or <code><a title="sprit.HVSRBatch" href="index.html#sprit.HVSRBatch">HVSRBatch</a> object</code></dt>
<dd>If a single file/data point is being processed, a HVSRData object will be returned. Otherwise, it will be a HVSRBatch object. See their documention for more information.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>RuntimeError</code></dt>
<dd>If the input parameter may not be read correctly. This is raised if the input_params() function fails. This raises an error since no other data processing or reading steps will be able to carried out correctly.</dd>
<dt><code>RuntimeError</code></dt>
<dd>If the data is not read/fetched correctly using fetch_data(), an error will be raised. This is raised if the fetch_data() function fails. This raises an error since no other data processing steps will be able to carried out correctly.</dd>
<dt><code>RuntimeError</code></dt>
<dd>If the data being processed is a single file, an error will be raised if generate_ppsds() does not work correctly. No errors are raised for remove_noise() errors (since that is an optional step) and the process_hvsr() step (since that is the last processing step) .</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run(datapath, source=&#39;file&#39;, verbose=False, **kwargs):
    &#34;&#34;&#34;The sprit.run() is the main function that allows you to do all your HVSR processing in one simple step (sprit.run() is how you would call it in your code, but it may also be called using sprit.sprit_hvsr.run())
    
    The datapath parameter of sprit.run() is the only required parameter. This can be either a single file, a list of files (one for each component, for example), a directory (in which case, all obspy-readable files will be added to an HVSRBatch instance), a Rasp. Shake raw data directory, or sample data.
    
        The sprit.run() function calls the following functions. This is the recommended order/set of functions to run to process HVSR using SpRIT. See the API documentation for these functions for more information:
        - input_params(): The datapath parameter of input_params() is the only required variable, though others may also need to be called for your data to process correctly.
        - fetch_data(): the source parameter of fetch_data() is the only explicit variable in the sprit.run() function aside from datapath and verbose. Everything else gets delivered to the correct function via the kwargs dictionary
        - remove_noise(): by default, the kind of noise removal is remove_method=&#39;auto&#39;. See the remove_noise() documentation for more information. If remove_method is set to anything other than one of the explicit options in remove_noise, noise removal will not be carried out.
        - generate_ppsds(): generates ppsds for each component, which will be combined/used later. Any parameter of obspy.signal.spectral_estimation.PPSD() may also be read into this function.
        - process_hvsr(): this is the main function processing the hvsr curve and statistics. See process_hvsr() documentation for more details. The hvsr_band parameter sets the frequency spectrum over which these calculations occur.
        - check_peaks(): this is the main function that will find and &#39;score&#39; peaks to get a best peak. The parameter peak_freq_range can be set to limit the frequencies within which peaks are checked and scored.
        - get_report(): this is the main function that will print, plot, and/or save the results of the data. See the get_report() API documentation for more information.
        - export_data(): this function exports the final data output as a pickle file (by default, this pickle object has a .hvsr extension). This can be used to read data back into SpRIT without having to reprocess data.

    Parameters
    ----------
    datapath : str or filepath object that can be read by obspy
        Filepath to data to be processed. This may be a file or directory, depending on what kind of data is being processed (this can be specified with the source parameter). 
        For sample data, The following can be specified as the datapath parameter:
            - Any integer 1-6 (inclusive), or the string (e.g., datapath=&#34;1&#34; or datapath=1 will work)
            - The word &#34;sample&#34; before any integer (e.g., datapath=&#34;sample1&#34;)
            - The word &#34;sample&#34; will default to &#34;sample1&#34; if source=&#39;file&#39;. 
            - If source=&#39;batch&#39;, datapath should be datapath=&#39;sample&#39; or datapath=&#39;batch&#39;. In this case, it will read and process all the sample files using the HVSRBatch class. Set verbose=True to see all the information in the sample batch csv file.
    source : str, optional
        _description_, by default &#39;file&#39;
    verbose : bool, optional
        _description_, by default False
    **kwargs
        Keyword arguments for the functions listed above. The keyword arguments are unique, so they will get parsed out and passed into the appropriate function.

    Returns
    -------
    hvsr_results : sprit.HVSRData or sprit.HVSRBatch object
        If a single file/data point is being processed, a HVSRData object will be returned. Otherwise, it will be a HVSRBatch object. See their documention for more information.

    Raises
    ------
    RuntimeError
        If the input parameter may not be read correctly. This is raised if the input_params() function fails. This raises an error since no other data processing or reading steps will be able to carried out correctly.
    RuntimeError
        If the data is not read/fetched correctly using fetch_data(), an error will be raised. This is raised if the fetch_data() function fails. This raises an error since no other data processing steps will be able to carried out correctly.
    RuntimeError
        If the data being processed is a single file, an error will be raised if generate_ppsds() does not work correctly. No errors are raised for remove_noise() errors (since that is an optional step) and the process_hvsr() step (since that is the last processing step) .
    &#34;&#34;&#34;
    #Get the input parameters
    input_params_kwargs = {k: v for k, v in locals()[&#39;kwargs&#39;].items() if k in input_params.__code__.co_varnames}
    try:
        params = input_params(datapath=datapath, verbose=verbose, **input_params_kwargs)
    except:
        #Even if batch, this is reading in data for all sites so we want to raise error, not just warn
        raise RuntimeError(&#39;Input parameters not read correctly, see sprit.input_params() function and parameters&#39;)
        #If input_params fails, initialize params as an HVSRDATA
        params = {&#39;ProcessingStatus&#39;:{&#39;InputStatus&#39;:False, &#39;OverallStatus&#39;:False}}
        params.update(input_params_kwargs)
        params = sprit_utils.make_it_classy(params)

    #Fetch Data
    try:
        fetch_data_kwargs = {k: v for k, v in locals()[&#39;kwargs&#39;].items() if k in fetch_data.__code__.co_varnames}
        dataIN = fetch_data(params=params, source=source, verbose=verbose, **fetch_data_kwargs)    
    except:
        #Even if batch, this is reading in data for all sites so we want to raise error, not just warn
        raise RuntimeError(&#39;Data not read correctly, see sprit.fetch_data() function and parameters for more details.&#39;)
    
    #Remove Noise
    try:
        remove_noise_kwargs = {k: v for k, v in locals()[&#39;kwargs&#39;].items() if k in remove_noise.__code__.co_varnames}
        data_noiseRemoved = remove_noise(hvsr_data=dataIN, verbose=verbose,**remove_noise_kwargs)   
    except:
        data_noiseRemoved = dataIN
        
        #Reformat data so HVSRData and HVSRBatch data both work here
        if isinstance(data_noiseRemoved, HVSRData):
            data_noiseRemoved = {&#39;place_holder_sitename&#39;:data_noiseRemoved}
            dataIN = {&#39;place_holder_sitename&#39;:dataIN}
            
        for site_name in data_noiseRemoved.keys():
            data_noiseRemoved[site_name][&#39;ProcessingStatus&#39;][&#39;RemoveNoiseStatus&#39;]=False
            #Since noise removal is not required for data processing, check others first
            if dataIN[site_name][&#39;ProcessingStatus&#39;][&#39;OverallStatus&#39;]:
                data_noiseRemoved[site_name][&#39;ProcessingStatus&#39;][&#39;OverallStatus&#39;] = True        
            else:
                data_noiseRemoved[site_name][&#39;ProcessingStatus&#39;][&#39;OverallStatus&#39;] = False

            #If it wasn&#39;t originally HVSRBatch, make it HVSRData object again
            if not data_noiseRemoved[site_name][&#39;batch&#39;]:
                data_noiseRemoved = data_noiseRemoved[site_name]
    
    #Generate PPSDs
    try:
        generate_ppsds_kwargs = {k: v for k, v in locals()[&#39;kwargs&#39;].items() if k in generate_ppsds.__code__.co_varnames}
        PPSDkwargs = {k: v for k, v in locals()[&#39;kwargs&#39;].items() if k in PPSD.__init__.__code__.co_varnames}
        generate_ppsds_kwargs.update(PPSDkwargs)
        ppsd_data = generate_ppsds(params=data_noiseRemoved, verbose=verbose,**generate_ppsds_kwargs)
    except Exception as e:
        if source == &#39;file&#39; or source==&#39;raw&#39;:
            if hasattr(e, &#39;message&#39;):
                errMsg = e.message
            else:
                errMsg = e
            raise RuntimeError(f&#34;generate_ppsds() error: {errMsg}&#34;)

        #Reformat data so HVSRData and HVSRBatch data both work here
        ppsd_data = data_noiseRemoved
        if isinstance(ppsd_data, HVSRData):
            ppsd_data = {&#39;place_holder_sitename&#39;:ppsd_data}
            
        for site_name in ppsd_data.keys(): #This should work more or less the same for batch and regular data now
            ppsd_data[site_name][&#39;ProcessingStatus&#39;][&#39;PPSDStatus&#39;]=False
            ppsd_data[site_name][&#39;ProcessingStatus&#39;][&#39;OverallStatus&#39;] = False
    
            #If it wasn&#39;t originally HVSRBatch, make it HVSRData object again
            if not ppsd_data[site_name][&#39;batch&#39;]:
                ppsd_data = ppsd_data[site_name]
    
    #Process HVSR Curves
    try:
        process_hvsr_kwargs = {k: v for k, v in locals()[&#39;kwargs&#39;].items() if k in process_hvsr.__code__.co_varnames}
        hvsr_results = process_hvsr(params=ppsd_data, verbose=verbose,**process_hvsr_kwargs)
    except Exception as e:
        traceback.print_exception(sys.exc_info()[1])
        exc_type, exc_obj, tb = sys.exc_info()
        f = tb.tb_frame
        lineno = tb.tb_lineno
        filename = f.f_code.co_filename
        errLineNo = str(traceback.extract_tb(sys.exc_info()[2])[-1].lineno)
        error_category = type(e).__name__.title().replace(&#39;error&#39;, &#39;Error&#39;)
        error_message = f&#34;{e} ({errLineNo})&#34;
        print(f&#34;{error_category} ({errLineNo}): {error_message}&#34;)
        print(lineno, filename, f)

        hvsr_results = ppsd_data
        if isinstance(hvsr_results, HVSRData):
            hvsr_results = {&#39;place_holder_sitename&#39;:hvsr_results}
            
        for site_name in hvsr_results.keys(): #This should work more or less the same for batch and regular data now
        
            hvsr_results[site_name][&#39;ProcessingStatus&#39;][&#39;HVStatus&#39;]=False
            hvsr_results[site_name][&#39;ProcessingStatus&#39;][&#39;OverallStatus&#39;] = False
            
            #If it wasn&#39;t originally HVSRBatch, make it HVSRData object again
            if not hvsr_results[site_name][&#39;batch&#39;]:
                hvsr_results = hvsr_results[site_name]            
            
    #Final post-processing/reporting

    #Check peaks
    check_peaks_kwargs = {k: v for k, v in locals()[&#39;kwargs&#39;].items() if k in check_peaks.__code__.co_varnames}
    hvsr_results = check_peaks(hvsr_data=hvsr_results, verbose=verbose, **check_peaks_kwargs)

    get_report_kwargs = {k: v for k, v in locals()[&#39;kwargs&#39;].items() if k in get_report.__code__.co_varnames}
    get_report(hvsr_results=hvsr_results, verbose=verbose, **get_report_kwargs)

    if verbose:
        if &#39;report_format&#39; in get_report_kwargs.keys():
            #if report_format is &#39;print&#39;, we would have already printed it in previous step
            if get_report_kwargs[&#39;report_format&#39;]==&#39;print&#39; or &#39;print&#39; in get_report_kwargs[&#39;report_format&#39;] or isinstance(hvsr_results, HVSRBatch):
                #We do not need to print another report if already printed to terminal
                pass
            else:
                #We will just change the report_format kwarg to print, since we already got the originally intended report format above, 
                #   now need to print for verbose output
                get_report_kwargs[&#39;report_format&#39;]=&#39;print&#39;
                get_report(hvsr_results=hvsr_results, **get_report_kwargs)
                
            if get_report_kwargs[&#39;report_format&#39;]==&#39;plot&#39; or &#39;plot&#39; in get_report_kwargs[&#39;report_format&#39;]:
                #We do not need to plot another report if already plotted
                pass
            else:
                hvplot_kwargs = {k: v for k, v in locals()[&#39;kwargs&#39;].items() if k in plot_hvsr.__code__.co_varnames}
                hvsr_results[&#39;HV_Plot&#39;] = plot_hvsr(hvsr_results, **hvplot_kwargs)
        else:
            pass
    
    #Export processed data if export_path(as pickle currently, default .hvsr extension)
    if &#39;export_path&#39; in kwargs.keys():
        if kwargs[&#39;export_path&#39;] is None:
            pass
        else:
            if &#39;ext&#39; in kwargs.keys():
                ext = kwargs[&#39;ext&#39;]
            else:
                ext = &#39;hvsr&#39;
            export_data(hvsr_data=hvsr_results, export_path=kwargs[&#39;export_path&#39;], ext=ext, verbose=verbose)        

    return hvsr_results</code></pre>
</details>
</dd>
<dt id="sprit.sprit_hvsr.test_function"><code class="name flex">
<span>def <span class="ident">test_function</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_function():
    print(&#39;is this working?&#39;)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="sprit.sprit_hvsr.HVSRBatch"><code class="flex name class">
<span>class <span class="ident">HVSRBatch</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>HVSRBatch is the data container used for batch processing. It contains several HVSRData objects (one for each site). These can be accessed using their site name, either square brackets (HVSRBatchVariable["SiteName"]) or the dot (HVSRBatchVariable.SiteName) accessor.</p>
<p>The dot accessor may not work if there is a space in the site name.</p>
<p>All of the
functions in the sprit.pacakge are designed to perform the bulk of their operations iteratively on the individual HVSRData objects contained in the HVSRBatch object, and do little with the HVSRBatch object itself, besides using it determine which sites are contained within it.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class HVSRBatch:
    &#34;&#34;&#34;HVSRBatch is the data container used for batch processing. It contains several HVSRData objects (one for each site). These can be accessed using their site name, either square brackets (HVSRBatchVariable[&#34;SiteName&#34;]) or the dot (HVSRBatchVariable.SiteName) accessor.
    
    The dot accessor may not work if there is a space in the site name.
    
    All of the  functions in the sprit.pacakge are designed to perform the bulk of their operations iteratively on the individual HVSRData objects contained in the HVSRBatch object, and do little with the HVSRBatch object itself, besides using it determine which sites are contained within it.
    
    &#34;&#34;&#34;
    @check_instance
    def __init__(self, sites_dict):
      
        self._batch_dict = sites_dict
        self.batch_dict = self._batch_dict
        self.batch = True

        for sitename, hvsrdata in sites_dict.items():
            setattr(self, sitename, hvsrdata)
            self[sitename][&#39;batch&#39;]=True
            
        self.sites = list(self._batch_dict.keys())

    #METHODS
    def __to_json(self, filepath):
        &#34;&#34;&#34;Not yet implemented, but may allow import/export to json files in the future, rather than just .hvsr pickles

        Parameters
        ----------
        filepath : filepath object
            Location to save HVSRBatch object as json
        &#34;&#34;&#34;
        # open the file with the given filepath
        with open(filepath, &#39;w&#39;) as f:
            # dump the JSON string to the file
            json.dump(self, f, default=lambda o: o.__dict__, sort_keys=True, indent=4)

    def export(self, export_path=True, ext=&#39;hvsr&#39;):
        &#34;&#34;&#34;Method to export HVSRData objects in HVSRBatch container to indivdual .hvsr pickle files.

        Parameters
        ----------
        export_path : filepath, default=True
            Filepath to save file. Can be either directory (which will assign a filename based on the HVSRData attributes). By default True. If True, it will first try to save each file to the same directory as datapath, then if that does not work, to the current working directory, then to the user&#39;s home directory, by default True
        ext : str, optional
            The extension to use for the output, by default &#39;hvsr&#39;. This is still a pickle file that can be read with pickle.load(), but will have .hvsr extension.
        &#34;&#34;&#34;
        export_data(hvsr_data=self, export_path=export_path, ext=ext)

    def keys(self):
        &#34;&#34;&#34;Method to return the &#34;keys&#34; of the HVSRBatch object. For HVSRBatch objects, these are the site names. Functions similar to dict.keys().

        Returns
        -------
        dict_keys
            A dict_keys object listing the site names of each of the HVSRData objects contained in the HVSRBatch object
        &#34;&#34;&#34;
        return self.batch_dict.keys()

    def items(self):
        &#34;&#34;&#34;Method to return both the site names and the HVSRData object as a set of dict_items tuples. Functions similar to dict.items().

        Returns
        -------
        _type_
            _description_
        &#34;&#34;&#34;
        return self.batch_dict.items()

    def copy(self, type=&#39;shallow&#39;):
        &#34;&#34;&#34;Make a copy of the HVSRBatch object. Uses python copy module.
        
        Parameters
        ----------
        type : str {&#39;shallow&#39;, &#39;deep&#39;}
            Based on input, creates either a shallow or deep copy of the HVSRBatch object. Shallow is equivalent of copy.copy(). Input of &#39;deep&#39; is equivalent of copy.deepcopy() (still experimental). Defaults to shallow.
    
        &#34;&#34;&#34;
        if type.lower()==&#39;deep&#39;:
            return HVSRBatch(copy.deepcopy(self._batch_dict))
        else:
            return HVSRBatch(copy.copy(self._batch_dict))

    #Method wrapper of sprit.plot_hvsr function
    def plot(self, **kwargs):
        &#34;&#34;&#34;Method to plot data, based on the sprit.plot_hvsr() function. All the same kwargs and default values apply as plot_hvsr(). For return_fig, returns it to the &#39;Plot_Report&#39; attribute of each HVSRData object

        Returns
        -------
        _type_
            _description_
        &#34;&#34;&#34;
        for sitename in self:
            if &#39;return_fig&#39; in kwargs.keys() and kwargs[&#39;return_fig&#39;]:
                self[sitename][&#39;Plot_Report&#39;] = plot_hvsr(self[sitename], **kwargs)
            else:
                plot_hvsr(self[sitename], **kwargs)

        return self
    
    def get_report(self, **kwargs):
        &#34;&#34;&#34;Method to get report from processed data, in print, graphical, or tabular format.

        Returns
        -------
        Variable
            May return nothing, pandas.Dataframe, or pyplot Figure, depending on input.
        &#34;&#34;&#34;
        if &#39;report_format&#39; in kwargs.keys():
            if &#39;csv&#39; == kwargs[&#39;report_format&#39;]:
                for sitename in self:
                    rowList = []
                    rowList.append(get_report(self[sitename], **kwargs))
                return pd.concat(rowList, ignore_index=True)
            elif &#39;plot&#39; == kwargs[&#39;report_format&#39;]:
                plotDict = {}
                for sitename in self:
                    if &#39;return_fig&#39; in kwargs.keys() and kwargs[&#39;return_fig&#39;]:
                        plotDict[sitename] = get_report(self[sitename], **kwargs)
                    else:
                        get_report(self[sitename], **kwargs)
                return plotDict
            
        #Only report_format left is print, doesn&#39;t return anything, so doesn&#39;t matter if defalut or not
        for sitename in self:
            get_report(self[sitename], **kwargs)
        return

    def report(self, **kwargs):
        &#34;&#34;&#34;Wrapper of get_report()&#34;&#34;&#34;
        return self.get_report(**kwargs)

    def __iter__(self):
        return iter(self._batch_dict.keys())

    def __setitem__(self, key, value):
        setattr(self, key, value)

    def __getitem__(self, key):
        return getattr(self, key)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="sprit.sprit_hvsr.HVSRBatch.copy"><code class="name flex">
<span>def <span class="ident">copy</span></span>(<span>self, type='shallow')</span>
</code></dt>
<dd>
<div class="desc"><p>Make a copy of the HVSRBatch object. Uses python copy module.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>type</code></strong> :&ensp;<code>str {'shallow', 'deep'}</code></dt>
<dd>Based on input, creates either a shallow or deep copy of the HVSRBatch object. Shallow is equivalent of copy.copy(). Input of 'deep' is equivalent of copy.deepcopy() (still experimental). Defaults to shallow.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def copy(self, type=&#39;shallow&#39;):
    &#34;&#34;&#34;Make a copy of the HVSRBatch object. Uses python copy module.
    
    Parameters
    ----------
    type : str {&#39;shallow&#39;, &#39;deep&#39;}
        Based on input, creates either a shallow or deep copy of the HVSRBatch object. Shallow is equivalent of copy.copy(). Input of &#39;deep&#39; is equivalent of copy.deepcopy() (still experimental). Defaults to shallow.

    &#34;&#34;&#34;
    if type.lower()==&#39;deep&#39;:
        return HVSRBatch(copy.deepcopy(self._batch_dict))
    else:
        return HVSRBatch(copy.copy(self._batch_dict))</code></pre>
</details>
</dd>
<dt id="sprit.sprit_hvsr.HVSRBatch.export"><code class="name flex">
<span>def <span class="ident">export</span></span>(<span>self, export_path=True, ext='hvsr')</span>
</code></dt>
<dd>
<div class="desc"><p>Method to export HVSRData objects in HVSRBatch container to indivdual .hvsr pickle files.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>export_path</code></strong> :&ensp;<code>filepath</code>, default=<code>True</code></dt>
<dd>Filepath to save file. Can be either directory (which will assign a filename based on the HVSRData attributes). By default True. If True, it will first try to save each file to the same directory as datapath, then if that does not work, to the current working directory, then to the user's home directory, by default True</dd>
<dt><strong><code>ext</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The extension to use for the output, by default 'hvsr'. This is still a pickle file that can be read with pickle.load(), but will have .hvsr extension.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def export(self, export_path=True, ext=&#39;hvsr&#39;):
    &#34;&#34;&#34;Method to export HVSRData objects in HVSRBatch container to indivdual .hvsr pickle files.

    Parameters
    ----------
    export_path : filepath, default=True
        Filepath to save file. Can be either directory (which will assign a filename based on the HVSRData attributes). By default True. If True, it will first try to save each file to the same directory as datapath, then if that does not work, to the current working directory, then to the user&#39;s home directory, by default True
    ext : str, optional
        The extension to use for the output, by default &#39;hvsr&#39;. This is still a pickle file that can be read with pickle.load(), but will have .hvsr extension.
    &#34;&#34;&#34;
    export_data(hvsr_data=self, export_path=export_path, ext=ext)</code></pre>
</details>
</dd>
<dt id="sprit.sprit_hvsr.HVSRBatch.get_report"><code class="name flex">
<span>def <span class="ident">get_report</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Method to get report from processed data, in print, graphical, or tabular format.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Variable</code></dt>
<dd>May return nothing, pandas.Dataframe, or pyplot Figure, depending on input.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_report(self, **kwargs):
    &#34;&#34;&#34;Method to get report from processed data, in print, graphical, or tabular format.

    Returns
    -------
    Variable
        May return nothing, pandas.Dataframe, or pyplot Figure, depending on input.
    &#34;&#34;&#34;
    if &#39;report_format&#39; in kwargs.keys():
        if &#39;csv&#39; == kwargs[&#39;report_format&#39;]:
            for sitename in self:
                rowList = []
                rowList.append(get_report(self[sitename], **kwargs))
            return pd.concat(rowList, ignore_index=True)
        elif &#39;plot&#39; == kwargs[&#39;report_format&#39;]:
            plotDict = {}
            for sitename in self:
                if &#39;return_fig&#39; in kwargs.keys() and kwargs[&#39;return_fig&#39;]:
                    plotDict[sitename] = get_report(self[sitename], **kwargs)
                else:
                    get_report(self[sitename], **kwargs)
            return plotDict
        
    #Only report_format left is print, doesn&#39;t return anything, so doesn&#39;t matter if defalut or not
    for sitename in self:
        get_report(self[sitename], **kwargs)
    return</code></pre>
</details>
</dd>
<dt id="sprit.sprit_hvsr.HVSRBatch.items"><code class="name flex">
<span>def <span class="ident">items</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Method to return both the site names and the HVSRData object as a set of dict_items tuples. Functions similar to dict.items().</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>_type_</code></dt>
<dd><em>description</em></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def items(self):
    &#34;&#34;&#34;Method to return both the site names and the HVSRData object as a set of dict_items tuples. Functions similar to dict.items().

    Returns
    -------
    _type_
        _description_
    &#34;&#34;&#34;
    return self.batch_dict.items()</code></pre>
</details>
</dd>
<dt id="sprit.sprit_hvsr.HVSRBatch.keys"><code class="name flex">
<span>def <span class="ident">keys</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Method to return the "keys" of the HVSRBatch object. For HVSRBatch objects, these are the site names. Functions similar to dict.keys().</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict_keys</code></dt>
<dd>A dict_keys object listing the site names of each of the HVSRData objects contained in the HVSRBatch object</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def keys(self):
    &#34;&#34;&#34;Method to return the &#34;keys&#34; of the HVSRBatch object. For HVSRBatch objects, these are the site names. Functions similar to dict.keys().

    Returns
    -------
    dict_keys
        A dict_keys object listing the site names of each of the HVSRData objects contained in the HVSRBatch object
    &#34;&#34;&#34;
    return self.batch_dict.keys()</code></pre>
</details>
</dd>
<dt id="sprit.sprit_hvsr.HVSRBatch.plot"><code class="name flex">
<span>def <span class="ident">plot</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Method to plot data, based on the sprit.plot_hvsr() function. All the same kwargs and default values apply as plot_hvsr(). For return_fig, returns it to the 'Plot_Report' attribute of each HVSRData object</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>_type_</code></dt>
<dd><em>description</em></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot(self, **kwargs):
    &#34;&#34;&#34;Method to plot data, based on the sprit.plot_hvsr() function. All the same kwargs and default values apply as plot_hvsr(). For return_fig, returns it to the &#39;Plot_Report&#39; attribute of each HVSRData object

    Returns
    -------
    _type_
        _description_
    &#34;&#34;&#34;
    for sitename in self:
        if &#39;return_fig&#39; in kwargs.keys() and kwargs[&#39;return_fig&#39;]:
            self[sitename][&#39;Plot_Report&#39;] = plot_hvsr(self[sitename], **kwargs)
        else:
            plot_hvsr(self[sitename], **kwargs)

    return self</code></pre>
</details>
</dd>
<dt id="sprit.sprit_hvsr.HVSRBatch.report"><code class="name flex">
<span>def <span class="ident">report</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Wrapper of get_report()</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def report(self, **kwargs):
    &#34;&#34;&#34;Wrapper of get_report()&#34;&#34;&#34;
    return self.get_report(**kwargs)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="sprit.sprit_hvsr.HVSRData"><code class="flex name class">
<span>class <span class="ident">HVSRData</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>HVSRData is the basic data class of the sprit package. It contains all the processed data, input parameters, and reports.</p>
<p>These attributes and objects can be accessed using square brackets or the dot accessor. For example, to access the site name, HVSRData['site'] and HVSRData.site will both return the site name.</p>
<p>Some of the methods that work on the HVSRData object (e.g., .plot() and .get_report()) are essentially wrappers for some of the main sprit package functions (sprit.plot_hvsr() and sprit.get_report(), respectively)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class HVSRData:
    &#34;&#34;&#34;HVSRData is the basic data class of the sprit package. It contains all the processed data, input parameters, and reports.
    
    These attributes and objects can be accessed using square brackets or the dot accessor. For example, to access the site name, HVSRData[&#39;site&#39;] and HVSRData.site will both return the site name.
    
    Some of the methods that work on the HVSRData object (e.g., .plot() and .get_report()) are essentially wrappers for some of the main sprit package functions (sprit.plot_hvsr() and sprit.get_report(), respectively)
    &#34;&#34;&#34;
    #Old way of using this
    #def __new__(cls, params):
    #    if isinstance(params, (cls, HVSRBatch)):
    #        return params
    #    return super().__new__(cls)

    @check_instance    
    def __init__(self, params):
        self.params = params
        #self.datastream = None
        self.batch = False
        #self.tsteps_used = []

        for key, value in params.items():
            setattr(self, key, value)
            if key==&#39;input_params&#39;:
                for k, v in params[key].items():
                    setattr(self, k, v)

    def __setitem__(self, key, value):
        setattr(self, key, value)

    def __getitem__(self, key):
        return getattr(self, key)

    def __to_json(self, filepath):
        &#34;&#34;&#34;Not yet supported, will export HVSRData object to json&#34;&#34;&#34;
        # open the file with the given filepath
        def unseriable_fun(o):
            if isinstance(o, np.ndarray):
                output = o.tolist()
            try:
                output = o.__dict__
            except:
                output = dir(o)
            return output

        with open(filepath, &#39;w&#39;) as f:
            # dump the JSON string to the file
            json.dump(self, f, default=unseriable_fun, sort_keys=True, indent=4)

    def export(self, export_path=None, ext=&#39;hvsr&#39;):
        &#34;&#34;&#34;Method to export HVSRData objects to .hvsr pickle files.

        Parameters
        ----------
        export_path : filepath, default=True
            Filepath to save file. Can be either directory (which will assign a filename based on the HVSRData attributes). By default True. If True, it will first try to save each file to the same directory as datapath, then if that does not work, to the current working directory, then to the user&#39;s home directory, by default True
        ext : str, optional
            The extension to use for the output, by default &#39;hvsr&#39;. This is still a pickle file that can be read with pickle.load(), but will have .hvsr extension.
        &#34;&#34;&#34;
        export_data(hvsr_data=self, export_path=export_path, ext=ext)

    #METHODS (many reflect dictionary methods)
    def keys(self):
        &#34;&#34;&#34;Method to return the &#34;keys&#34; of the HVSRData object. For HVSRData objects, these are the attributes and parameters of the object. Functions similar to dict.keys().

        Returns
        -------
        dict_keys
            A dict_keys object of the HVSRData objects attributes, parameters, etc.
        &#34;&#34;&#34;        
        keyList = []
        for k in dir(self):
            if not k.startswith(&#39;_&#39;):
                keyList.append(k)
        return keyList

    def items(self):
        &#34;&#34;&#34;Method to return the &#34;items&#34; of the HVSRData object. For HVSRData objects, this is a dict_items object with the keys and values in tuples. Functions similar to dict.items().

        Returns
        -------
        dict_items
            A dict_items object of the HVSRData objects attributes, parameters, etc.
        &#34;&#34;&#34;                
        return self.params.items()

    def copy(self, type=&#39;shallow&#39;):
        &#34;&#34;&#34;Make a copy of the HVSRData object. Uses python copy module.
        
        Parameters
        ----------
        type : str {&#39;shallow&#39;, &#39;deep&#39;}
            Based on input, creates either a shallow or deep copy of the HVSRData object. Shallow is equivalent of copy.copy(). Input of type=&#39;deep&#39; is equivalent of copy.deepcopy() (still experimental). Defaults to shallow.
    
        &#34;&#34;&#34;
        if type.lower()==&#39;deep&#39;:
            return HVSRData(copy.deepcopy(self.params))
        else:
            return HVSRData(copy.copy(self.params))
        
    def plot(self, **kwargs):
        &#34;&#34;&#34;Method to plot data, wrapper of sprit.plot_hvsr()

        Returns
        -------
        matplotlib.Figure, matplotlib.Axis (if return_fig=True)
        &#34;&#34;&#34;
        if &#39;close_figs&#39; not in kwargs.keys():
            kwargs[&#39;close_figs&#39;]=True
        plot_return = plot_hvsr(self, **kwargs)
        plt.show()
        return plot_return
        
    def get_report(self, **kwargs):
        &#34;&#34;&#34;Method to get report from processed data, in print, graphical, or tabular format.

        Returns
        -------
        Variable
            May return nothing, pandas.Dataframe, or pyplot Figure, depending on input.
        &#34;&#34;&#34;
        report_return = get_report(self, **kwargs)
        return report_return

    def report(self, **kwargs):
        &#34;&#34;&#34;Wrapper of get_report()&#34;&#34;&#34;
        report_return = get_report(self, **kwargs)
        return report_return
    
    #ATTRIBUTES
    #params
    @property
    def params(self):
        &#34;&#34;&#34;Dictionary containing the parameters used to process the data

        Returns
        -------
        dict
            Dictionary containing the process parameters
        &#34;&#34;&#34;
        return self._params

    @params.setter
    def params(self, value):
        if not (isinstance(value, dict)):
            raise ValueError(&#34;params must be a dict type, currently passing {} type.&#34;.format(type(value)))
        self._params = value
    
    #datastream
    @property
    def datastream(self):
        &#34;&#34;&#34;A copy of the original obspy datastream read in. This helps to retain the original data even after processing is carried out.

        Returns
        -------
        obspy.core.Stream.stream
            Obspy stream
        &#34;&#34;&#34;
        return self._datastream

    @datastream.setter
    def datastream(self, value):
        if value is not None and (not isinstance(value, obspy.core.stream.Stream)):
            raise ValueError(&#34;datastream must be an obspy Stream.&#34;)
        self._datastream = value
        
    #batch
    @property
    def batch(self):
        &#34;&#34;&#34;Whether this HVSRData object is part of an HVSRBatch object. This is used throughout the code to help direct the object into the proper processing pipeline.

        Returns
        -------
        bool
            True if HVSRData object is part of HVSRBatch object, otherwise, False
        &#34;&#34;&#34;
        return self._batch

    @batch.setter
    def batch(self, value):
        if value == 0:
            value = False
        elif value == 1:
            value = True
        else:
            value = None
        if not isinstance(value, bool):
            raise ValueError(&#34;batch must be boolean type&#34;)
        self._batch = value

    #PPSD object from obspy (static)
    @property
    def ppsds_obspy(self):
        &#34;&#34;&#34;The original ppsd information from the obspy.signal.spectral_estimation.PPSD(), so as to keep original if copy is manipulated/changed.&#34;&#34;&#34;        
        return self._ppsds_obspy

    @ppsds_obspy.setter
    def ppsds_obspy(self, value):
        &#34;&#34;&#34;Checks whether the ppsd_obspy is of the proper type before saving as attribute&#34;&#34;&#34;
        if not isinstance(value, obspy.signal.spectral_estimation.PPSD):
            if not isinstance(value, dict):
                raise ValueError(&#34;ppsds_obspy must be obspy.PPSD or dict with osbpy.PPSDs&#34;)
            else:
                for key in value.keys():
                    if not isinstance(value[key], obspy.signal.spectral_estimation.PPSD):
                        raise ValueError(&#34;ppsds_obspy must be obspy.PPSD or dict with osbpy.PPSDs&#34;)
        self._ppsds_obspy=value
                        
    #PPSD dict, copied from obspy ppsds (dynamic)
    @property
    def ppsds(self):
        &#34;&#34;&#34;Dictionary copy of the class object obspy.signal.spectral_estimation.PPSD(). The dictionary copy allows manipulation of the data in PPSD, whereas that data cannot be easily manipulated in the original Obspy object.

        Returns
        -------
        dict
            Dictionary copy of the PPSD information from generate_ppsds()
        &#34;&#34;&#34;
        return self._ppsds

    @ppsds.setter
    def ppsds(self, value):
        if not isinstance(value, dict):
            raise ValueError(&#34;ppsds dict with infomration from osbpy.PPSD (created by sprit.generate_ppsds())&#34;)                  
        self._ppsds=value</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="sprit.sprit_hvsr.HVSRData.batch"><code class="name">var <span class="ident">batch</span></code></dt>
<dd>
<div class="desc"><p>Whether this HVSRData object is part of an HVSRBatch object. This is used throughout the code to help direct the object into the proper processing pipeline.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>bool</code></dt>
<dd>True if HVSRData object is part of HVSRBatch object, otherwise, False</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def batch(self):
    &#34;&#34;&#34;Whether this HVSRData object is part of an HVSRBatch object. This is used throughout the code to help direct the object into the proper processing pipeline.

    Returns
    -------
    bool
        True if HVSRData object is part of HVSRBatch object, otherwise, False
    &#34;&#34;&#34;
    return self._batch</code></pre>
</details>
</dd>
<dt id="sprit.sprit_hvsr.HVSRData.datastream"><code class="name">var <span class="ident">datastream</span></code></dt>
<dd>
<div class="desc"><p>A copy of the original obspy datastream read in. This helps to retain the original data even after processing is carried out.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>obspy.core.Stream.stream</code></dt>
<dd>Obspy stream</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def datastream(self):
    &#34;&#34;&#34;A copy of the original obspy datastream read in. This helps to retain the original data even after processing is carried out.

    Returns
    -------
    obspy.core.Stream.stream
        Obspy stream
    &#34;&#34;&#34;
    return self._datastream</code></pre>
</details>
</dd>
<dt id="sprit.sprit_hvsr.HVSRData.params"><code class="name">var <span class="ident">params</span></code></dt>
<dd>
<div class="desc"><p>Dictionary containing the parameters used to process the data</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>Dictionary containing the process parameters</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def params(self):
    &#34;&#34;&#34;Dictionary containing the parameters used to process the data

    Returns
    -------
    dict
        Dictionary containing the process parameters
    &#34;&#34;&#34;
    return self._params</code></pre>
</details>
</dd>
<dt id="sprit.sprit_hvsr.HVSRData.ppsds"><code class="name">var <span class="ident">ppsds</span></code></dt>
<dd>
<div class="desc"><p>Dictionary copy of the class object obspy.signal.spectral_estimation.PPSD(). The dictionary copy allows manipulation of the data in PPSD, whereas that data cannot be easily manipulated in the original Obspy object.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>Dictionary copy of the PPSD information from generate_ppsds()</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def ppsds(self):
    &#34;&#34;&#34;Dictionary copy of the class object obspy.signal.spectral_estimation.PPSD(). The dictionary copy allows manipulation of the data in PPSD, whereas that data cannot be easily manipulated in the original Obspy object.

    Returns
    -------
    dict
        Dictionary copy of the PPSD information from generate_ppsds()
    &#34;&#34;&#34;
    return self._ppsds</code></pre>
</details>
</dd>
<dt id="sprit.sprit_hvsr.HVSRData.ppsds_obspy"><code class="name">var <span class="ident">ppsds_obspy</span></code></dt>
<dd>
<div class="desc"><p>The original ppsd information from the obspy.signal.spectral_estimation.PPSD(), so as to keep original if copy is manipulated/changed.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def ppsds_obspy(self):
    &#34;&#34;&#34;The original ppsd information from the obspy.signal.spectral_estimation.PPSD(), so as to keep original if copy is manipulated/changed.&#34;&#34;&#34;        
    return self._ppsds_obspy</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="sprit.sprit_hvsr.HVSRData.copy"><code class="name flex">
<span>def <span class="ident">copy</span></span>(<span>self, type='shallow')</span>
</code></dt>
<dd>
<div class="desc"><p>Make a copy of the HVSRData object. Uses python copy module.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>type</code></strong> :&ensp;<code>str {'shallow', 'deep'}</code></dt>
<dd>Based on input, creates either a shallow or deep copy of the HVSRData object. Shallow is equivalent of copy.copy(). Input of type='deep' is equivalent of copy.deepcopy() (still experimental). Defaults to shallow.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def copy(self, type=&#39;shallow&#39;):
    &#34;&#34;&#34;Make a copy of the HVSRData object. Uses python copy module.
    
    Parameters
    ----------
    type : str {&#39;shallow&#39;, &#39;deep&#39;}
        Based on input, creates either a shallow or deep copy of the HVSRData object. Shallow is equivalent of copy.copy(). Input of type=&#39;deep&#39; is equivalent of copy.deepcopy() (still experimental). Defaults to shallow.

    &#34;&#34;&#34;
    if type.lower()==&#39;deep&#39;:
        return HVSRData(copy.deepcopy(self.params))
    else:
        return HVSRData(copy.copy(self.params))</code></pre>
</details>
</dd>
<dt id="sprit.sprit_hvsr.HVSRData.export"><code class="name flex">
<span>def <span class="ident">export</span></span>(<span>self, export_path=None, ext='hvsr')</span>
</code></dt>
<dd>
<div class="desc"><p>Method to export HVSRData objects to .hvsr pickle files.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>export_path</code></strong> :&ensp;<code>filepath</code>, default=<code>True</code></dt>
<dd>Filepath to save file. Can be either directory (which will assign a filename based on the HVSRData attributes). By default True. If True, it will first try to save each file to the same directory as datapath, then if that does not work, to the current working directory, then to the user's home directory, by default True</dd>
<dt><strong><code>ext</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The extension to use for the output, by default 'hvsr'. This is still a pickle file that can be read with pickle.load(), but will have .hvsr extension.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def export(self, export_path=None, ext=&#39;hvsr&#39;):
    &#34;&#34;&#34;Method to export HVSRData objects to .hvsr pickle files.

    Parameters
    ----------
    export_path : filepath, default=True
        Filepath to save file. Can be either directory (which will assign a filename based on the HVSRData attributes). By default True. If True, it will first try to save each file to the same directory as datapath, then if that does not work, to the current working directory, then to the user&#39;s home directory, by default True
    ext : str, optional
        The extension to use for the output, by default &#39;hvsr&#39;. This is still a pickle file that can be read with pickle.load(), but will have .hvsr extension.
    &#34;&#34;&#34;
    export_data(hvsr_data=self, export_path=export_path, ext=ext)</code></pre>
</details>
</dd>
<dt id="sprit.sprit_hvsr.HVSRData.get_report"><code class="name flex">
<span>def <span class="ident">get_report</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Method to get report from processed data, in print, graphical, or tabular format.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Variable</code></dt>
<dd>May return nothing, pandas.Dataframe, or pyplot Figure, depending on input.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_report(self, **kwargs):
    &#34;&#34;&#34;Method to get report from processed data, in print, graphical, or tabular format.

    Returns
    -------
    Variable
        May return nothing, pandas.Dataframe, or pyplot Figure, depending on input.
    &#34;&#34;&#34;
    report_return = get_report(self, **kwargs)
    return report_return</code></pre>
</details>
</dd>
<dt id="sprit.sprit_hvsr.HVSRData.items"><code class="name flex">
<span>def <span class="ident">items</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Method to return the "items" of the HVSRData object. For HVSRData objects, this is a dict_items object with the keys and values in tuples. Functions similar to dict.items().</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict_items</code></dt>
<dd>A dict_items object of the HVSRData objects attributes, parameters, etc.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def items(self):
    &#34;&#34;&#34;Method to return the &#34;items&#34; of the HVSRData object. For HVSRData objects, this is a dict_items object with the keys and values in tuples. Functions similar to dict.items().

    Returns
    -------
    dict_items
        A dict_items object of the HVSRData objects attributes, parameters, etc.
    &#34;&#34;&#34;                
    return self.params.items()</code></pre>
</details>
</dd>
<dt id="sprit.sprit_hvsr.HVSRData.keys"><code class="name flex">
<span>def <span class="ident">keys</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Method to return the "keys" of the HVSRData object. For HVSRData objects, these are the attributes and parameters of the object. Functions similar to dict.keys().</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict_keys</code></dt>
<dd>A dict_keys object of the HVSRData objects attributes, parameters, etc.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def keys(self):
    &#34;&#34;&#34;Method to return the &#34;keys&#34; of the HVSRData object. For HVSRData objects, these are the attributes and parameters of the object. Functions similar to dict.keys().

    Returns
    -------
    dict_keys
        A dict_keys object of the HVSRData objects attributes, parameters, etc.
    &#34;&#34;&#34;        
    keyList = []
    for k in dir(self):
        if not k.startswith(&#39;_&#39;):
            keyList.append(k)
    return keyList</code></pre>
</details>
</dd>
<dt id="sprit.sprit_hvsr.HVSRData.plot"><code class="name flex">
<span>def <span class="ident">plot</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Method to plot data, wrapper of sprit.plot_hvsr()</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>matplotlib.Figure, matplotlib.Axis (if return_fig=True)</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot(self, **kwargs):
    &#34;&#34;&#34;Method to plot data, wrapper of sprit.plot_hvsr()

    Returns
    -------
    matplotlib.Figure, matplotlib.Axis (if return_fig=True)
    &#34;&#34;&#34;
    if &#39;close_figs&#39; not in kwargs.keys():
        kwargs[&#39;close_figs&#39;]=True
    plot_return = plot_hvsr(self, **kwargs)
    plt.show()
    return plot_return</code></pre>
</details>
</dd>
<dt id="sprit.sprit_hvsr.HVSRData.report"><code class="name flex">
<span>def <span class="ident">report</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Wrapper of get_report()</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def report(self, **kwargs):
    &#34;&#34;&#34;Wrapper of get_report()&#34;&#34;&#34;
    report_return = get_report(self, **kwargs)
    return report_return</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="sprit" href="index.html">sprit</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="sprit.sprit_hvsr.batch_data_read" href="#sprit.sprit_hvsr.batch_data_read">batch_data_read</a></code></li>
<li><code><a title="sprit.sprit_hvsr.check_instance" href="#sprit.sprit_hvsr.check_instance">check_instance</a></code></li>
<li><code><a title="sprit.sprit_hvsr.check_peaks" href="#sprit.sprit_hvsr.check_peaks">check_peaks</a></code></li>
<li><code><a title="sprit.sprit_hvsr.export_data" href="#sprit.sprit_hvsr.export_data">export_data</a></code></li>
<li><code><a title="sprit.sprit_hvsr.fetch_data" href="#sprit.sprit_hvsr.fetch_data">fetch_data</a></code></li>
<li><code><a title="sprit.sprit_hvsr.generate_ppsds" href="#sprit.sprit_hvsr.generate_ppsds">generate_ppsds</a></code></li>
<li><code><a title="sprit.sprit_hvsr.get_metadata" href="#sprit.sprit_hvsr.get_metadata">get_metadata</a></code></li>
<li><code><a title="sprit.sprit_hvsr.get_report" href="#sprit.sprit_hvsr.get_report">get_report</a></code></li>
<li><code><a title="sprit.sprit_hvsr.gui" href="#sprit.sprit_hvsr.gui">gui</a></code></li>
<li><code><a title="sprit.sprit_hvsr.import_data" href="#sprit.sprit_hvsr.import_data">import_data</a></code></li>
<li><code><a title="sprit.sprit_hvsr.input_params" href="#sprit.sprit_hvsr.input_params">input_params</a></code></li>
<li><code><a title="sprit.sprit_hvsr.plot_hvsr" href="#sprit.sprit_hvsr.plot_hvsr">plot_hvsr</a></code></li>
<li><code><a title="sprit.sprit_hvsr.plot_stream" href="#sprit.sprit_hvsr.plot_stream">plot_stream</a></code></li>
<li><code><a title="sprit.sprit_hvsr.process_hvsr" href="#sprit.sprit_hvsr.process_hvsr">process_hvsr</a></code></li>
<li><code><a title="sprit.sprit_hvsr.remove_noise" href="#sprit.sprit_hvsr.remove_noise">remove_noise</a></code></li>
<li><code><a title="sprit.sprit_hvsr.remove_outlier_curves" href="#sprit.sprit_hvsr.remove_outlier_curves">remove_outlier_curves</a></code></li>
<li><code><a title="sprit.sprit_hvsr.run" href="#sprit.sprit_hvsr.run">run</a></code></li>
<li><code><a title="sprit.sprit_hvsr.test_function" href="#sprit.sprit_hvsr.test_function">test_function</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="sprit.sprit_hvsr.HVSRBatch" href="#sprit.sprit_hvsr.HVSRBatch">HVSRBatch</a></code></h4>
<ul class="two-column">
<li><code><a title="sprit.sprit_hvsr.HVSRBatch.copy" href="#sprit.sprit_hvsr.HVSRBatch.copy">copy</a></code></li>
<li><code><a title="sprit.sprit_hvsr.HVSRBatch.export" href="#sprit.sprit_hvsr.HVSRBatch.export">export</a></code></li>
<li><code><a title="sprit.sprit_hvsr.HVSRBatch.get_report" href="#sprit.sprit_hvsr.HVSRBatch.get_report">get_report</a></code></li>
<li><code><a title="sprit.sprit_hvsr.HVSRBatch.items" href="#sprit.sprit_hvsr.HVSRBatch.items">items</a></code></li>
<li><code><a title="sprit.sprit_hvsr.HVSRBatch.keys" href="#sprit.sprit_hvsr.HVSRBatch.keys">keys</a></code></li>
<li><code><a title="sprit.sprit_hvsr.HVSRBatch.plot" href="#sprit.sprit_hvsr.HVSRBatch.plot">plot</a></code></li>
<li><code><a title="sprit.sprit_hvsr.HVSRBatch.report" href="#sprit.sprit_hvsr.HVSRBatch.report">report</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sprit.sprit_hvsr.HVSRData" href="#sprit.sprit_hvsr.HVSRData">HVSRData</a></code></h4>
<ul class="two-column">
<li><code><a title="sprit.sprit_hvsr.HVSRData.batch" href="#sprit.sprit_hvsr.HVSRData.batch">batch</a></code></li>
<li><code><a title="sprit.sprit_hvsr.HVSRData.copy" href="#sprit.sprit_hvsr.HVSRData.copy">copy</a></code></li>
<li><code><a title="sprit.sprit_hvsr.HVSRData.datastream" href="#sprit.sprit_hvsr.HVSRData.datastream">datastream</a></code></li>
<li><code><a title="sprit.sprit_hvsr.HVSRData.export" href="#sprit.sprit_hvsr.HVSRData.export">export</a></code></li>
<li><code><a title="sprit.sprit_hvsr.HVSRData.get_report" href="#sprit.sprit_hvsr.HVSRData.get_report">get_report</a></code></li>
<li><code><a title="sprit.sprit_hvsr.HVSRData.items" href="#sprit.sprit_hvsr.HVSRData.items">items</a></code></li>
<li><code><a title="sprit.sprit_hvsr.HVSRData.keys" href="#sprit.sprit_hvsr.HVSRData.keys">keys</a></code></li>
<li><code><a title="sprit.sprit_hvsr.HVSRData.params" href="#sprit.sprit_hvsr.HVSRData.params">params</a></code></li>
<li><code><a title="sprit.sprit_hvsr.HVSRData.plot" href="#sprit.sprit_hvsr.HVSRData.plot">plot</a></code></li>
<li><code><a title="sprit.sprit_hvsr.HVSRData.ppsds" href="#sprit.sprit_hvsr.HVSRData.ppsds">ppsds</a></code></li>
<li><code><a title="sprit.sprit_hvsr.HVSRData.ppsds_obspy" href="#sprit.sprit_hvsr.HVSRData.ppsds_obspy">ppsds_obspy</a></code></li>
<li><code><a title="sprit.sprit_hvsr.HVSRData.report" href="#sprit.sprit_hvsr.HVSRData.report">report</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>